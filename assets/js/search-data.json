{
  
    
        "post0": {
            "title": "Employee Exit Survey Data Cleaning and Aggregation",
            "content": ". Unsplash | Kon Karampelas Overview . This project deals with messy data from employee exit surveys from 2 government institutions in Queensland, Australia: . Department of Education, Training and Employment (DETE) | Technical and Further Education (TAFE) | . The project aims to determine what percentage of the resignees was dissatisfied with work: . based on age group | based on career stage | . On the technical side, this project is intended to showcase the use of several intermediate Pandas techniques for data cleaning and manipulation, including vectorized methods, mapping functions across data, dropping rows and columns, and combining DataFrames. . . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Clean and Analyze Employee Exit Surveys. The general project flow and research questions came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data Overview . TAFE Survey . The TAFE employee exit survey data can be found here. . Unfortunately, the CSV file itself is no longer available on the Australian government&#39;s websites, so I used a copy that I downloaded from Dataquest. . Columns . Below is information about the TAFE columns. . tafe = pd.read_csv(&quot;./2021-06-01-EES-Files/tafe_survey.csv&quot;) tafe.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 702 entries, 0 to 701 Data columns (total 72 columns): # Column Non-Null Count Dtype -- -- 0 Record ID 702 non-null float64 1 Institute 702 non-null object 2 WorkArea 702 non-null object 3 CESSATION YEAR 695 non-null float64 4 Reason for ceasing employment 701 non-null object 5 Contributing Factors. Career Move - Public Sector 437 non-null object 6 Contributing Factors. Career Move - Private Sector 437 non-null object 7 Contributing Factors. Career Move - Self-employment 437 non-null object 8 Contributing Factors. Ill Health 437 non-null object 9 Contributing Factors. Maternity/Family 437 non-null object 10 Contributing Factors. Dissatisfaction 437 non-null object 11 Contributing Factors. Job Dissatisfaction 437 non-null object 12 Contributing Factors. Interpersonal Conflict 437 non-null object 13 Contributing Factors. Study 437 non-null object 14 Contributing Factors. Travel 437 non-null object 15 Contributing Factors. Other 437 non-null object 16 Contributing Factors. NONE 437 non-null object 17 Main Factor. Which of these was the main factor for leaving? 113 non-null object 18 InstituteViews. Topic:1. I feel the senior leadership had a clear vision and direction 608 non-null object 19 InstituteViews. Topic:2. I was given access to skills training to help me do my job better 613 non-null object 20 InstituteViews. Topic:3. I was given adequate opportunities for personal development 610 non-null object 21 InstituteViews. Topic:4. I was given adequate opportunities for promotion within %Institute]Q25LBL% 608 non-null object 22 InstituteViews. Topic:5. I felt the salary for the job was right for the responsibilities I had 615 non-null object 23 InstituteViews. Topic:6. The organisation recognised when staff did good work 607 non-null object 24 InstituteViews. Topic:7. Management was generally supportive of me 614 non-null object 25 InstituteViews. Topic:8. Management was generally supportive of my team 608 non-null object 26 InstituteViews. Topic:9. I was kept informed of the changes in the organisation which would affect me 610 non-null object 27 InstituteViews. Topic:10. Staff morale was positive within the Institute 602 non-null object 28 InstituteViews. Topic:11. If I had a workplace issue it was dealt with quickly 601 non-null object 29 InstituteViews. Topic:12. If I had a workplace issue it was dealt with efficiently 597 non-null object 30 InstituteViews. Topic:13. If I had a workplace issue it was dealt with discreetly 601 non-null object 31 WorkUnitViews. Topic:14. I was satisfied with the quality of the management and supervision within my work unit 609 non-null object 32 WorkUnitViews. Topic:15. I worked well with my colleagues 605 non-null object 33 WorkUnitViews. Topic:16. My job was challenging and interesting 607 non-null object 34 WorkUnitViews. Topic:17. I was encouraged to use my initiative in the course of my work 610 non-null object 35 WorkUnitViews. Topic:18. I had sufficient contact with other people in my job 613 non-null object 36 WorkUnitViews. Topic:19. I was given adequate support and co-operation by my peers to enable me to do my job 609 non-null object 37 WorkUnitViews. Topic:20. I was able to use the full range of my skills in my job 609 non-null object 38 WorkUnitViews. Topic:21. I was able to use the full range of my abilities in my job. ; Category:Level of Agreement; Question:YOUR VIEWS ABOUT YOUR WORK UNIT] 608 non-null object 39 WorkUnitViews. Topic:22. I was able to use the full range of my knowledge in my job 608 non-null object 40 WorkUnitViews. Topic:23. My job provided sufficient variety 611 non-null object 41 WorkUnitViews. Topic:24. I was able to cope with the level of stress and pressure in my job 610 non-null object 42 WorkUnitViews. Topic:25. My job allowed me to balance the demands of work and family to my satisfaction 611 non-null object 43 WorkUnitViews. Topic:26. My supervisor gave me adequate personal recognition and feedback on my performance 606 non-null object 44 WorkUnitViews. Topic:27. My working environment was satisfactory e.g. sufficient space, good lighting, suitable seating and working area 610 non-null object 45 WorkUnitViews. Topic:28. I was given the opportunity to mentor and coach others in order for me to pass on my skills and knowledge prior to my cessation date 609 non-null object 46 WorkUnitViews. Topic:29. There was adequate communication between staff in my unit 603 non-null object 47 WorkUnitViews. Topic:30. Staff morale was positive within my work unit 606 non-null object 48 Induction. Did you undertake Workplace Induction? 619 non-null object 49 InductionInfo. Topic:Did you undertake a Corporate Induction? 432 non-null object 50 InductionInfo. Topic:Did you undertake a Institute Induction? 483 non-null object 51 InductionInfo. Topic: Did you undertake Team Induction? 440 non-null object 52 InductionInfo. Face to Face Topic:Did you undertake a Corporate Induction; Category:How it was conducted? 555 non-null object 53 InductionInfo. On-line Topic:Did you undertake a Corporate Induction; Category:How it was conducted? 555 non-null object 54 InductionInfo. Induction Manual Topic:Did you undertake a Corporate Induction? 555 non-null object 55 InductionInfo. Face to Face Topic:Did you undertake a Institute Induction? 530 non-null object 56 InductionInfo. On-line Topic:Did you undertake a Institute Induction? 555 non-null object 57 InductionInfo. Induction Manual Topic:Did you undertake a Institute Induction? 553 non-null object 58 InductionInfo. Face to Face Topic: Did you undertake Team Induction; Category? 555 non-null object 59 InductionInfo. On-line Topic: Did you undertake Team Induction?process you undertook and how it was conducted.] 555 non-null object 60 InductionInfo. Induction Manual Topic: Did you undertake Team Induction? 555 non-null object 61 Workplace. Topic:Did you and your Manager develop a Performance and Professional Development Plan (PPDP)? 608 non-null object 62 Workplace. Topic:Does your workplace promote a work culture free from all forms of unlawful discrimination? 594 non-null object 63 Workplace. Topic:Does your workplace promote and practice the principles of employment equity? 587 non-null object 64 Workplace. Topic:Does your workplace value the diversity of its employees? 586 non-null object 65 Workplace. Topic:Would you recommend the Institute as an employer to others? 581 non-null object 66 Gender. What is your Gender? 596 non-null object 67 CurrentAge. Current Age 596 non-null object 68 Employment Type. Employment Type 596 non-null object 69 Classification. Classification 596 non-null object 70 LengthofServiceOverall. Overall Length of Service at Institute (in years) 596 non-null object 71 LengthofServiceCurrent. Length of Service at current workplace (in years) 596 non-null object dtypes: float64(2), object(70) memory usage: 395.0+ KB . The formatting is different because some column names are apparently too long. However, we can see that: . There are 702 rows and 72 columns. | A few columns contain decimals, and most contain text. | Many of the columns have missing values. | . Dataquest notes a few columns in this dataset: . &quot;Record ID: An id used to identify the participant of the survey&quot; | &quot;Reason for ceasing employment: The reason why the person&#39;s employment ended&quot; | &quot;LengthofServiceOverall. Overall Length of Service at Institute (in years): The length of the person&#39;s employment (in years)&quot; | . Additionally, there are groups of columns that all start with the same name. These group names are: . Contributing Factors | Institute Views | Work Unit Views | Induction Info | Workplace | . Currently, there are too many columns for analysis. However, since the Contributing Factors columns are directly related to the employee&#39;s resignation, we could just keep those and remove the other 4 groups of columns. . Descriptive Statistics . Below are descriptive statistics for the columns. . tafe.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . Record ID Institute WorkArea CESSATION YEAR Reason for ceasing employment Contributing Factors. Career Move - Public Sector Contributing Factors. Career Move - Private Sector Contributing Factors. Career Move - Self-employment Contributing Factors. Ill Health Contributing Factors. Maternity/Family ... Workplace. Topic:Does your workplace promote a work culture free from all forms of unlawful discrimination? Workplace. Topic:Does your workplace promote and practice the principles of employment equity? Workplace. Topic:Does your workplace value the diversity of its employees? Workplace. Topic:Would you recommend the Institute as an employer to others? Gender. What is your Gender? CurrentAge. Current Age Employment Type. Employment Type Classification. Classification LengthofServiceOverall. Overall Length of Service at Institute (in years) LengthofServiceCurrent. Length of Service at current workplace (in years) . count 7.020000e+02 | 702 | 702 | 695.000000 | 701 | 437 | 437 | 437 | 437 | 437 | ... | 594 | 587 | 586 | 581 | 596 | 596 | 596 | 596 | 596 | 596 | . unique NaN | 12 | 2 | NaN | 6 | 2 | 2 | 2 | 2 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 9 | 5 | 9 | 7 | 7 | . top NaN | Brisbane North Institute of TAFE | Non-Delivery (corporate) | NaN | Resignation | - | - | - | - | - | ... | Yes | Yes | Yes | Yes | Female | 56 or older | Permanent Full-time | Administration (AO) | Less than 1 year | Less than 1 year | . freq NaN | 161 | 432 | NaN | 340 | 375 | 336 | 420 | 403 | 411 | ... | 536 | 512 | 488 | 416 | 389 | 162 | 237 | 293 | 147 | 177 | . mean 6.346026e+17 | NaN | NaN | 2011.423022 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . std 2.515071e+14 | NaN | NaN | 0.905977 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . min 6.341330e+17 | NaN | NaN | 2009.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 25% 6.343954e+17 | NaN | NaN | 2011.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 50% 6.345835e+17 | NaN | NaN | 2011.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 75% 6.348005e+17 | NaN | NaN | 2012.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . max 6.350730e+17 | NaN | NaN | 2013.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 11 rows × 72 columns . Some text columns appear to have values that only contain a single hyphen (-); these will have to be investigated later. . DETE Survey . The DETE employee exit survey can be found here. The copy used in this project is a slightly modified version downloaded from Dataquest for convenience. It is still complete in terms of the number of entries. . Columns . Below are the column names and types. . dete = pd.read_csv(&quot;./2021-06-01-EES-Files/dete_survey.csv&quot;) dete.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 822 entries, 0 to 821 Data columns (total 56 columns): # Column Non-Null Count Dtype -- -- 0 ID 822 non-null int64 1 SeparationType 822 non-null object 2 Cease Date 822 non-null object 3 DETE Start Date 822 non-null object 4 Role Start Date 822 non-null object 5 Position 817 non-null object 6 Classification 455 non-null object 7 Region 822 non-null object 8 Business Unit 126 non-null object 9 Employment Status 817 non-null object 10 Career move to public sector 822 non-null bool 11 Career move to private sector 822 non-null bool 12 Interpersonal conflicts 822 non-null bool 13 Job dissatisfaction 822 non-null bool 14 Dissatisfaction with the department 822 non-null bool 15 Physical work environment 822 non-null bool 16 Lack of recognition 822 non-null bool 17 Lack of job security 822 non-null bool 18 Work location 822 non-null bool 19 Employment conditions 822 non-null bool 20 Maternity/family 822 non-null bool 21 Relocation 822 non-null bool 22 Study/Travel 822 non-null bool 23 Ill Health 822 non-null bool 24 Traumatic incident 822 non-null bool 25 Work life balance 822 non-null bool 26 Workload 822 non-null bool 27 None of the above 822 non-null bool 28 Professional Development 808 non-null object 29 Opportunities for promotion 735 non-null object 30 Staff morale 816 non-null object 31 Workplace issue 788 non-null object 32 Physical environment 817 non-null object 33 Worklife balance 815 non-null object 34 Stress and pressure support 810 non-null object 35 Performance of supervisor 813 non-null object 36 Peer support 812 non-null object 37 Initiative 813 non-null object 38 Skills 811 non-null object 39 Coach 767 non-null object 40 Career Aspirations 746 non-null object 41 Feedback 792 non-null object 42 Further PD 768 non-null object 43 Communication 814 non-null object 44 My say 812 non-null object 45 Information 816 non-null object 46 Kept informed 813 non-null object 47 Wellness programs 766 non-null object 48 Health &amp; Safety 793 non-null object 49 Gender 798 non-null object 50 Age 811 non-null object 51 Aboriginal 16 non-null object 52 Torres Strait 3 non-null object 53 South Sea 7 non-null object 54 Disability 23 non-null object 55 NESB 32 non-null object dtypes: bool(18), int64(1), object(37) memory usage: 258.6+ KB . ID contains integers, whereas all other columns either contain text or booleans. Also, many of the columns have missing values. . There are 822 rows (employees) and 56 columns (survey questions). Dataquest describes a few notable columns as follows: . &quot;ID: An id used to identify the participant of the survey&quot; | &quot;SeparationType: The reason why the person&#39;s employment ended&quot; | &quot;Cease Date: The year or month the person&#39;s employment ended&quot; | &quot;DETE Start Date: The year the person began employment with the DETE&quot; | . Also, if we look closely, we can see that many of the columns in the DETE dataset match columns in the TAFE dataset. For example: . print(&quot;TAFE Columns&quot;) print(list(tafe.columns[5:17])) print(&quot; nDETE Columns&quot;) print(list(dete.columns[10:28])) . TAFE Columns [&#39;Contributing Factors. Career Move - Public Sector &#39;, &#39;Contributing Factors. Career Move - Private Sector &#39;, &#39;Contributing Factors. Career Move - Self-employment&#39;, &#39;Contributing Factors. Ill Health&#39;, &#39;Contributing Factors. Maternity/Family&#39;, &#39;Contributing Factors. Dissatisfaction&#39;, &#39;Contributing Factors. Job Dissatisfaction&#39;, &#39;Contributing Factors. Interpersonal Conflict&#39;, &#39;Contributing Factors. Study&#39;, &#39;Contributing Factors. Travel&#39;, &#39;Contributing Factors. Other&#39;, &#39;Contributing Factors. NONE&#39;] DETE Columns [&#39;Career move to public sector&#39;, &#39;Career move to private sector&#39;, &#39;Interpersonal conflicts&#39;, &#39;Job dissatisfaction&#39;, &#39;Dissatisfaction with the department&#39;, &#39;Physical work environment&#39;, &#39;Lack of recognition&#39;, &#39;Lack of job security&#39;, &#39;Work location&#39;, &#39;Employment conditions&#39;, &#39;Maternity/family&#39;, &#39;Relocation&#39;, &#39;Study/Travel&#39;, &#39;Ill Health&#39;, &#39;Traumatic incident&#39;, &#39;Work life balance&#39;, &#39;Workload&#39;, &#39;None of the above&#39;] . These two sets of columns represent the &quot;Contributing Factors&quot; group in each survey. These are some of the columns that we want to keep for analysis; this will be addressed in data cleaning later. . Descriptive Statistics . We can view descriptive statistics for all columns below. . dete.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . ID SeparationType Cease Date DETE Start Date Role Start Date Position Classification Region Business Unit Employment Status ... Kept informed Wellness programs Health &amp; Safety Gender Age Aboriginal Torres Strait South Sea Disability NESB . count 822.000000 | 822 | 822 | 822 | 822 | 817 | 455 | 822 | 126 | 817 | ... | 813 | 766 | 793 | 798 | 811 | 16 | 3 | 7 | 23 | 32 | . unique NaN | 9 | 25 | 51 | 46 | 15 | 8 | 9 | 14 | 5 | ... | 6 | 6 | 6 | 2 | 10 | 1 | 1 | 1 | 1 | 1 | . top NaN | Age Retirement | 2012 | Not Stated | Not Stated | Teacher | Primary | Metropolitan | Education Queensland | Permanent Full-time | ... | A | A | A | Female | 61 or older | Yes | Yes | Yes | Yes | Yes | . freq NaN | 285 | 344 | 73 | 98 | 324 | 161 | 135 | 54 | 434 | ... | 401 | 253 | 386 | 573 | 222 | 16 | 3 | 7 | 23 | 32 | . mean 411.693431 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . std 237.705820 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . min 1.000000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 25% 206.250000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 50% 411.500000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 75% 616.750000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . max 823.000000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 11 rows × 56 columns . The Cease Date, DETE Start Date, and Role Start Date columns are interesting because these are in text format and some of the values are &quot;Not Stated&quot;. . Also, based on counts of non-null values shown earlier, some columns have many missing values. These will have to be addressed in data cleaning. . Data Cleaning . Placeholders for Missing Values . Earlier, we noticed that some columns in the DETE data contain &quot;Not Stated&quot; values. These are likely to be placeholders for missing data. . Therefore, we can replace all &quot;Not Stated&quot; values with np.nan null values. . dete = dete.replace(&quot;Not Stated&quot;, np.nan) . Dropping Columns . In the TAFE dataset, there are 4 other big groups of columns other than Contributing Factors: . Institute Views | Work Unit Views | Induction Info | Workplace | . We want to remove these columns in order to limit the columns in our dataset to the most relevant ones. This is done below. . tafe = tafe.drop( labels = tafe.columns[17:66], axis = 1, ) list(tafe.columns) . [&#39;Record ID&#39;, &#39;Institute&#39;, &#39;WorkArea&#39;, &#39;CESSATION YEAR&#39;, &#39;Reason for ceasing employment&#39;, &#39;Contributing Factors. Career Move - Public Sector &#39;, &#39;Contributing Factors. Career Move - Private Sector &#39;, &#39;Contributing Factors. Career Move - Self-employment&#39;, &#39;Contributing Factors. Ill Health&#39;, &#39;Contributing Factors. Maternity/Family&#39;, &#39;Contributing Factors. Dissatisfaction&#39;, &#39;Contributing Factors. Job Dissatisfaction&#39;, &#39;Contributing Factors. Interpersonal Conflict&#39;, &#39;Contributing Factors. Study&#39;, &#39;Contributing Factors. Travel&#39;, &#39;Contributing Factors. Other&#39;, &#39;Contributing Factors. NONE&#39;, &#39;Gender. What is your Gender?&#39;, &#39;CurrentAge. Current Age&#39;, &#39;Employment Type. Employment Type&#39;, &#39;Classification. Classification&#39;, &#39;LengthofServiceOverall. Overall Length of Service at Institute (in years)&#39;, &#39;LengthofServiceCurrent. Length of Service at current workplace (in years)&#39;] . There are now only 23 columns in the TAFE dataset. . Earlier, in the Data Overview, we mentioned that DETE has similar columns to the ones in TAFE. For example, look at the columns from indices 28 to 48: . dete.columns[28:49] . Index([&#39;Professional Development&#39;, &#39;Opportunities for promotion&#39;, &#39;Staff morale&#39;, &#39;Workplace issue&#39;, &#39;Physical environment&#39;, &#39;Worklife balance&#39;, &#39;Stress and pressure support&#39;, &#39;Performance of supervisor&#39;, &#39;Peer support&#39;, &#39;Initiative&#39;, &#39;Skills&#39;, &#39;Coach&#39;, &#39;Career Aspirations&#39;, &#39;Feedback&#39;, &#39;Further PD&#39;, &#39;Communication&#39;, &#39;My say&#39;, &#39;Information&#39;, &#39;Kept informed&#39;, &#39;Wellness programs&#39;, &#39;Health &amp; Safety&#39;], dtype=&#39;object&#39;) . These are equivalent to the TAFE columns under the Institute Views, Work Unit Views, Induction Info, and Workspace groups. We don&#39;t need these groups since the Contributing Factors group is directly related to the reason why the employees resigned. . Thus, we will remove the columns shown above from the DETE dataset. . dete = dete.drop( dete.columns[28:49], axis = 1, ) len(dete.columns) . 35 . dete = dete.replace(&quot;Not Stated&quot;, np.nan) . Matching Columns in TAFE and DETE . Below are some important columns in TAFE and DETE which have matching information. . DETE Survey TAFE Survey . ID | Record ID | . SeparationType | Reason for ceasing employment | . Cease Date | CESSATION YEAR | . DETE Start Date | LengthofServiceOverall. Overall Length of Service at Institute (in years) | . Age | CurrentAge. Current Age | . Gender | Gender. What is your Gender? | . Notably, DETE Start Date and LengthofServiceOverall are matching columns because one can tell how long the employee has been working based on the date when they first started working. . We want to make the column names the same between the two datasets. . Before we do that, we will simplify the names in DETE. . dete.columns = ( dete.columns .str.lower() # All lowercase .str.strip() # Remove whitespace on sides .str.replace(&quot; &quot;, &quot;_&quot;) # Replace spaces with underscores ) dete.columns . Index([&#39;id&#39;, &#39;separationtype&#39;, &#39;cease_date&#39;, &#39;dete_start_date&#39;, &#39;role_start_date&#39;, &#39;position&#39;, &#39;classification&#39;, &#39;region&#39;, &#39;business_unit&#39;, &#39;employment_status&#39;, &#39;career_move_to_public_sector&#39;, &#39;career_move_to_private_sector&#39;, &#39;interpersonal_conflicts&#39;, &#39;job_dissatisfaction&#39;, &#39;dissatisfaction_with_the_department&#39;, &#39;physical_work_environment&#39;, &#39;lack_of_recognition&#39;, &#39;lack_of_job_security&#39;, &#39;work_location&#39;, &#39;employment_conditions&#39;, &#39;maternity/family&#39;, &#39;relocation&#39;, &#39;study/travel&#39;, &#39;ill_health&#39;, &#39;traumatic_incident&#39;, &#39;work_life_balance&#39;, &#39;workload&#39;, &#39;none_of_the_above&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;aboriginal&#39;, &#39;torres_strait&#39;, &#39;south_sea&#39;, &#39;disability&#39;, &#39;nesb&#39;], dtype=&#39;object&#39;) . The DETE columns have been simplified. . Next, we will change some of the TAFE column labels to match the ones in the DETE dataset. . new_columns = { &#39;Record ID&#39;: &#39;id&#39;, &#39;CESSATION YEAR&#39;: &#39;cease_date&#39;, &#39;Reason for ceasing employment&#39;: &#39;separationtype&#39;, &#39;Gender. What is your Gender?&#39;: &#39;gender&#39;, &#39;CurrentAge. Current Age&#39;: &#39;age&#39;, &#39;Employment Type. Employment Type&#39;: &#39;employment_status&#39;, &#39;Classification. Classification&#39;: &#39;position&#39;, &#39;LengthofServiceOverall. Overall Length of Service at Institute (in years)&#39;: &#39;institute_service&#39;, &#39;LengthofServiceCurrent. Length of Service at current workplace (in years)&#39;: &#39;role_service&#39;, } tafe = tafe.rename( new_columns, axis = 1, ) list(tafe.columns) . [&#39;id&#39;, &#39;Institute&#39;, &#39;WorkArea&#39;, &#39;cease_date&#39;, &#39;separationtype&#39;, &#39;Contributing Factors. Career Move - Public Sector &#39;, &#39;Contributing Factors. Career Move - Private Sector &#39;, &#39;Contributing Factors. Career Move - Self-employment&#39;, &#39;Contributing Factors. Ill Health&#39;, &#39;Contributing Factors. Maternity/Family&#39;, &#39;Contributing Factors. Dissatisfaction&#39;, &#39;Contributing Factors. Job Dissatisfaction&#39;, &#39;Contributing Factors. Interpersonal Conflict&#39;, &#39;Contributing Factors. Study&#39;, &#39;Contributing Factors. Travel&#39;, &#39;Contributing Factors. Other&#39;, &#39;Contributing Factors. NONE&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;employment_status&#39;, &#39;position&#39;, &#39;institute_service&#39;, &#39;role_service&#39;] . The Contributing Factors columns&#39; names haven&#39;t been changed yet, but this will be dealt with later. . Identifying Employees who Resigned . The DETE and TAFE exit surveys were given to all employees who left the institutions. Some of them were terminated from employment, some resigned, and some retired. . The goal of this project is to find out why employees resigned. Thus, we have to find out who resigned, and drop the data for the rest. . Let&#39;s do this for the DETE dataset first. The separationtype column explains the reason why the employee ceased to work at the institution. What values does this column contain? . dete[&quot;separationtype&quot;].value_counts() . Age Retirement 285 Resignation-Other reasons 150 Resignation-Other employer 91 Resignation-Move overseas/interstate 70 Voluntary Early Retirement (VER) 67 Ill Health Retirement 61 Other 49 Contract Expired 34 Termination 15 Name: separationtype, dtype: int64 . It looks like the values that are relevant to us are the ones that start with &quot;Resignation.&quot; We&#39;ll keep the rows which indicate resignation and drop the rest. . dete = dete.loc[ # Check if the string starts with &quot;Resignation.&quot; dete[&quot;separationtype&quot;].str.startswith(&quot;Resignation&quot;) ] dete.shape . (311, 35) . After we dropped non-resignation rows, the DETE dataset was left with 311 rows. . Next, we&#39;ll do the same for the TAFE dataset. What are the values in its separationtype column? . tafe[&quot;separationtype&quot;].value_counts() . Resignation 340 Contract Expired 127 Retrenchment/ Redundancy 104 Retirement 82 Transfer 25 Termination 23 Name: separationtype, dtype: int64 . This time, there is only one value which indicates resignation. We&#39;ll use that to identify the rows to keep. . tafe = tafe.loc[ # Check if the value is &quot;Resignation.&quot; tafe[&quot;separationtype&quot;] == &quot;Resignation&quot; ] tafe.shape . (340, 23) . Dropping rows resulted in having 340 rows left in the TAFE dataset. . Now, both datasets have been narrowed down to data about employees who intentionally left the institutions. . Date Columns . Next, we&#39;ll clean and inspect date columns. Specifically, these are: . DETE dataset dete_start_date: The date when the employee started to work at DETE. | cease_date: The date when the employee ceased to work at DETE. | . | TAFE dataset cease_date: The date when the employee ceased to work at TAFE. | . | . Let&#39;s start with DETE&#39;s cease_date. What are its values? . dete[&quot;cease_date&quot;].value_counts() . 2012 126 2013 74 01/2014 22 12/2013 17 06/2013 14 09/2013 11 07/2013 9 11/2013 9 10/2013 6 08/2013 4 05/2013 2 05/2012 2 2010 1 09/2010 1 07/2006 1 07/2012 1 Name: cease_date, dtype: int64 . We can see that some values only contain years, and others state a month before the year. Since we can&#39;t assume the month for entries without one, we will remove all of the months. Only the years will remain, and we will store them as numerical data. . dete[&quot;cease_date&quot;] = ( dete[&quot;cease_date&quot;] .str.extract(&quot;(20[0-1][0-9])&quot;) # Extract year using a regular expression .astype(np.float64) # Turn years into decimals ) dete[&quot;cease_date&quot;].value_counts().sort_index() . 2006.0 1 2010.0 2 2012.0 129 2013.0 146 2014.0 22 Name: cease_date, dtype: int64 . The DETE cease_date column now only contains year values. These range from 2006 to 2014. . Next, let&#39;s look at DETE&#39;s dete_start_date column. . dete[&quot;dete_start_date&quot;].value_counts().sort_index() . 1963 1 1971 1 1972 1 1973 1 1974 2 1975 1 1976 2 1977 1 1980 5 1982 1 1983 2 1984 1 1985 3 1986 3 1987 1 1988 4 1989 4 1990 5 1991 4 1992 6 1993 5 1994 6 1995 4 1996 6 1997 5 1998 6 1999 8 2000 9 2001 3 2002 6 2003 6 2004 14 2005 15 2006 13 2007 21 2008 22 2009 13 2010 17 2011 24 2012 21 2013 10 Name: dete_start_date, dtype: int64 . The column contains only years, no months, so it is quite clean. Most of the values are from 2004 to 2013. The years in the late 1900&#39;s don&#39;t seem like outliers because there are many values spread throughout those years. . It also makes sense that there are no dete_start_date values after 2014, since the latest cease_date is 2014. . There&#39;s no need to clean this column, but let&#39;s convert it to numerical data for consistency. . dete[&quot;dete_start_date&quot;] = dete[&quot;dete_start_date&quot;].astype(np.float64) . Lastly, let&#39;s look at the TAFE dataset&#39;s cease_date column. . tafe[&quot;cease_date&quot;].value_counts().sort_index() . 2009.0 2 2010.0 68 2011.0 116 2012.0 94 2013.0 55 Name: cease_date, dtype: int64 . The data here looks like it&#39;s already clean. The years are expressed as decimals, and there are no outliers to clean up. This column won&#39;t be changed. . The TAFE cease_date years range from 2009 to 2013. DETE&#39;s cease_date values range from 2006 to 2014. Therefore, both datasets give information about roughly the same period in time. . Years of Service . Remember that one of the goals of the project is to compare dissatisfaction rates between resignees who had worked for a short time and those who had worked for a longer time. Thus, we need to know how many years of service each employee has had. . The TAFE dataset already has a column called institute_service which gives information on this. . tafe[&quot;institute_service&quot;].value_counts().sort_index() . 1-2 64 11-20 26 3-4 63 5-6 33 7-10 21 Less than 1 year 73 More than 20 years 10 Name: institute_service, dtype: int64 . These values are somewhat difficult to use since these indicate ranges of years of service. This will be dealt with later, but for now, we have to make a matching column in the DETE dataset. . In order to do this, we will subtract dete_start_date from cease_date. This will result in the number of years that each employee has spent working at DETE. The new column will be called institute_service like in the TAFE dataset. . dete[&quot;institute_service&quot;] = dete[&quot;cease_date&quot;] - dete[&quot;dete_start_date&quot;] dete[&quot;institute_service&quot;].value_counts(bins = 10) . (-0.05, 4.9] 92 (4.9, 9.8] 75 (9.8, 14.7] 30 (14.7, 19.6] 26 (19.6, 24.5] 24 (24.5, 29.4] 8 (29.4, 34.3] 8 (34.3, 39.2] 7 (39.2, 44.1] 2 (44.1, 49.0] 1 Name: institute_service, dtype: int64 . It can be seen that DETE employees&#39; years of service range from under 4.9 to over 44.1. Let&#39;s view the distribution in a histogram. . sns.histplot( data = dete, x = &quot;institute_service&quot;, ) plt.title(&quot;DETE Employees&#39; Years of Service&quot;) plt.xlabel(&quot;Years of Service&quot;) plt.grid(True) plt.show() . The distribution is right-skewed. Most employees who resigned had worked at DETE for under 10 years. . tafe[&quot;dissatisfaction&quot;] = ( tafe[[&quot;Contributing Factors. Dissatisfaction&quot;, &quot;Contributing Factors. Job Dissatisfaction&quot;]] .any(axis = 1, skipna = False) ) tafe[&quot;dissatisfaction&quot;].value_counts(dropna = False) . - 277 Contributing Factors. Dissatisfaction 55 NaN 8 Name: dissatisfaction, dtype: int64 . &quot;Contributing Factors&quot; Columns . The Contributing Factors columns are about factors which may have influenced the employee&#39;s choice to resign. In the TAFE dataset, these columns have hyphen (&quot;-&quot;) values. This leads us to wonder what they represent, and whether or not we have to clean them. . Let&#39;s look at column 5, one of the columns with hyphens. . tafe.iloc[:, 5].value_counts(dropna = False) . - 284 Career Move - Public Sector 48 NaN 8 Name: Contributing Factors. Career Move - Public Sector , dtype: int64 . Most of the values are hyphens. The rest are null values or &quot;Career Move - Public Sector&quot;. . We can infer that the Contributing Factors group of columns represent options in a checkbox item in the survey. That&#39;s why each column only has 2 valid values: . &quot;-&quot; means that the option was not selected. | &quot;Career Move - Public Sector&quot; means that the option was selected. | . Thus, we can change the values in column 5 into True (selected) and False (not selected) for ease of use. . def identify_selection(value): if pd.isnull(value): return np.nan else: return value != &quot;-&quot; # Apply the function elementwise. tafe.iloc[:, 5] = tafe.iloc[:, 5].apply(identify_selection) tafe.iloc[:, 5].value_counts(dropna = False) . False 284 True 48 NaN 8 Name: Contributing Factors. Career Move - Public Sector , dtype: int64 . Now, the column only has True, False, and NaN values. . Let us apply this transformaton to the entire group of &quot;Contributing Factors&quot; columns (5 to 16). . tafe.iloc[:, 6:17] = tafe.iloc[:, 6:17].applymap(identify_selection) tafe.iloc[:, 5:17].head() . Contributing Factors. Career Move - Public Sector Contributing Factors. Career Move - Private Sector Contributing Factors. Career Move - Self-employment Contributing Factors. Ill Health Contributing Factors. Maternity/Family Contributing Factors. Dissatisfaction Contributing Factors. Job Dissatisfaction Contributing Factors. Interpersonal Conflict Contributing Factors. Study Contributing Factors. Travel Contributing Factors. Other Contributing Factors. NONE . 3 False | False | False | False | False | False | False | False | False | True | False | False | . 4 False | True | False | False | False | False | False | False | False | False | False | False | . 5 False | False | False | False | False | False | False | False | False | False | True | False | . 6 False | True | False | False | True | False | False | False | False | False | True | False | . 7 False | False | False | False | False | False | False | False | False | False | True | False | . Since these columns are now boolean columns, these will be easier to use in analysis. . It is worth noting that what we did matches the format used for Contributing Factors columns in DETE: . dete.iloc[:5, 10:28] . career_move_to_public_sector career_move_to_private_sector interpersonal_conflicts job_dissatisfaction dissatisfaction_with_the_department physical_work_environment lack_of_recognition lack_of_job_security work_location employment_conditions maternity/family relocation study/travel ill_health traumatic_incident work_life_balance workload none_of_the_above . 3 False | True | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | . 5 False | True | False | False | False | False | False | False | False | True | True | False | False | False | False | False | False | False | . 8 False | True | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | . 9 False | False | True | True | True | False | False | False | False | False | False | False | False | False | False | False | False | False | . 11 False | False | False | False | False | False | False | False | False | False | True | True | False | False | False | False | False | False | . Thus, we will be able to use similar techniques to identify dissatisfaction in both datasets. . Identifying Dissatisfaction . This project focuses on employees who resigned due to dissatisfaction with their work at the government institute. Thus, we have to identify which employees were dissatisfied. . In the TAFE dataset, the following columns indicate dissatisfaction: . Contributing Factors. Dissatisfaction | Contributing Factors. Job Dissatisfaction | . We will create a new dissatisfaction column. It will be a boolean column that contains True if at least 1 of the above columns has a value of True. . tafe[&quot;dissatisfaction&quot;] = ( tafe[[ &quot;Contributing Factors. Dissatisfaction&quot;, &quot;Contributing Factors. Job Dissatisfaction&quot;, ]] .any(axis = 1, skipna = False) ) tafe[&quot;dissatisfaction&quot;].value_counts(dropna = False) . False 241 True 91 NaN 8 Name: dissatisfaction, dtype: int64 . It looks like 91 employees in the TAFE dataset resigned due to dissatisfaction. . As for the DETE dataset, there are many relevant columns. . job_dissatisfaction | dissatisfaction_with_the_department | physical_work_environment | lack_of_recognition | lack_of_job_security | work_location | employment_conditions | work_life_balance | workload | . Remember that in the survey, these phrases were options in a checkbox item asking why the employee left. A True value in any of these columns means that the employee considered it a reason why he/she resigned. . We will create a new dissatisfaction column in the DETE dataset in the same way as we did for the TAFE dataset. If at least 1 of the above columns is True, the corresponding value in the new column will be True. . dete[&quot;dissatisfaction&quot;] = ( dete[[ &#39;job_dissatisfaction&#39;, &#39;dissatisfaction_with_the_department&#39;, &#39;physical_work_environment&#39;, &#39;lack_of_recognition&#39;, &#39;lack_of_job_security&#39;, &#39;work_location&#39;, &#39;employment_conditions&#39;, &#39;work_life_balance&#39;, &#39;workload&#39;, ]] .any(axis = 1, skipna = False) ) dete[&quot;dissatisfaction&quot;].value_counts(dropna = False) . False 162 True 149 Name: dissatisfaction, dtype: int64 . The results show that 149 of DETE employees who resigned had been dissatisfied. . Combining DETE and TAFE . At this point, the data cleaning we&#39;ve done is sufficient for us to combine the two datasets. We will stack them vertically; they will share columns with identical names. . We will still need to be able to differentiate between DETE and TAFE employees, so we will indicate this in a new column. . In the DETE dataset, the new institute column will contain the string &quot;DETE&quot;. . dete[&quot;institute&quot;] = &quot;DETE&quot; dete[&quot;institute&quot;].head() . 3 DETE 5 DETE 8 DETE 9 DETE 11 DETE Name: institute, dtype: object . The value in the TAFE dataset will be &quot;TAFE&quot;. . tafe[&quot;institute&quot;] = &quot;TAFE&quot; tafe[&quot;institute&quot;].head() . 3 TAFE 4 TAFE 5 TAFE 6 TAFE 7 TAFE Name: institute, dtype: object . Let&#39;s now concatenate the 2 datasets vertically. . combined = pd.concat( [dete, tafe], axis = 0, # Vertical concatenation ignore_index = True, ) print(combined.shape) combined.head() . (651, 53) . id separationtype cease_date dete_start_date role_start_date position classification region business_unit employment_status ... Contributing Factors. Ill Health Contributing Factors. Maternity/Family Contributing Factors. Dissatisfaction Contributing Factors. Job Dissatisfaction Contributing Factors. Interpersonal Conflict Contributing Factors. Study Contributing Factors. Travel Contributing Factors. Other Contributing Factors. NONE role_service . 0 4.0 | Resignation-Other reasons | 2012.0 | 2005.0 | 2006 | Teacher | Primary | Central Queensland | NaN | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 6.0 | Resignation-Other reasons | 2012.0 | 1994.0 | 1997 | Guidance Officer | NaN | Central Office | Education Queensland | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 9.0 | Resignation-Other reasons | 2012.0 | 2009.0 | 2009 | Teacher | Secondary | North Queensland | NaN | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 10.0 | Resignation-Other employer | 2012.0 | 1997.0 | 2008 | Teacher Aide | NaN | NaN | NaN | Permanent Part-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 12.0 | Resignation-Move overseas/interstate | 2012.0 | 2009.0 | 2009 | Teacher | Secondary | Far North Queensland | NaN | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 53 columns . The combined dataset has 651 rows (employees) and 53 columns. . Given our research questions, the most important columns to keep are institute_service, age, and dissatisfaction. We would only keep the other columns if we wanted to group the employees on certain characteristics. . Thus, we will remove columns that have under 500 non-null values. These columns wouldn&#39;t have enough useful information for us to use. . combined.dropna( axis = 1, thresh = 500, inplace = True, ) print(combined.shape) combined.head() . (651, 10) . id separationtype cease_date position employment_status gender age institute_service dissatisfaction institute . 0 4.0 | Resignation-Other reasons | 2012.0 | Teacher | Permanent Full-time | Female | 36-40 | 7.0 | False | DETE | . 1 6.0 | Resignation-Other reasons | 2012.0 | Guidance Officer | Permanent Full-time | Female | 41-45 | 18.0 | True | DETE | . 2 9.0 | Resignation-Other reasons | 2012.0 | Teacher | Permanent Full-time | Female | 31-35 | 3.0 | False | DETE | . 3 10.0 | Resignation-Other employer | 2012.0 | Teacher Aide | Permanent Part-time | Female | 46-50 | 15.0 | True | DETE | . 4 12.0 | Resignation-Move overseas/interstate | 2012.0 | Teacher | Permanent Full-time | Male | 31-35 | 3.0 | False | DETE | . Only 10 columns were left in the dataset. The 3 most important columns were kept, along with a few columns about useful demographic data. . Cleaning Age Data . One of the goals of this project involves the age data. Thus, we need to ensure that this data is clean. . Let us view the unique values in the column. . combined[&quot;age&quot;].value_counts().sort_index() . 20 or younger 10 21 25 33 21-25 29 26 30 32 26-30 35 31 35 32 31-35 29 36 40 32 36-40 41 41 45 45 41-45 48 46 50 39 46-50 42 51-55 71 56 or older 29 56-60 26 61 or older 23 Name: age, dtype: int64 . We can see that: . All values represent a range of ages. | Some have hyphens, and others have spaces. | A few are phrases that say &quot;or younger&quot; or &quot;or older.&quot; | . Therefore, we will clean the data by defining the following function and applying it elementwise to the column: . def fix_age(text): if pd.isnull(text): result = np.nan elif &quot; &quot; in text: result = text.replace(&quot; &quot;, &quot;-&quot;) elif text in [&quot;56-60&quot;, &quot;61 or older&quot;]: result = &quot;56 or older&quot; else: result = text return result combined[&quot;age&quot;] = combined[&quot;age&quot;].apply(fix_age) combined[&quot;age&quot;].value_counts().sort_index() . 20 or younger 10 21-25 62 26-30 67 31-35 61 36-40 73 41-45 93 46-50 81 51-55 71 56 or older 78 Name: age, dtype: int64 . Now, the values in the age column are consistent. . Categorizing Years of Service . Remember that DETE and TAFE differed in the format of their institute_service columns: . DETE had decimal numbers representing the number of years. | TAFE had strings, each of which represented a range of years. | . We could take the TAFE ranges and replace them with the middle value of each range. However, this would make the data inaccurate. . Instead, we will transform the data to become more general. We&#39;ll group the data into categories which represent ranges of years. This will apply to the entire institute_service column in the combined dataset. . The article &quot;Age is Just a Number: Engage Employees by Career Stage, Too&quot; states that employee engagement can be better understood in the context of career stage, i.e., the number of years working at the company. Career stage influences an employee&#39;s work attitude and virtues that they value in the workplace. . We can also say that career stage influences employees&#39; decisions to resign. The most obvious example of this is that a new employee isn&#39;t very invested in the company and would be more likely to leave due to initial dissatisfaction. . The article gives the following 4 career stages: . Newbie (0 to 3 years) | Sophomore (3 to 7 years) | Tenured (7 to 11 years) | Sage (11 or more years) | . We will use the above career stages as categories in the institute_service column. But first, we have to be prepared for what kinds of values we will have to transform. . combined[&quot;institute_service&quot;].unique() . array([7.0, 18.0, 3.0, 15.0, 14.0, 5.0, nan, 30.0, 32.0, 39.0, 17.0, 9.0, 6.0, 1.0, 35.0, 38.0, 36.0, 19.0, 4.0, 26.0, 10.0, 8.0, 2.0, 0.0, 23.0, 13.0, 16.0, 12.0, 21.0, 20.0, 24.0, 33.0, 22.0, 28.0, 49.0, 11.0, 41.0, 27.0, 42.0, 25.0, 29.0, 34.0, 31.0, &#39;3-4&#39;, &#39;7-10&#39;, &#39;1-2&#39;, &#39;Less than 1 year&#39;, &#39;11-20&#39;, &#39;5-6&#39;, &#39;More than 20 years&#39;], dtype=object) . The values above include: . Decimal numbers | A range of years (&quot;x-y&quot;) | A phrase describing a range of years (&quot;Less than x years&quot;) | . Extracting Numbers . Based on the values present, we will extract the number of years using the regular expression ([0-9]{1,2}). This will capture the first 1 or 2 digit whole number in a string. . For decimals, the part of the number before the decimal point will be extracted. | For ranges of years, the minimum year will be extracted. | For phrases, the first number to appear will be extracted. | . combined[&quot;service_num&quot;] = ( combined[&quot;institute_service&quot;] .astype(str) # Convert to strings. .str.extract(&quot;([0-9]{1,2})&quot;) # Extract 1 or 2-digit number. .astype(np.float64) # Convert to decimals. ) combined[&quot;service_num&quot;].unique() . array([ 7., 18., 3., 15., 14., 5., nan, 30., 32., 39., 17., 9., 6., 1., 35., 38., 36., 19., 4., 26., 10., 8., 2., 0., 23., 13., 16., 12., 21., 20., 24., 33., 22., 28., 49., 11., 41., 27., 42., 25., 29., 34., 31.]) . This worked for the decimals, as seen below. The initial and final values are identical. . combined.loc[ combined[&quot;institute_service&quot;].astype(str).str.contains(&quot;.&quot;), [&quot;institute_service&quot;, &quot;service_num&quot;] ].head() . institute_service service_num . 0 7.0 | 7.0 | . 1 18.0 | 18.0 | . 2 3.0 | 3.0 | . 3 15.0 | 15.0 | . 4 3.0 | 3.0 | . We can tell that this worked for the ranges because the minimum value was extracted. For example, for &quot;3-4&quot;, the number 3 was extracted. . combined.loc[ combined[&quot;institute_service&quot;].astype(str).str.contains(&quot;-&quot;), [&quot;institute_service&quot;, &quot;service_num&quot;] ].head() . institute_service service_num . 312 3-4 | 3.0 | . 313 7-10 | 7.0 | . 314 3-4 | 3.0 | . 315 3-4 | 3.0 | . 316 3-4 | 3.0 | . It also worked for the phrases, as seen below. The number in the phrase was extracted. . combined.loc[ combined[&quot;institute_service&quot;].astype(str).str.contains(&quot;than&quot;), [&quot;institute_service&quot;, &quot;service_num&quot;] ].head() . institute_service service_num . 318 Less than 1 year | 1.0 | . 329 Less than 1 year | 1.0 | . 332 More than 20 years | 20.0 | . 333 Less than 1 year | 1.0 | . 334 Less than 1 year | 1.0 | . Mapping Numbers to Categories . Now that we have numbers, we can map them to the career stages mentioned earlier: . Newbie (0 to 3 years) | Sophomore (3 to 7 years) | Tenured (7 to 11 years) | Sage (11 or more years) | . We&#39;ll do this by defining a function then applying it elementwise to the column. . def career_stage(years): if pd.isnull(years): stage = np.nan elif years &lt; 3.0: stage = &quot;Newbie&quot; elif years &lt; 7.0: stage = &quot;Sophomore&quot; elif years &lt; 11.0: stage = &quot;Tenured&quot; elif years &gt;= 11.0: stage = &quot;Sage&quot; return stage # Apply the function elementwise and make a new column. combined[&quot;service_cat&quot;] = combined[&quot;service_num&quot;].apply(career_stage) combined[&quot;service_cat&quot;].value_counts() . Newbie 193 Sophomore 172 Sage 136 Tenured 62 Name: service_cat, dtype: int64 . The results show that most of the employees who resigned were Newbies. . Data cleaning is now complete, so we can go to data analysis. . Data Analysis . Dissatisfaction by Age Group . First, we can investigate the dissatisfaction rates of resignees by their age group. . df = combined.dropna(subset = [&quot;dissatisfaction&quot;, &quot;age&quot;]).copy() # Cast booleans to integers. df[&quot;dissatisfaction&quot;] = df[&quot;dissatisfaction&quot;].astype(int) table_3 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;age&quot;], aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0. ).reset_index() table_3 . age dissatisfaction . 0 20 or younger | 0.200000 | . 1 21-25 | 0.306452 | . 2 26-30 | 0.417910 | . 3 31-35 | 0.377049 | . 4 36-40 | 0.342466 | . 5 41-45 | 0.376344 | . 6 46-50 | 0.382716 | . 7 51-55 | 0.422535 | . 8 56 or older | 0.423077 | . This table is visualized in the bar graph below. . sns.barplot( data = table_3, x = &quot;age&quot;, y = &quot;dissatisfaction&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Age Group&quot;) plt.xlabel(&quot;Age Group&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 30, ha = &quot;right&quot;) plt.grid(True) plt.show() . Notably, the dissatisfaction rate is: . Lowest (20%) among resignees aged 20 or younger. | Above 30% among resignees aged 21 or older. | Highest (42%) among resignees aged 56 or older, 51-55, or 26-30. | . Let&#39;s group the data further using the institute (DETE or TAFE). . table_4 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;age&quot;, &quot;institute&quot;], aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0. ).reset_index() table_4 . age institute dissatisfaction . 0 20 or younger | DETE | 0.000000 | . 1 20 or younger | TAFE | 0.222222 | . 2 21-25 | DETE | 0.310345 | . 3 21-25 | TAFE | 0.303030 | . 4 26-30 | DETE | 0.571429 | . 5 26-30 | TAFE | 0.250000 | . 6 31-35 | DETE | 0.551724 | . 7 31-35 | TAFE | 0.218750 | . 8 36-40 | DETE | 0.390244 | . 9 36-40 | TAFE | 0.281250 | . 10 41-45 | DETE | 0.479167 | . 11 41-45 | TAFE | 0.266667 | . 12 46-50 | DETE | 0.452381 | . 13 46-50 | TAFE | 0.307692 | . 14 51-55 | DETE | 0.593750 | . 15 51-55 | TAFE | 0.282051 | . 16 56 or older | DETE | 0.551020 | . 17 56 or older | TAFE | 0.206897 | . The table is visualized in the bar graph below. . sns.barplot( data = table_4, x = &quot;age&quot;, y = &quot;dissatisfaction&quot;, hue = &quot;institute&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Age Group&quot;) plt.xlabel(&quot;Age Group&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 30, ha = &quot;right&quot;) plt.grid(True) plt.show() . We can see that employees of DETE generally had much higher dissatisfaction rates than employees of TAFE across all age groups. . The exception is the &quot;20 or younger&quot; age group, which has a 0% dissatisfaction rate for DETE employees. However, this is due to the fact that only one resignee from DETE was 20 years old or younger. . ( combined .loc[combined[&quot;institute&quot;] == &quot;DETE&quot;, &quot;age&quot;] .value_counts() .sort_index() ) . 20 or younger 1 21-25 29 26-30 35 31-35 29 36-40 41 41-45 48 46-50 42 51-55 32 56 or older 49 Name: age, dtype: int64 . Thus, we can generally say that the dissatisfaction rate was much higher among DETE resignees compared to TAFE resignees. . Also, the peak dissatisfaction rates occurred in age groups around 26-30 years old and 51-55 years old for both institutes. . Dissatisfaction by Career Stage . Next, we will determine what percentage of the resignees was dissatisfied with work based on career stage. . df = combined.dropna(subset = [&quot;dissatisfaction&quot;, &quot;service_cat&quot;]).copy() # Cast booleans to integers. df[&quot;dissatisfaction&quot;] = df[&quot;dissatisfaction&quot;].astype(int) table_1 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;service_cat&quot;], aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0. ).reset_index() table_1 . service_cat dissatisfaction . 0 Newbie | 0.295337 | . 1 Sage | 0.485294 | . 2 Sophomore | 0.343023 | . 3 Tenured | 0.516129 | . The table is visualized in the figure below. . sns.barplot( data = table_1, x = &quot;service_cat&quot;, y = &quot;dissatisfaction&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Career Stage&quot;) plt.xlabel(&quot;Career Stage&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 0) plt.grid(True) plt.show() . Interestingly, the dissatisfaction rate is highest (around 50%) within the Tenured and Sage groups of resignees. This is surprising since these are the groups of employees who have been working at the institute for the longest time. . One explanation could be that they became dissatisfied because they spent so much time at the company without career growth or without sufficient variety in their work. . Next, we can group the data further by the specific institute of the employees: . table_2 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;service_cat&quot;, &quot;institute&quot;], # Group on institute too aggfunc = np.mean, ).reset_index() table_2 . service_cat institute dissatisfaction . 0 Newbie | DETE | 0.375000 | . 1 Newbie | TAFE | 0.262774 | . 2 Sage | DETE | 0.560000 | . 3 Sage | TAFE | 0.277778 | . 4 Sophomore | DETE | 0.460526 | . 5 Sophomore | TAFE | 0.250000 | . 6 Tenured | DETE | 0.609756 | . 7 Tenured | TAFE | 0.333333 | . This table is visualized below. . sns.barplot( data = table_2, x = &quot;service_cat&quot;, y = &quot;dissatisfaction&quot;, hue = &quot;institute&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Career Stage&quot;) plt.xlabel(&quot;Career Stage&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 0) plt.grid(True) plt.show() . The chart shows that the trend is generally consistent between DETE and TAFE. Tenured and Sage resignees have a higher dissatisfaction rate than other groups. . However, it looks like the dissatisfaction rates in DETE are also much higher than the rates in TAFE. It can be said that dissatisfaction influences the resignation of a greater percentage of people in DETE than it does in TAFE. . Conclusion . In this project, we worked with 2 datasets of employee exit survey data from the DETE and TAFE government institutes in Australia. We cleaned, transformed, and combined these datasets. Then, we analyzed dissatisfaction rates of resignees based on age and based on career stage. . We found the following notable points: . Dissatisfaction rate was highest among resignees in age groups around 26-30 years old and 51-55 years old for both institutes. | Dissatisfaction rate was highest among resignees who had been working at the institute for over 7 years. | The dissatisfaction rate was much higher among DETE resignees compared to TAFE resignees. This may have to do with the nature or conditions of the work of DETE employees. | . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/2021/06/01/Employee-Exit-Survey-Data-Cleaning-Aggregation.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/2021/06/01/Employee-Exit-Survey-Data-Cleaning-Aggregation.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
            "content": "Overview . &quot;Explanatory&quot; visualizations are created in order to explain something to other people. These are made to inform general, non-technical audiences. Thus, we want to make a chart with the following characteristics: . Eye-catching | Visually pleasing | Easy to understand | Has a clear main point, as opposed to being too detailed | . Put simply, it is similar to an infographic. However, instead of being a standalone image, it is usually put in an article with accompanying text. . In this project, I detail the process of designing an explanatory chart of USD-PHP exchange rates. I used the &quot;Forex data since 2011-1-1&quot; dataset, which was uploaded by user emrecanaltinsoy on Kaggle. By the end of the project, I was able to make the following chart. . . . Tip: The larger implications of exchange rate trends are beyond the scope of this project. For Filipinos, I suggest reading &quot;[ANALYSIS] Why the stronger peso mirrors a weaker PH economy&quot; (Punongbayan 2020), which is an interesting recent article. . . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Storytelling Data Visualization on Exchange Rates. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib.style as style import matplotlib.lines as lines # Use matplotlib&#39;s default style. style.use(&quot;default&quot;) . Data Overview . The &quot;Forex data since 2011-1-1&quot; dataset contains USD exchange rates for various currencies, each in a different column. Every row corresponds to one day, and the dataset has been updated every day since January 1, 2011. My copy was downloaded on May 27, 2021. . Below are the first 5 rows of the dataset. . exchange = pd.read_csv(&quot;./2021-05-28-MEC-Files/forex_usd_data.csv&quot;) exchange.head() . date(y-m-d) Argentine Peso Australian Dollar Bahraini Dinar Botswana Pula Brazilian Real Bruneian Dollar Bulgarian Lev Canadian Dollar Chilean Peso ... Sri Lankan Rupee Swedish Krona Swiss Franc Taiwan New Dollar Thai Baht Trinidadian Dollar Turkish Lira Emirati Dirham British Pound Venezuelan Bolivar . 0 2011-01-01 | 3.9690 | 0.977326 | 0.377050 | 6.472492 | 1.659500 | 1.284500 | 1.463830 | 0.997700 | 467.750000 | ... | 110.940002 | 6.721450 | 0.934500 | 29.140000 | 30.020000 | 6.34 | 1.537400 | 3.67310 | 0.640553 | 4.3 | . 1 2011-01-02 | 3.9690 | 0.977326 | 0.377050 | 6.472492 | 1.659500 | 1.283500 | 1.463830 | 0.997700 | 467.750000 | ... | 110.940002 | 6.721450 | 0.933800 | 29.099001 | 30.020000 | 6.34 | 1.537400 | 3.67310 | 0.641067 | 4.3 | . 2 2011-01-03 | 3.9735 | 0.980569 | 0.377055 | 6.472492 | 1.646288 | 1.284367 | 1.462799 | 0.990444 | 465.649994 | ... | 110.919998 | 6.693788 | 0.933069 | 29.120000 | 30.084999 | 6.39 | 1.557411 | 3.67320 | 0.645615 | 4.3 | . 3 2011-01-04 | 3.9710 | 0.995580 | 0.377060 | 6.480881 | 1.666747 | 1.287438 | 1.469525 | 0.999076 | 487.850006 | ... | 110.820000 | 6.726967 | 0.947903 | 29.175004 | 30.104903 | 6.36 | 1.547801 | 3.67315 | 0.641558 | 4.3 | . 4 2011-01-05 | 3.9715 | 0.999522 | 0.377050 | 6.548788 | 1.670312 | 1.291450 | 1.485031 | 0.994376 | 495.149993 | ... | 110.820000 | 6.766127 | 0.964490 | 29.170000 | 30.216193 | 6.38 | 1.543853 | 3.67310 | 0.645308 | 4.3 | . 5 rows × 54 columns . We are only interested in USD-PHP exchange rates, so we will take the &quot;Philippine Peso&quot; column. . php = exchange[[&quot;date(y-m-d)&quot;, &quot;Philippine Peso&quot;]].copy() php.head() . date(y-m-d) Philippine Peso . 0 2011-01-01 | 43.639999 | . 1 2011-01-02 | 43.639999 | . 2 2011-01-03 | 43.799999 | . 3 2011-01-04 | 43.550002 | . 4 2011-01-05 | 43.900002 | . Below is more information on the 2 columns. . php.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3798 entries, 0 to 3797 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 date(y-m-d) 3798 non-null object 1 Philippine Peso 3798 non-null float64 dtypes: float64(1), object(1) memory usage: 59.5+ KB . The dataset has 3798 entries and 2 columns. There are no missing values. . The date column contains text, whereas the PHP column contains decimal numbers. . Data Cleaning . Column Labels . The column labels will first be changed so that these are easier to use. . php.columns = [&quot;date&quot;, &quot;usd_php&quot;] php.head() . date usd_php . 0 2011-01-01 | 43.639999 | . 1 2011-01-02 | 43.639999 | . 2 2011-01-03 | 43.799999 | . 3 2011-01-04 | 43.550002 | . 4 2011-01-05 | 43.900002 | . Date Column . The date column contains text with the format {4 digit year}-{2 digit month}-{2 digit day}. . Below, I convert the text to datetime objects for ease of use. . php[&quot;date&quot;] = pd.to_datetime(php[&quot;date&quot;]) php.sort_values( by = &quot;date&quot;, ascending = True, inplace = True, ) php[&quot;date&quot;] . 0 2011-01-01 1 2011-01-02 2 2011-01-03 3 2011-01-04 4 2011-01-05 ... 3793 2021-05-21 3794 2021-05-22 3795 2021-05-23 3796 2021-05-24 3797 2021-05-25 Name: date, Length: 3798, dtype: datetime64[ns] . Descriptive Statistics . Before we can clean the data, we have to view its descriptive statistics. . php.describe(datetime_is_numeric = True) . date usd_php . count 3798 | 3798.000000 | . mean 2016-03-13 12:00:00 | 47.010148 | . min 2011-01-01 00:00:00 | 3.094050 | . 25% 2013-08-07 06:00:00 | 43.639330 | . 50% 2016-03-13 12:00:00 | 46.890740 | . 75% 2018-10-18 18:00:00 | 50.584238 | . max 2021-05-25 00:00:00 | 54.323583 | . std NaN | 3.920411 | . As expected, the dates range from November 1, 2011 to May 25, 2021. . However, the minimum USD-PHP exchange rate in the data is 3.09. This is very low compared to the other percentiles. . Exchange Rate Outliers . There may be some outliers in the data. We can confirm this using a boxplot. . sns.boxplot( data = php, y = &quot;usd_php&quot; ) plt.title(&quot;USD-PHP Exchange Rate Distribution&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . Indeed, most of the values fall between 40 and 55, whereas a few outliers exist below 10. These may be inaccurate data. . How many outliers are there? . (php[&quot;usd_php&quot;] .value_counts(bins = 10) .sort_index() ) . (3.0420000000000003, 8.217] 2 (8.217, 13.34] 0 (13.34, 18.463] 0 (18.463, 23.586] 0 (23.586, 28.709] 0 (28.709, 33.832] 0 (33.832, 38.955] 0 (38.955, 44.078] 1157 (44.078, 49.201] 1289 (49.201, 54.324] 1350 Name: usd_php, dtype: int64 . There are only 2 values less than 10. It is highly unlikely that these values are accurate. This can be shown using a line chart. . sns.lineplot( data = php, x = &quot;date&quot;, y = &quot;usd_php&quot;, ) plt.title(&quot;USD-PHP Exchange Rate Over Time&quot;) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . The outliers exist somewhere in the 2014 data, and they create an unnatural dip in the chart. . Thus, the inaccurate datapoints will be dropped from the dataset. . php = php.loc[php[&quot;usd_php&quot;] &gt; 10] php.describe(datetime_is_numeric = True) . date usd_php . count 3796 | 3796.000000 | . mean 2016-03-13 22:32:45.015806208 | 47.033285 | . min 2011-01-01 00:00:00 | 40.500000 | . 25% 2013-08-06 18:00:00 | 43.639981 | . 50% 2016-03-14 12:00:00 | 46.892504 | . 75% 2018-10-19 06:00:00 | 50.584680 | . max 2021-05-25 00:00:00 | 54.323583 | . std NaN | 3.789566 | . The minimum USD-PHP rate is now 40.5, which makes more sense. . Exploratory Data Analysis . Basic Line Chart . First, we start with a basic line chart that shows the exchange rates on all days in the dataset. . sns.lineplot( data = php, x = &quot;date&quot;, y = &quot;usd_php&quot;, ) plt.title(&quot;USD-PHP Exchange Rate Over Time&quot;) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . The chart shows that the exchange rate dropped to under 41.0 in 2013. It then steadily climbed up to over 54.0 in 2018 before moving down again afterwards. . For Filipinos, it is ideal for the USD-PHP rate to be lower so that the peso has more power. Thus, it can be said that the exchange rate was better from 2011 to 2015 compared to how it has been in recent years. However, note that the USD-PHP exchange rate is not the only descriptor of the Philippines&#39; economy. . Rolling Average . In order to focus more on general trends than small fluctuations, we can graph the rolling average (or moving average). The rolling average is taken by replacing each datapoint with the mean average of a certain number of the datapoints leading up to it. . Using a rolling average can make a graph look visually cleaner and make general trends easier to see. . The number of datapoints used in each average is called the rolling window. This can be specified in Pandas using pd.Series.rolling(). Below, we use a rolling window of 182 days (around half a year) in order to transform the rate data. . php[&quot;rolling&quot;] = php[&quot;usd_php&quot;].rolling(182).mean() php.tail() . date usd_php rolling . 3793 2021-05-21 | 47.919983 | 48.237840 | . 3794 2021-05-22 | 47.929517 | 48.236031 | . 3795 2021-05-23 | 47.929517 | 48.234226 | . 3796 2021-05-24 | 48.068599 | 48.232923 | . 3797 2021-05-25 | 48.155798 | 48.232809 | . The last 5 rows of the dataset are shown above. The rolling averages are not exactly equal to the original numbers, but these are close enough to show the same trend. . A line chart of the rolling averages is shown below. . sns.lineplot( data = php, x = &quot;date&quot;, y = &quot;rolling&quot;, ) plt.title(&quot;USD-PHP Exchange Rate Over Time: Rolling Average&quot;) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . The same trends mentioned earlier are clearer to see in the above chart. . Explanatory Chart . One idea of a useful chart would be to compare the USD-PHP exchange rate trends before and during the COVID-19 pandemic. . Note: A similar idea was suggested in the Dataquest guided project. However, I did not look at Dataquest&#8217;s solution notebook. I wrote my code on my own. . Key Concepts . The following concepts will be used throughout the process of designing the chart. I learned these from Dataquest&#39;s &quot;Data Scientist in Python&quot; course. . Familiarity . Audiences prefer familiar charts since they can understand these easily. Therefore, it is better to use a basic chart as a template than to use something obscure or create something entirely new. . In the case of showing USD-PHP exchange rates over time, it is best to use a basic line chart as a template. . Data-Ink Ratio . When making an explanatory chart, one must maximize the data-ink ratio. . Data refers to the elements that represent data and its relationships, like bars and lines. | Ink refers to the total amount of ink that the chart would use if it were printed on paper. | . Maximizing the data-ink ratio means focusing more on data-related elements and minimizing the use of other, less important elements. . This helps the audience understand the main point without being distracted by other details. . Gestalt Psychology . Gestalt psychology is founded on the idea that people tend to see patterns rather than individual objects. . Under Gestalt psychology, there are several Principles of Grouping. These are ways to visually group elements together. . Proximity: Elements are close to each other | Similarity: Elements are similar due to color, shape, etc. | Enclosure: Elements are enclosed in an outer shape, like a rectangle | Connection: Elements are connected by a form, usually a line | . When designing charts, these are helpful in implying relationships between elements instead of explicitly stating them. . Visual Style . Before we start making the chart, we have to choose a style. . I chose Matplotlib&#39;s built-in &quot;fivethirtyeight&quot; style. It&#39;s based on the style of the charts used on the FiveThirtyEight website by Nate Silver. . Additionally, I used color-hex.com to get hex codes for specific kinds of blue, orange, and dark gray that I want to use in my chart. . style.use(&quot;fivethirtyeight&quot;) # Color hex codes c_blue = &quot;#14c4dd&quot; c_orange = &quot;#ffa500&quot; c_dark_gray = &quot;#d2d2d2&quot; . Setting up Subplots . In order to make the chart fresh and interesting, we have to make it more complex than 1 plot with a line chart. In our case, I have the following ideas: . Show 2 line charts, one on top of the other. | The upper chart shows how the rate changed from 2011 to 2021. | The lower chart zooms into the pandemic portion of the line chart from 2020 to 2021. | . In order to do this, I will create a Matplotlib Figure with two Axes (subplots), as described above. . # There is an upper and lower subplot. fig, (ax1, ax2) = plt.subplots( nrows = 2, ncols = 1, figsize = (10, 10), # 10 inches x 10 inches dpi = 80, ) . The Upper Subplot . Next, we design the upper subplot. Here&#39;s what it looks like with a basic line chart of the raw exchange rate data. . ax1.plot( php[&quot;date&quot;], php[&quot;usd_php&quot;], color = c_blue, ) fig . The line above looks messy. For the general audience, the overall trends are more important than the specific daily values. Thus, we will use the rolling average to make the chart cleaner. . ax1.clear() ax1.plot( php[&quot;date&quot;], php[&quot;rolling&quot;], color = c_blue, ) fig . Additionally, we will split the line into: . The pre-pandemic portion (blue) | The pandemic portion (orange) | . The pandemic portion will also be enclosed in a dark gray box, in order to further separate it from the pre-pandemic portion. . ax1.clear() # Main line chart (2011-2019) php_pre = php.loc[php[&quot;date&quot;].dt.year &lt; 2020] ax1.plot( php_pre[&quot;date&quot;], php_pre[&quot;rolling&quot;], color = c_blue, ) # Pandemic part of line chart (2020-2021) php_pandemic = php.loc[php[&quot;date&quot;].dt.year.between(2020, 2021)] ax1.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;rolling&quot;], color = c_orange, ) # Special background for pandemic portion ax1.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 1, ) fig . Next, we want to maximize the data-ink ratio by removing unnecessary elements. We will do the following: . On the x-axis, show only the labels for 2012, 2016, and 2020. | On the y-axis, show only the labels for 48, 50, and 52. | Remove grid lines. | . We will also add 1 grid line at y = 50 so that it can guide viewers. It would be particularly helpful for Filipino viewers since they commonly think that USD 1 = PHP 50. . ax1.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels ax1.set_xticks([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_xticklabels([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_yticks([48, 50, 52]) ax1.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax1.grid(False) # Horizontal line at y = 50 ax1.axhline(50, linewidth = 1, color = &quot;gray&quot;) fig . Now, the upper subplot is much cleaner; there is less visual noise. . The last step for the upper subplot would be to add informative text: . &quot;Pre-Pandemic&quot; label for the blue line | A comment about the upward trend leading up to 2018 | . ax1.text( x = &quot;2013-07&quot;, y = 48, s = &quot;Pre-Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;center&quot;, va = &quot;center&quot;, ) # Comment on upward trend ax1.text( x = &quot;2018-10&quot;, y = 46, s = &quot;Rate climbs up to 54 nin Oct 2018&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) fig . The &quot;Pre-Pandemic&quot; label was set to gray because it is a structural element; it is less important. . On the other hand, the long comment was set to black because it states a statistic from the data and helps tell a story about the data. It is more important, so it should be darker. . We have finished designing the upper subplot. . Lower Subplot . Next, the lower subplot will zoom in on the pandemic portion of the data, which is from 2020 to 2021. Since this is the more important part, we should use the raw data for more detail. . ax2.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;usd_php&quot;], color = c_orange, ) fig . Then, we will make changes similar to the ones done for the upper subplot: . On the x-axis, show only the labels for January 2020, July 2020, and January 2021. | On the y-axis, show only the labels for 48, 50, and 52. | Remove grid lines. | Add 1 grid line at y = 50. | Enclose the entire line in a dark gray box. | . ax2.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels. ax2.set_xticks([&quot;2020-01&quot;, &quot;2020-07&quot;, &quot;2021-01&quot;]) ax2.set_xticklabels([&#39;Jan 2020&#39;, &#39;Jul 2020&#39;, &#39;Jan 2021&#39;]) ax2.set_yticks([48, 50, 52]) ax2.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax2.grid(False) # Horizontal line at y = 50 ax2.axhline(50, linewidth = 1, color = &quot;gray&quot;) # Special background for pandemic portion ax2.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 2, ) fig . We are using the principles of enclosure and similarity to visually imply that the pandemic portion in the upper subplot is being shown more closely in the lower subplot. . Enclosure: dark gray boxes | Similarity: orange lines, horizontal grid line, y-axis labels | . Next, we add another comment in black text, this time about the downward trend leading up to the present day. . ax2.text( x = &quot;2021-01&quot;, y = 49.25, s = &quot;Rate drops down to 48 nin May 2021&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) fig . We have finished designing the lower subplot. However, we still need to add some finishing touches. . Figure-Level Customization . In this last step, we customize the chart on the level of the Matplotlib Figure. This involves both of the subplots and the space around them. . What we want to do is to use the principles of proximity and connection to make the relationship between the 2 subplots even clearer. . Proximity: Increase the space between the two subplots. | Connection: Draw a line connecting the two dark gray boxes. Add a &quot;COVID-19 Pandemic&quot; label next to the line. | . | . These are done in the code below. . fig.tight_layout(pad = 5) # Line connection between gray boxes fig.add_artist( lines.Line2D( xdata = [0.82, 0.82], ydata = [0.44, 0.585], color = c_dark_gray, alpha = 1, ) ) # &quot;COVID-19&quot; label between subplots fig.text( x = 0.8, y = 0.5, s = &quot;COVID-19 Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;right&quot;, va = &quot;center&quot;, ) fig . Now, when one first reads the chart, it is very clear that the gray boxes contain data about exchange rates in the COVID-19 pandemic. . The last touch would be to add a title and subtitle to the chart. Since the title is typically the first thing a viewer reads on a chart, it is best to state a statistic related to the data, like &quot;USD-PHP Rate Drops to 48 after 1 Year in the Pandemic.&quot; . This way, the title becomes a data element. The data-ink ratio is increased. . fig.text( x = 0.5, y = 0.95, s = &quot;USD-PHP Rate Drops to 48 after 1 Year in the Pandemic&quot;, size = 16, weight = &quot;bold&quot;, ha = &quot;center&quot;, va = &quot;center&quot;, ) # Subtitle fig.text( x = 0.5, y = 0.92, s = &quot;USD-PHP exchange rate over time&quot;, size = 12, ha = &quot;center&quot;, va = &quot;center&quot;, ) fig . That&#39;s it. The explanatory chart is complete. . Full Code . The full code to make the graph is shown below. The comments explain which part does what. . style.use(&quot;fivethirtyeight&quot;) # Color hex codes c_blue = &quot;#14c4dd&quot; c_orange = &quot;#ffa500&quot; c_dark_gray = &quot;#d2d2d2&quot; # Figure has 2 rows and 1 column. # There is an upper and lower subplot. fig, (ax1, ax2) = plt.subplots( nrows = 2, ncols = 1, figsize = (10, 10), dpi = 80, ) # UPPER subplot # Main line chart (2011-2019) php_pre = php.loc[php[&quot;date&quot;].dt.year &lt; 2020] ax1.plot( php_pre[&quot;date&quot;], php_pre[&quot;rolling&quot;], color = c_blue, ) # Pandemic part of line chart (2020-2021) php_pandemic = php.loc[php[&quot;date&quot;].dt.year.between(2020, 2021)] ax1.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;rolling&quot;], color = c_orange, ) # Special background for pandemic portion ax1.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 1, ) # Set tick label color to gray. ax1.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels ax1.set_xticks([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_xticklabels([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_yticks([48, 50, 52]) ax1.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax1.grid(False) # Horizontal line at y = 50 ax1.axhline(50, linewidth = 1, color = &quot;gray&quot;) # Text in upper subplot ax1.text( x = &quot;2018-10&quot;, y = 46, s = &quot;Rate climbs up to 54 nin Oct 2018&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) ax1.text( x = &quot;2013-07&quot;, y = 48, s = &quot;Pre-Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;center&quot;, va = &quot;center&quot;, ) # LOWER subplot # Pandemic portion, zoomed in ax2.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;usd_php&quot;], color = c_orange, ) # Set tick labels to gray. ax2.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels. ax2.set_xticks([&quot;2020-01&quot;, &quot;2020-07&quot;, &quot;2021-01&quot;]) ax2.set_xticklabels([&#39;Jan 2020&#39;, &#39;Jul 2020&#39;, &#39;Jan 2021&#39;]) ax2.set_yticks([48, 50, 52]) ax2.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax2.grid(False) # Horizontal line at y = 50 ax2.axhline(50, linewidth = 1, color = &quot;gray&quot;) # Special background for pandemic portion ax2.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 2, ) # Text in lower subplot ax2.text( x = &quot;2021-01&quot;, y = 49.25, s = &quot;Rate drops down to 48 nin May 2021&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) # FIGURE level customization # Add space between the subplots fig.tight_layout(pad = 5) # Line connection between pandemic parts fig.add_artist( lines.Line2D( xdata = [0.82, 0.82], ydata = [0.44, 0.585], color = c_dark_gray, alpha = 1, ) ) # Title with statistic fig.text( x = 0.5, y = 0.95, s = &quot;USD-PHP Rate Drops to 48 after 1 Year in the Pandemic&quot;, size = 16, weight = &quot;bold&quot;, ha = &quot;center&quot;, va = &quot;center&quot;, ) # Subtitle fig.text( x = 0.5, y = 0.92, s = &quot;USD-PHP exchange rate over time&quot;, size = 12, ha = &quot;center&quot;, va = &quot;center&quot;, ) # &quot;COVID-19&quot; label between subplots fig.text( x = 0.8, y = 0.5, s = &quot;COVID-19 Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;right&quot;, va = &quot;center&quot;, ) # Save chart locally. plt.savefig(&quot;./2021-05-28-MEC-Files/2021-05-28-explanatory-chart.png&quot;) # Show chart plt.show() . Conclusion . In this project, we cleaned and explored data about USD-PHP exchange rates over time. . We then discussed several key concepts in the creation of an explanatory chart, such as Familiarity, Data-Ink Ratio, and Gestalt Psychology. These concepts were applied throughout the process of making an explanatory chart that compared exchange rate trends before and during the pandemic. . We were ultimately able to create a chart that is simple and clean, yet eye-catching and informative. . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/2021/05/28/Making-Explanatory-Chart-USD-PHP-Exchange-Rates.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/2021/05/28/Making-Explanatory-Chart-USD-PHP-Exchange-Rates.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Indicators of Heavy Traffic on the I-94 Highway",
            "content": ". Unsplash | Alexander Popov Overview . Interstate 94 or I-94 is a highway in the USA that stretches from Montana in the west to Michigan in the east. In 2019, John Hogue donated a dataset of traffic volume, weather, and holiday data on I-94 from 2012 to 2018. This can be found on the following UCI Machine Learning Repository page: Metro Interstate Traffic Volume Data Set. . The goal of this project is to determine possible indicators of heavy traffic on I-94. Exploratory data analysis will be conducted with Seaborn visualizations. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Finding Heavy Traffic Indicators on I-94. The general project flow and research questions came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data Overview . The following details about the I-94 dataset are stated in the archive. . holiday: Categorical; US National holidays plus regional holiday, Minnesota State Fair | temp: Numeric; Average temperature (Kelvin) | rain_1h: Numeric; Amount (mm) of rain that occurred in the hour | snow_1h: Numeric; Amount (mm) of snow that occurred in the hour | clouds_all: Numeric; Percentage of cloud cover (%) | weather_main: Categorical; Short textual description of the current weather | weather_description: Categorical; Longer textual description of the current weather | date_time: DateTime; Hour of the data collected in local CST time | traffic_volume: Numeric; Hourly I-94 ATR 301 reported westbound traffic volume . Note: The data were collected from a station somewhere between Minneapolis and St. Paul, Minnesota. Only westbound traffic was recorded, not eastbound. The data are not representative of the entire I-94 highway. | . Below are the first few rows of the dataset. . highway = pd.read_csv(&quot;./2021-05-25-IHT-Files/Metro_Interstate_Traffic_Volume.csv&quot;) highway.head() . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . 0 None | 288.28 | 0.0 | 0.0 | 40 | Clouds | scattered clouds | 2012-10-02 09:00:00 | 5545 | . 1 None | 289.36 | 0.0 | 0.0 | 75 | Clouds | broken clouds | 2012-10-02 10:00:00 | 4516 | . 2 None | 289.58 | 0.0 | 0.0 | 90 | Clouds | overcast clouds | 2012-10-02 11:00:00 | 4767 | . 3 None | 290.13 | 0.0 | 0.0 | 90 | Clouds | overcast clouds | 2012-10-02 12:00:00 | 5026 | . 4 None | 291.14 | 0.0 | 0.0 | 75 | Clouds | broken clouds | 2012-10-02 13:00:00 | 4918 | . There are columns about the date, traffic volume, weather, and occurrence of holidays. With this dataset, one could determine how traffic or weather changed over time. One could also determine how weather and holidays affected traffic volume. . Let us view the information on each column below. . highway.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48204 entries, 0 to 48203 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 holiday 48204 non-null object 1 temp 48204 non-null float64 2 rain_1h 48204 non-null float64 3 snow_1h 48204 non-null float64 4 clouds_all 48204 non-null int64 5 weather_main 48204 non-null object 6 weather_description 48204 non-null object 7 date_time 48204 non-null object 8 traffic_volume 48204 non-null int64 dtypes: float64(3), int64(2), object(4) memory usage: 3.3+ MB . It turns out that most of the numeric columns are float64, which refers to decimal numbers. . The integer columns are clouds_all, which is in percentage, and traffic volume, which tells the number of cars. . The date_time column is listed as an object or text column. Let&#39;s convert this to datetime format for ease of use. . highway[&quot;date_time&quot;] = pd.to_datetime(highway[&quot;date_time&quot;]) highway[&quot;date_time&quot;] . 0 2012-10-02 09:00:00 1 2012-10-02 10:00:00 2 2012-10-02 11:00:00 3 2012-10-02 12:00:00 4 2012-10-02 13:00:00 ... 48199 2018-09-30 19:00:00 48200 2018-09-30 20:00:00 48201 2018-09-30 21:00:00 48202 2018-09-30 22:00:00 48203 2018-09-30 23:00:00 Name: date_time, Length: 48204, dtype: datetime64[ns] . Lastly, all of the Non-Null Counts match the total number of datapoints (48204). This could mean that there are no missing values in the dataset. It could also mean that missing values are expressed in a non-conventional way. . Data Cleaning . Duplicate Entries . There may be duplicate entries, i.e., multiple entries for the same date and hour. These must be removed. . highway.drop_duplicates( subset = [&quot;date_time&quot;], keep = &quot;first&quot;, inplace = True, ) highway.shape . (40575, 9) . There were 7629 duplicate entries removed from the original 48204 entries. . Descriptive Statistics . Next, we view the descriptive statistics in order to find abnormal values. . highway.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . count 40575 | 40575.000000 | 40575.000000 | 40575.000000 | 40575.000000 | 40575 | 40575 | 40575 | 40575.000000 | . unique 12 | NaN | NaN | NaN | NaN | 11 | 35 | NaN | NaN | . top None | NaN | NaN | NaN | NaN | Clouds | sky is clear | NaN | NaN | . freq 40522 | NaN | NaN | NaN | NaN | 15123 | 11642 | NaN | NaN | . mean NaN | 281.316763 | 0.318632 | 0.000117 | 44.199162 | NaN | NaN | 2015-12-23 22:16:28.835489792 | 3290.650474 | . min NaN | 0.000000 | 0.000000 | 0.000000 | 0.000000 | NaN | NaN | 2012-10-02 09:00:00 | 0.000000 | . 25% NaN | 271.840000 | 0.000000 | 0.000000 | 1.000000 | NaN | NaN | 2014-02-02 19:30:00 | 1248.500000 | . 50% NaN | 282.860000 | 0.000000 | 0.000000 | 40.000000 | NaN | NaN | 2016-06-02 14:00:00 | 3427.000000 | . 75% NaN | 292.280000 | 0.000000 | 0.000000 | 90.000000 | NaN | NaN | 2017-08-02 23:30:00 | 4952.000000 | . max NaN | 310.070000 | 9831.300000 | 0.510000 | 100.000000 | NaN | NaN | 2018-09-30 23:00:00 | 7280.000000 | . std NaN | 13.816618 | 48.812640 | 0.005676 | 38.683447 | NaN | NaN | NaN | 1984.772909 | . Most of the descriptives make sense, except for a few details: . The minimum temp is 0 Kelvin. This is called absolute zero, the lowest possible temperature of any object, equivalent to $-273.15^{ circ} text{C}$. It is unreasonable for the I-94 highway to reach such a low temperature, even in winter. | The maximum rain_1h is 9,831 mm, or 9.8 meters. Either this indicates a very high flood, or this is inaccurate. | The minimum traffic_volume is 0 cars. This may be possible, but it is still best to inspect the data. | . Temperature Outliers . Let us graph a boxplot in order to find outliers among the temperature values. . sns.boxplot( data = highway, y = &quot;temp&quot;, ) plt.title(&quot;Temperature on I-94 (Kelvin)&quot;) plt.ylabel(&quot;Temperature (Kelvin)&quot;) plt.grid(True) plt.show() . Indeed, most values fall between 250 Kelvin and 300 Kelvin ($-23.15^{ circ} text{C}$ and $26.85^{ circ} text{C}$). The only outliers are at 0 Kelvin. This supports the idea that the zeroes are placeholders for missing values. . How many missing values are there? . (highway[&quot;temp&quot;] .value_counts(bins = 10) .sort_index() ) . (-0.311, 31.007] 10 (31.007, 62.014] 0 (62.014, 93.021] 0 (93.021, 124.028] 0 (124.028, 155.035] 0 (155.035, 186.042] 0 (186.042, 217.049] 0 (217.049, 248.056] 99 (248.056, 279.063] 17372 (279.063, 310.07] 23094 Name: temp, dtype: int64 . Only 10 datapoints have zero-values in temp. Thus, these can be dropped from the dataset. . highway = highway.loc[highway[&quot;temp&quot;] != 0] highway.shape . (40565, 9) . Now, there are 40565 rows in the dataset. Temperature outliers have been removed. . Rain Level Outliers . Similarly, we graph a boxplot below for the rain_1h column. . sns.boxplot( data = highway, y = &quot;rain_1h&quot;, ) plt.title(&quot;Hourly Rain Level on I-94 (mm)&quot;) plt.ylabel(&quot;Hourly Rain Level (mm)&quot;) plt.grid(True) plt.show() . Most of the values are close to 0 mm, and there are only a few outliers near 10,000 mm. How many outliers are there? . highway[&quot;rain_1h&quot;].value_counts(bins = 10).sort_index() . (-9.831999999999999, 983.13] 40564 (983.13, 1966.26] 0 (1966.26, 2949.39] 0 (2949.39, 3932.52] 0 (3932.52, 4915.65] 0 (4915.65, 5898.78] 0 (5898.78, 6881.91] 0 (6881.91, 7865.04] 0 (7865.04, 8848.17] 0 (8848.17, 9831.3] 1 Name: rain_1h, dtype: int64 . There is only 1 outlying datapoint. Since a 9.8 m flood level is so unrealistic given that most of the other values are small, this datapoint will be dropped. . highway = highway.loc[highway[&quot;rain_1h&quot;] &lt; 1000] highway.shape . (40564, 9) . The dataset is left with 40564 rows. . Traffic Volume Outliers . Below is the boxplot of traffic volume values. We want to see if the zero-values are reasonable or if these are distant outliers. . sns.boxplot( data = highway, y = &quot;traffic_volume&quot;, ) plt.title(&quot;Traffic Volume on I-94&quot;) plt.ylabel(&quot;Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The boxplot shows that 0 is within the approximate lower bound. It is not too distant from most of the datapoints to be considered an outlier. . Let us view a histogram to understand the distribution better. . sns.histplot( data = highway, x = &quot;traffic_volume&quot;, ) plt.title(&quot;Traffic Volume on I-94&quot;) plt.xlabel(&quot;Hourly Number of Cars&quot;) plt.grid(True) plt.show() . This is an unusual distribution. There appear to be 3 peaks: . less than 1000 cars | around 3000 cars | around 4500 cars | . It is common that less than 1000 cars pass through this I-94 station per hour. Therefore, it is likely that the 0-values are not outliers and do not need to be dropped from the dataset. . Data cleaning is done, so here are the new descriptive statistics for the dataset. . highway.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . count 40564 | 40564.000000 | 40564.000000 | 40564.000000 | 40564.000000 | 40564 | 40564 | 40564 | 40564.000000 | . unique 12 | NaN | NaN | NaN | NaN | 11 | 35 | NaN | NaN | . top None | NaN | NaN | NaN | NaN | Clouds | sky is clear | NaN | NaN | . freq 40511 | NaN | NaN | NaN | NaN | 15123 | 11632 | NaN | NaN | . mean NaN | 281.385602 | 0.076353 | 0.000117 | 44.209299 | NaN | NaN | 2015-12-24 02:14:28.937974528 | 3291.081402 | . min NaN | 243.390000 | 0.000000 | 0.000000 | 0.000000 | NaN | NaN | 2012-10-02 09:00:00 | 0.000000 | . 25% NaN | 271.850000 | 0.000000 | 0.000000 | 1.000000 | NaN | NaN | 2014-02-03 02:45:00 | 1249.750000 | . 50% NaN | 282.867500 | 0.000000 | 0.000000 | 40.000000 | NaN | NaN | 2016-06-02 19:30:00 | 3429.000000 | . 75% NaN | 292.280000 | 0.000000 | 0.000000 | 90.000000 | NaN | NaN | 2017-08-03 02:15:00 | 4952.000000 | . max NaN | 310.070000 | 55.630000 | 0.510000 | 100.000000 | NaN | NaN | 2018-09-30 23:00:00 | 7280.000000 | . std NaN | 13.092942 | 0.769729 | 0.005677 | 38.682163 | NaN | NaN | NaN | 1984.638849 | . Due to data cleaning, the following have changed: . Minimum temp is 243.39 Kelvin | Maximum rain_1h is 55.63 mm | . These are more reasonable values than before. . Exploratory Data Analysis . Traffic Volume: Day vs. Night . At the end of the data cleaning, we noticed that there were 3 peaks (most common values) in the traffic volume data. It is possible that this can be explained by comparing traffic volume between daytime and nighttime. . In order to do this, we can make a new column half which labels each entry as &quot;day&quot; or &quot;night.&quot; We will consider daytime to be from 6:00 AM to 6:00 PM, or 6:00 to 18:00. . highway[&quot;half&quot;] = ( highway[&quot;date_time&quot;] .dt.hour.between(6, 17) # Boolean Series where True represents day .replace({True: &quot;day&quot;, False: &quot;night&quot;}) # Replace booleans with strings ) highway[&quot;half&quot;].value_counts() . night 20418 day 20146 Name: half, dtype: int64 . There are 20418 nighttime entries and 20146 daytime entries. . Now we can compare the day and night histograms for traffic volume. . sns.histplot( data = highway, x = &quot;traffic_volume&quot;, hue = &quot;half&quot;, palette = &quot;RdBu&quot;, ) plt.title(&quot;Traffic Volume on I-94: Day and Night&quot;) plt.xlabel(&quot;Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The histogram above shows that: . In the nighttime, the traffic volume is commonly under 1000 or around 3000. | In the daytime, the traffic volume is commonly around 5000. | . Therefore, traffic is generally heavier in the daytime, between 6:00 AM and 6:00 PM. . Since we want to determine what influences heavy traffic, it would be best to focus our analysis on the daytime entries. Thus, such entries will be put in a separate DataFrame called daytime. . daytime = ( highway .loc[highway[&quot;half&quot;] == &quot;day&quot;] .drop(columns = &quot;half&quot;) ) daytime.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . count 20146 | 20146.000000 | 20146.000000 | 20146.000000 | 20146.000000 | 20146 | 20146 | 20146 | 20146.000000 | . unique 1 | NaN | NaN | NaN | NaN | 10 | 33 | NaN | NaN | . top None | NaN | NaN | NaN | NaN | Clouds | sky is clear | NaN | NaN | . freq 20146 | NaN | NaN | NaN | NaN | 8360 | 4971 | NaN | NaN | . mean NaN | 282.018771 | 0.076210 | 0.000124 | 47.342500 | NaN | NaN | 2015-12-26 11:32:49.582050816 | 4784.630100 | . min NaN | 243.390000 | 0.000000 | 0.000000 | 0.000000 | NaN | NaN | 2012-10-02 09:00:00 | 1.000000 | . 25% NaN | 272.220000 | 0.000000 | 0.000000 | 1.000000 | NaN | NaN | 2014-02-04 08:15:00 | 4311.000000 | . 50% NaN | 283.640000 | 0.000000 | 0.000000 | 40.000000 | NaN | NaN | 2016-06-07 11:30:00 | 4943.000000 | . 75% NaN | 293.370000 | 0.000000 | 0.000000 | 90.000000 | NaN | NaN | 2017-08-06 09:45:00 | 5678.000000 | . max NaN | 310.070000 | 44.450000 | 0.510000 | 100.000000 | NaN | NaN | 2018-09-30 17:00:00 | 7280.000000 | . std NaN | 13.330019 | 0.737429 | 0.005673 | 37.808456 | NaN | NaN | NaN | 1293.502893 | . This DataFrame contains only 20146 rows. The descriptive statistics are naturally somewhat different from before. . Effect of Units of Time . It is possible that traffic volume is influenced by certain units of time. For example, it could be influenced by the month, the day of the week, or the hour of the day. In this section, we investigate these factors. . By Month . First, does the month affect the traffic volume? . Let us make a new column that indicates the month as a number. . daytime[&quot;month&quot;] = daytime[&quot;date_time&quot;].dt.month daytime[&quot;month&quot;] . 0 10 1 10 2 10 3 10 4 10 .. 48191 9 48192 9 48194 9 48196 9 48197 9 Name: month, Length: 20146, dtype: int64 . Then, we can calculate and graph the average traffic volume per month. The median will be used instead of the mean since the data are not normally distributed. . Table: . (daytime .groupby(&quot;month&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . month . 1 4651.5 | . 2 4886.0 | . 3 5062.0 | . 4 5105.0 | . 5 5052.0 | . 6 5050.0 | . 7 4799.5 | . 8 5056.0 | . 9 4925.5 | . 10 5056.0 | . 11 4876.0 | . 12 4722.0 | . Line chart: . sns.lineplot( data = daytime, x = &quot;month&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Month on Traffic Volume&quot;) plt.xlabel(&quot;Month&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The line chart shows that the median traffic volume is highest in April, possibly because this is in spring. Traffic volume is lowest in January and December since these are in the middle of winter. July also has less traffic since it is in the middle of summer. . By Day of the Week . Next, we will investigate the effect of the day of the week on the traffic volume. . daytime[&quot;day&quot;] = daytime[&quot;date_time&quot;].dt.dayofweek (daytime .groupby(&quot;day&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . day . 0 4971.5 | . 1 5268.0 | . 2 5355.0 | . 3 5404.0 | . 4 5399.0 | . 5 4194.0 | . 6 3737.0 | . Note that 0 means Monday and 6 means Sunday. . The corresponding line chart is shown below. . sns.lineplot( data = daytime, x = &quot;day&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Day of Week on Traffic Volume&quot;) plt.xlabel(&quot;Day of the Week&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The line chart shows that the traffic volume is very high from Monday to Friday, then dips by 1000 cars on Saturday and Sunday. It makes sense that traffic is heavier on weekdays and lighter on weekends. . By Hour of the Day . Lastly, we will investigate the effect of the time of day on the traffic volume. Since we have narrowed the dataset down to daytime entries, only hours from 6:00 AM to 6:00 PM are included. . First, though, let&#39;s make a column that indicates weekdays and weekends so that we can compare them. . daytime[&quot;day_type&quot;] = daytime[&quot;day&quot;].replace({ 0: &quot;business day&quot;, 1: &quot;business day&quot;, 2: &quot;business day&quot;, 3: &quot;business day&quot;, 4: &quot;business day&quot;, 5: &quot;weekend&quot;, 6: &quot;weekend&quot;, }) . Next, below is the table of median traffic volume grouped by the day of the week and the type of day. . daytime[&quot;hour&quot;] = daytime[&quot;date_time&quot;].dt.hour (daytime .groupby([&quot;hour&quot;, &quot;day_type&quot;]) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . hour day_type . 6 business day 5588.0 | . weekend 1092.0 | . 7 business day 6320.5 | . weekend 1547.0 | . 8 business day 5751.0 | . weekend 2268.0 | . 9 business day 5053.0 | . weekend 3147.0 | . 10 business day 4484.5 | . weekend 3722.0 | . 11 business day 4714.0 | . weekend 4114.0 | . 12 business day 4921.0 | . weekend 4442.5 | . 13 business day 4919.5 | . weekend 4457.0 | . 14 business day 5232.0 | . weekend 4457.0 | . 15 business day 5740.0 | . weekend 4421.0 | . 16 business day 6403.5 | . weekend 4438.0 | . 17 business day 5987.0 | . weekend 4261.5 | . The hours are in 24-hour time; 17:00 represents the hour from 5:00 PM to 6:00 PM. . In order to understand this table better, we can visualize it in the line chart below. . sns.lineplot( data = daytime, x = &quot;hour&quot;, y = &quot;traffic_volume&quot;, hue = &quot;day_type&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Hour of Day on Traffic Volume&quot;) plt.xlabel(&quot;Hour of the Day&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.legend(title = &quot;Day Type&quot;) plt.grid(True) plt.show() . On business days, traffic is heaviest at 7:00 AM and 4:00 PM. These are the times when people travel to or from work. Traffic is lightest around noontime. . On weekends, traffic volume increases from 6:00 AM to 12:00 PM and plateaus from there on. People are free to travel at any time on weekends since most don&#39;t have work. However, the number of cars is still lower on weekends compared to business days. . Effect of Weather . Up until now, we have investigated the possible effects of different units of time on the traffic volume. In this section, we focus on how traffic is affected by the weather. . The following are the weather-related columns in the dataset: . temp | rain_1h | snow_1h | clouds_all | weather_main | weather_description | . The first 4 are numerical and the last 2 are categorical. . Numerical Weather Columns . Let us inspect the Pearson&#39;s correlation coefficient between traffic volume and each of the numerical weather columns. . daytime.corr().loc[ [&quot;temp&quot;, &quot;rain_1h&quot;, &quot;snow_1h&quot;, &quot;clouds_all&quot;], [&quot;traffic_volume&quot;] ] . traffic_volume . temp 0.124311 | . rain_1h -0.022817 | . snow_1h -0.004145 | . clouds_all 0.000621 | . All 4 numerical weather columns appear to have very weak correlations with traffic volume. . The highest correlation involves temperature, but the coefficient is only 12.43. This indicates a weak positive relationship. As temperature increases, traffic volume also increases, but not consistently. . We can understand the correlation better using a scatter plot. . sns.scatterplot( data = daytime, x = &quot;temp&quot;, y = &quot;traffic_volume&quot;, ci = None, ) plt.title(&quot;Effect of Temperature on Traffic Volume&quot;) plt.xlabel(&quot;Temperature (Kelvin)&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.grid(True) plt.show() . Unfortunately, the datapoints are scattered quite consistently throughout all combinations of temperature and traffic volume. The correlation is weak; temperature is not a reliable indicator of traffic. Neither are the other numerical weather columns, since their coefficients were even weaker. . Categorical Weather Columns . Next, we&#39;ll see if the categorical weather columns can serve as better indicators of heavy traffic. . Short Descriptions of Weather . The weather_main column contains short, 1-word descriptions of the weather. . What are the categories under weather_main? . daytime[&quot;weather_main&quot;].value_counts() . Clouds 8360 Clear 5821 Rain 2392 Mist 1441 Snow 1165 Haze 472 Drizzle 223 Thunderstorm 175 Fog 90 Smoke 7 Name: weather_main, dtype: int64 . Clouds, Clear, and Rain are the most frequent descriptions of the weather. . Next, let us investigate the effect of the weather description on the traffic volume. The table is shown below. . (daytime .groupby(&quot;weather_main&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . weather_main . Clear 4936.0 | . Clouds 4974.5 | . Drizzle 5132.0 | . Fog 5588.0 | . Haze 4817.5 | . Mist 5053.0 | . Rain 4975.0 | . Smoke 4085.0 | . Snow 4570.0 | . Thunderstorm 4875.0 | . This is visualized in the bar plot below. . sns.barplot( data = daytime, x = &quot;weather_main&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Short Weather Description on Traffic Volume&quot;) plt.xlabel(&quot;Short Weather Description&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.xticks(rotation = 45) plt.grid(True) plt.show() . The traffic volume appears to be mostly consistent across short weather descriptions. Notably: . Traffic is heaviest during fog (5588 cars). | Traffic is lightest when there is smoke (4085 cars). | . However, the effects are quite small, reaching only up to a difference of 1000 cars. . Long Descriptions of Weather . The weather_description column contains longer descriptions of the weather. . Below are its categories. . daytime[&quot;weather_description&quot;].value_counts() . sky is clear 4971 broken clouds 2683 overcast clouds 2526 scattered clouds 2068 mist 1441 light rain 1415 few clouds 1083 Sky is Clear 850 light snow 811 moderate rain 666 haze 472 heavy snow 249 heavy intensity rain 207 light intensity drizzle 160 proximity thunderstorm 136 snow 90 fog 90 proximity shower rain 89 drizzle 57 thunderstorm 22 light shower snow 11 smoke 7 very heavy rain 7 light intensity shower rain 7 thunderstorm with light rain 7 heavy intensity drizzle 6 thunderstorm with heavy rain 5 thunderstorm with rain 3 sleet 3 proximity thunderstorm with rain 1 proximity thunderstorm with drizzle 1 light rain and snow 1 freezing rain 1 Name: weather_description, dtype: int64 . Notice that there is a sky is clear value and a Sky is Clear value with different capitalization. It is likely that these two categories mean the same thing, so let us combine them. . daytime[&quot;weather_description&quot;].replace( {&quot;Sky is Clear&quot;: &quot;sky is clear&quot;}, inplace = True, ) daytime[&quot;weather_description&quot;].value_counts().head() . sky is clear 5821 broken clouds 2683 overcast clouds 2526 scattered clouds 2068 mist 1441 Name: weather_description, dtype: int64 . The sky is clear category now has 5821 entries. . Now that that&#39;s cleaned, let&#39;s make a table showing the effect of the long weather description on the traffic volume. . (daytime .groupby(&quot;weather_description&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . weather_description . broken clouds 4925.0 | . drizzle 5132.0 | . few clouds 4977.0 | . fog 5588.0 | . freezing rain 4762.0 | . haze 4817.5 | . heavy intensity drizzle 5824.5 | . heavy intensity rain 4922.0 | . heavy snow 4673.0 | . light intensity drizzle 5086.5 | . light intensity shower rain 4695.0 | . light rain 5011.0 | . light rain and snow 5544.0 | . light shower snow 4324.0 | . light snow 4601.0 | . mist 5053.0 | . moderate rain 4940.0 | . overcast clouds 4968.0 | . proximity shower rain 4910.0 | . proximity thunderstorm 4849.5 | . proximity thunderstorm with drizzle 6667.0 | . proximity thunderstorm with rain 5730.0 | . scattered clouds 5032.5 | . sky is clear 4936.0 | . sleet 5174.0 | . smoke 4085.0 | . snow 4032.0 | . thunderstorm 5578.0 | . thunderstorm with heavy rain 5278.0 | . thunderstorm with light rain 4073.0 | . thunderstorm with rain 4270.0 | . very heavy rain 4802.0 | . The table is visualized in the bar graph below. . plt.figure(figsize = (14, 5)) sns.barplot( data = daytime, x = &quot;weather_description&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Long Weather Description on Traffic Volume&quot;) plt.xlabel(&quot;Long Weather Description&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.xticks(rotation = 45, ha = &quot;right&quot;) plt.grid(True) plt.show() . Similar to the bar graph of short weather descriptions, most of the values are around 5000 cars. Notably: . Traffic is heaviest in a proximity thunderstorm with drizzle (6667 cars). The word &quot;proximity&quot; was likely used to emphasize that the storm was very close to the station. | It makes sense that a drizzling thunderstorm directly over the highway would reduce visibility and make traffic pile up. | . | Traffic is lightest in snow (4032 cars). It is likely that more people choose not to travel outside when it is snowing. | . | . Conclusion . This project aimed to determine reliable indicators of heavy traffic along the I-94 highway. Data were narrowed down to daytime (as opposed to nighttime) entries, which have higher traffic volumes. Exploratory data analysis was conducted using mostly Seaborn visualizations. . The following conclusions were drawn: . Time has various effects on traffic volume. Traffic is heavier when: It is daytime (between 6:00 AM and 6:00 PM). | The month is from March to June. | The day is a business day, as opposed to a weekend. | The hour is 7:00 AM or 4:00 PM on a business day. | . | . . Weather affects traffic volume to a lesser extent. Traffic is heavier when: There is fog on the road (or other visual obstructions). | There is a nearby thunderstorm with drizzle or rain. | . | . . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/2021/05/25/Indicators-Heavy-Traffic-I-94-Highway.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/2021/05/25/Indicators-Heavy-Traffic-I-94-Highway.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Basic Data Cleaning with eBay Car Sales Data",
            "content": ". Unsplash | Parker Gibbs Overview . In 2016, Kaggle user orgezleka scraped data about used car advertisements on eBay Kleinanzeigen, which is the eBay Classifieds website of Germany. The full dataset is still available on Used Cars Data (data.world 2016), with over 370,000 datapoints. . The project aims to answer the following questions: . Which brands of the used cars are most expensive? | Does a used car&#39;s age affect its price? | Does a used car&#39;s mileage affect its price? | Does the presence of unrepaired damage affect a car&#39;s price? | . More importantly, this project aims to showcase basic data cleaning procedures. Thus, even columns beyond the scope of the research questions will be considered for cleaning. The assumption is that analysis will eventually be performed on all columns. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Exploring eBay Car Sales Data. The general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data Overview . Below is the information about each column in the dataset, taken from Used Cars Data (data.world 2016). . dateCrawled : when this ad was first crawled, all field-values are taken from this date | name : &quot;name&quot; of the car | seller : private or dealer | offerType | price : the price on the ad to sell the car | abtest | vehicleType | yearOfRegistration : at which year the car was first registered | gearbox | powerPS : power of the car in PS | model | kilometer : how many kilometers the car has driven | monthOfRegistration : at which month the car was first registered | fuelType | brand | notRepairedDamage : if the car has a damage which is not repaired yet | dateCreated : the date for which the ad at ebay was created | nrOfPictures : number of pictures in the ad | postalCode | lastSeenOnline : when the crawler saw this ad last online | . Let us view the first 5 rows of the dataset below. . autos = pd.read_csv(&quot;./2021-05-20-BDC-Files/autos.csv&quot;) autos.head() . dateCrawled name seller offerType price abtest vehicleType yearOfRegistration gearbox powerPS model kilometer monthOfRegistration fuelType brand notRepairedDamage dateCreated nrOfPictures postalCode lastSeen . 0 2016-03-24 11:52:17 | Golf_3_1.6 | privat | Angebot | 480 | test | NaN | 1993 | manuell | 0 | golf | 150000 | 0 | benzin | volkswagen | NaN | 2016-03-24 00:00:00 | 0 | 70435 | 2016-04-07 03:16:57 | . 1 2016-03-24 10:58:45 | A5_Sportback_2.7_Tdi | privat | Angebot | 18300 | test | coupe | 2011 | manuell | 190 | NaN | 125000 | 5 | diesel | audi | ja | 2016-03-24 00:00:00 | 0 | 66954 | 2016-04-07 01:46:50 | . 2 2016-03-14 12:52:21 | Jeep_Grand_Cherokee_&quot;Overland&quot; | privat | Angebot | 9800 | test | suv | 2004 | automatik | 163 | grand | 125000 | 8 | diesel | jeep | NaN | 2016-03-14 00:00:00 | 0 | 90480 | 2016-04-05 12:47:46 | . 3 2016-03-17 16:54:04 | GOLF_4_1_4__3T�RER | privat | Angebot | 1500 | test | kleinwagen | 2001 | manuell | 75 | golf | 150000 | 6 | benzin | volkswagen | nein | 2016-03-17 00:00:00 | 0 | 91074 | 2016-03-17 17:40:17 | . 4 2016-03-31 17:25:20 | Skoda_Fabia_1.4_TDI_PD_Classic | privat | Angebot | 3600 | test | kleinwagen | 2008 | manuell | 69 | fabia | 90000 | 7 | diesel | skoda | nein | 2016-03-31 00:00:00 | 0 | 60437 | 2016-04-06 10:17:21 | . We can get additional information about each column using DataFrame.info(). . autos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 371528 entries, 0 to 371527 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 dateCrawled 371528 non-null object 1 name 371528 non-null object 2 seller 371528 non-null object 3 offerType 371528 non-null object 4 price 371528 non-null int64 5 abtest 371528 non-null object 6 vehicleType 333659 non-null object 7 yearOfRegistration 371528 non-null int64 8 gearbox 351319 non-null object 9 powerPS 371528 non-null int64 10 model 351044 non-null object 11 kilometer 371528 non-null int64 12 monthOfRegistration 371528 non-null int64 13 fuelType 338142 non-null object 14 brand 371528 non-null object 15 notRepairedDamage 299468 non-null object 16 dateCreated 371528 non-null object 17 nrOfPictures 371528 non-null int64 18 postalCode 371528 non-null int64 19 lastSeen 371528 non-null object dtypes: int64(7), object(13) memory usage: 56.7+ MB . The dataset has 371,528 rows and 20 columns. Each column contains one of two types of data: . int64: 64-bit integers | object: text strings | . Also, some columns have a non-null count smaller than the number of entries. This means that there are missing values in these columns. The column notRepairedDamagehas the most missing values. . Data Cleaning . Column Labels . As seen earlier, all of the column labels are written in camel case. This means that the first letter of each word is capitalized, like CamelCase. . Column labels should ideally be written in snake_case so that these are easier to use. Thus, I will convert all column labels to snake case. . cols = list(autos.columns) new_cols = [] for label in cols: # List of indices of the first letter of each word. cap_inds = [0] + [index for index, character in enumerate(label) if character.isupper()] # List of 2-tuples. Each tuple contains the start index of the current word # and the start index of the next word. zipped = list(zip(cap_inds, cap_inds[1:] + [None])) # Split the label into a list of words. # Make them lowercase and combine them with underscores. word_list = [label[i:j] for i, j in zipped] word_list = [word.lower() for word in word_list] new_label = &quot;_&quot;.join(word_list) new_cols.append(new_label) autos.columns = new_cols autos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 371528 entries, 0 to 371527 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 date_crawled 371528 non-null object 1 name 371528 non-null object 2 seller 371528 non-null object 3 offer_type 371528 non-null object 4 price 371528 non-null int64 5 abtest 371528 non-null object 6 vehicle_type 333659 non-null object 7 year_of_registration 371528 non-null int64 8 gearbox 351319 non-null object 9 power_p_s 371528 non-null int64 10 model 351044 non-null object 11 kilometer 371528 non-null int64 12 month_of_registration 371528 non-null int64 13 fuel_type 338142 non-null object 14 brand 371528 non-null object 15 not_repaired_damage 299468 non-null object 16 date_created 371528 non-null object 17 nr_of_pictures 371528 non-null int64 18 postal_code 371528 non-null int64 19 last_seen 371528 non-null object dtypes: int64(7), object(13) memory usage: 56.7+ MB . Now, all of the column labels are in snake case. However, I will manually change a few of them to make them neater. . convert = { &quot;power_p_s&quot;: &quot;power_ps&quot;, &quot;nr_of_pictures&quot;: &quot;num_pictures&quot;, &quot;year_of_registration&quot;: &quot;year_reg&quot;, &quot;kilometer&quot;: &quot;mileage_km&quot;, &quot;month_of_registration&quot;: &quot;month_reg&quot;, &quot;not_repaired_damage&quot;: &quot;damage&quot;, } autos.columns = pd.Series(autos.columns).replace(convert) autos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 371528 entries, 0 to 371527 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 date_crawled 371528 non-null object 1 name 371528 non-null object 2 seller 371528 non-null object 3 offer_type 371528 non-null object 4 price 371528 non-null int64 5 abtest 371528 non-null object 6 vehicle_type 333659 non-null object 7 year_reg 371528 non-null int64 8 gearbox 351319 non-null object 9 power_ps 371528 non-null int64 10 model 351044 non-null object 11 mileage_km 371528 non-null int64 12 month_reg 371528 non-null int64 13 fuel_type 338142 non-null object 14 brand 371528 non-null object 15 damage 299468 non-null object 16 date_created 371528 non-null object 17 num_pictures 371528 non-null int64 18 postal_code 371528 non-null int64 19 last_seen 371528 non-null object dtypes: int64(7), object(13) memory usage: 56.7+ MB . All of the column labels are now easy to use. . Uninformative Columns . Next, we will look for columns filled with mostly 1 value. The other values in them are underrepresented, so we wouldn&#39;t be able to compare groups reliably. . We can inspect the columns using DataFrame.describe(). . autos.describe(include = &quot;all&quot;) # Include all columns, including text. . date_crawled name seller offer_type price abtest vehicle_type year_reg gearbox power_ps model mileage_km month_reg fuel_type brand damage date_created num_pictures postal_code last_seen . count 371528 | 371528 | 371528 | 371528 | 3.715280e+05 | 371528 | 333659 | 371528.000000 | 351319 | 371528.000000 | 351044 | 371528.000000 | 371528.000000 | 338142 | 371528 | 299468 | 371528 | 371528.0 | 371528.00000 | 371528 | . unique 280500 | 233528 | 2 | 2 | NaN | 2 | 8 | NaN | 2 | NaN | 251 | NaN | NaN | 7 | 40 | 2 | 114 | NaN | NaN | 182806 | . top 2016-03-24 14:49:47 | Ford_Fiesta | privat | Angebot | NaN | test | limousine | NaN | manuell | NaN | golf | NaN | NaN | benzin | volkswagen | nein | 2016-04-03 00:00:00 | NaN | NaN | 2016-04-06 13:45:54 | . freq 7 | 657 | 371525 | 371516 | NaN | 192585 | 95894 | NaN | 274214 | NaN | 30070 | NaN | NaN | 223857 | 79640 | 263182 | 14450 | NaN | NaN | 17 | . mean NaN | NaN | NaN | NaN | 1.729514e+04 | NaN | NaN | 2004.577997 | NaN | 115.549477 | NaN | 125618.688228 | 5.734445 | NaN | NaN | NaN | NaN | 0.0 | 50820.66764 | NaN | . std NaN | NaN | NaN | NaN | 3.587954e+06 | NaN | NaN | 92.866598 | NaN | 192.139578 | NaN | 40112.337051 | 3.712412 | NaN | NaN | NaN | NaN | 0.0 | 25799.08247 | NaN | . min NaN | NaN | NaN | NaN | 0.000000e+00 | NaN | NaN | 1000.000000 | NaN | 0.000000 | NaN | 5000.000000 | 0.000000 | NaN | NaN | NaN | NaN | 0.0 | 1067.00000 | NaN | . 25% NaN | NaN | NaN | NaN | 1.150000e+03 | NaN | NaN | 1999.000000 | NaN | 70.000000 | NaN | 125000.000000 | 3.000000 | NaN | NaN | NaN | NaN | 0.0 | 30459.00000 | NaN | . 50% NaN | NaN | NaN | NaN | 2.950000e+03 | NaN | NaN | 2003.000000 | NaN | 105.000000 | NaN | 150000.000000 | 6.000000 | NaN | NaN | NaN | NaN | 0.0 | 49610.00000 | NaN | . 75% NaN | NaN | NaN | NaN | 7.200000e+03 | NaN | NaN | 2008.000000 | NaN | 150.000000 | NaN | 150000.000000 | 9.000000 | NaN | NaN | NaN | NaN | 0.0 | 71546.00000 | NaN | . max NaN | NaN | NaN | NaN | 2.147484e+09 | NaN | NaN | 9999.000000 | NaN | 20000.000000 | NaN | 150000.000000 | 12.000000 | NaN | NaN | NaN | NaN | 0.0 | 99998.00000 | NaN | . The top row gives the most frequent value in the column. The freq row tells exactly how often the top value occurs. . We can see that in the seller column, &quot;privat&quot; appears 371,525 times; this is close to the total number of entries. Also, in the offer_type column, &quot;Angebot&quot; appears 371,516 times. . Let us first check the unique values of the seller column. . autos[&quot;seller&quot;].value_counts() . privat 371525 gewerblich 3 Name: seller, dtype: int64 . In English, these mean &quot;private&quot; and &quot;commercial.&quot; It is likely that private listings are put up by individuals, whereas commercial listings are put up by used car dealer companies. Most listings scraped were apparently private. . One idea is to compare private listing prices to commercial listing prices. However, there are only 3 commercial listings, so this comparison would be unreliable. Therefore, we can drop this column. . autos.drop(columns = &quot;seller&quot;, inplace = True) . Secondly, let&#39;s check the unique values of the offer_type column. . autos[&quot;offer_type&quot;].value_counts() . Angebot 371516 Gesuch 12 Name: offer_type, dtype: int64 . In English, these mean &quot;offer&quot; and &quot;request.&quot; This isn&#39;t very informative. It could have something to do with the system of sending Offers to buyers in eBay. In any case, it doesn&#39;t seem interesting to analyze, and there are only 12 request listings. Thus, we can also drop this column. . autos.drop(columns = &quot;offer_type&quot;, inplace = True) . Next, the numerical num_pictures column has a minimum of 0 and a maximum of 0. . autos[&quot;num_pictures&quot;].describe() . count 371528.0 mean 0.0 std 0.0 min 0.0 25% 0.0 50% 0.0 75% 0.0 max 0.0 Name: num_pictures, dtype: float64 . This means that none of the listings for used cars were found to have pictures. This data may be true or it may have come from an error, but either way, it is not useful. Thus, it will be dropped. . autos.drop(columns = &quot;num_pictures&quot;, inplace = True) . Outliers in Numerical Columns . Next, we inspect the ranges of numerical columns to see if there are any unusually high or low values. . autos.describe(include = np.number) . price year_reg power_ps mileage_km month_reg postal_code . count 3.715280e+05 | 371528.000000 | 371528.000000 | 371528.000000 | 371528.000000 | 371528.00000 | . mean 1.729514e+04 | 2004.577997 | 115.549477 | 125618.688228 | 5.734445 | 50820.66764 | . std 3.587954e+06 | 92.866598 | 192.139578 | 40112.337051 | 3.712412 | 25799.08247 | . min 0.000000e+00 | 1000.000000 | 0.000000 | 5000.000000 | 0.000000 | 1067.00000 | . 25% 1.150000e+03 | 1999.000000 | 70.000000 | 125000.000000 | 3.000000 | 30459.00000 | . 50% 2.950000e+03 | 2003.000000 | 105.000000 | 150000.000000 | 6.000000 | 49610.00000 | . 75% 7.200000e+03 | 2008.000000 | 150.000000 | 150000.000000 | 9.000000 | 71546.00000 | . max 2.147484e+09 | 9999.000000 | 20000.000000 | 150000.000000 | 12.000000 | 99998.00000 | . The following unusual characteristics are noticeable: . price: The minimum price is 0 (free) and the maximum price is larger than 2 billion. | year_reg: The earliest year of registration is 1000, and the latest year is 9999. | power_ps: The lowest power is 0 PS. The highest is 20,000 PS, which is much higher than the 75th percentile (only 150 PS). | month_reg: The month numbers range from 0 to 12, not 1 to 12. | . All of these columns shall be cleaned of inaccurate data. . Price in Euros . It was seen that the minimum value for price data was 0, and the maximum is over 2,000,000,000. Since this data was scraped from the German eBay, we can assume that these prices are in euros. . Both the minimum and maximum values seem to be very unrealistic for the price of a used car, so we want to clean this data. . Let us create a frequency table of the prices. We will first sort it by price, ascending. . autos[&quot;price&quot;].value_counts().sort_index() . 0 10778 1 1189 2 12 3 8 4 1 ... 32545461 1 74185296 1 99000000 1 99999999 15 2147483647 1 Name: price, Length: 5597, dtype: int64 . Two things are noticeable: . There are over 10,000 instances of 0 euro prices. | There are several extremely high outliers, not just the one above 2 billion. | . Since it is unlikely that the cars are actually free, 0 may represent missing values. As for the high outliers, these may have resulted from inaccurate scraping. . The next question is, what is the most frequent price value? Is it 0? . autos[&quot;price&quot;].value_counts().sort_values(ascending = False) . 0 10778 500 5670 1500 5394 1000 4649 1200 4594 ... 6375 1 19986 1 36850 1 5913 1 8188 1 Name: price, Length: 5597, dtype: int64 . Indeed, the most frequent price is 0, which makes it likely that it represents missing values. The next most frequent prices are 500, 1500, and 1000, most likely because these are easy for buyers to remember. . Now that we know about the low and high outliers, we can clean rows using the interquartile range or IQR. The IQR is bounded by the 25th and 75th percentiles. Reasonable lower and upper bounds can be approximated by multiplying the IQR length by 1.5 and: . subtracting it from the 25th percentile | adding it to the 75th percentile | . Reference: Detection and Removal of Outliers in Python – An Easy to Understand Guide . Below, I use this method to find a lower and upper bound, and I drop rows outside of them. . autos_desc = autos[&quot;price&quot;].describe() q25, q75 = autos_desc[[&quot;25%&quot;, &quot;75%&quot;]] iqr = q75 - q25 lower_bound = q25 - 1.5 * iqr upper_bound = q75 + 1.5 * iqr autos = autos.loc[autos[&quot;price&quot;].between(lower_bound, upper_bound)] print(&quot;Lower bound:&quot;, lower_bound) print(&quot;Upper bound:&quot;, upper_bound) print(&quot;New shape:&quot;, autos.shape) . Lower bound: -7925.0 Upper bound: 16275.0 New shape: (343420, 17) . Now, there are 343,420 rows remaining in the dataset. This is still quite large; not too many datapoints were removed. . Let us view the new distribution of prices in a histogram. . ax = sns.histplot( data = autos, x = &quot;price&quot;, ) ax.set_title(&quot;Used Car Prices, Cleaned Using IQR&quot;) ax.grid(True) plt.show() . Unfortunately, since the lower bound was negative, unrealistically low prices close to 0 were still accepted within the range. . Remember from earlier that the 2nd most frequent price value was found to be 500 euros. This makes it likely that these were correct prices, rather than incorrectly scraped values. Thus, let us keep all price values greater than or equal to 500. . autos = autos.loc[autos[&quot;price&quot;] &gt;= 500] autos.shape . (307358, 17) . Now, 307,358 datapoints remain. Let us look at the final distribution of prices: . ax = sns.histplot( data = autos, x = &quot;price&quot;, ) ax.set_title(&quot;Used Car Prices, Cleaned of Low Values&quot;) ax.grid(True) plt.show() . The distribution is still right-skewed, but at least the price range in the dataset is more reasonable now. . Metric Horsepower . We saw earlier that some power values were at 0 PS, and others were as high as 20,000 PS. . Note: A measure of 1 PS (metric horsepower) is equivalent to 0.98 HP (horsepower). (Deriquito 2020) . autos[&quot;power_ps&quot;].describe() . count 307358.000000 mean 112.551842 std 191.557378 min 0.000000 25% 75.000000 50% 105.000000 75% 143.000000 max 20000.000000 Name: power_ps, dtype: float64 . Above are the new descriptive statistics for metric horsepower, after the transformations that we have done recently. . According to &quot;What is the Average Car Horsepower?&quot; (2021), cars usually have a horsepower of 100 HP to 400 HP, though some have less. Faster cars have 400 HP to 1,000 HP, and some supercars have over 1,000 HP. The equivalent values in PS would be slightly higher. . Considering that these are used cars, it is unlikely for them to be very fast much less to be supercars. Thus, we will keep cars with metric horsepowers between 50 and 450, in order to take as many of the realistic values as possible. . autos = autos.loc[autos[&quot;power_ps&quot;].between(50, 450)] autos[&quot;power_ps&quot;].describe() . count 274514.000000 mean 121.166931 std 50.535320 min 50.000000 25% 82.000000 50% 114.000000 75% 150.000000 max 450.000000 Name: power_ps, dtype: float64 . Let us view the distribution of power values in a histogram. . ax = sns.histplot( data = autos, x = &quot;power_ps&quot;, ) ax.set_title(&quot;Metric Horsepower (PS) of Used Cars&quot;) ax.grid(True) plt.show() . Like the price data, the power data are right-skewed. Lower values are more frequent. Still, we managed to capture most (274,514) of the datapoints using this range. . Year of Registration . Earlier, we noticed that the years of registration ranged from 1000 to 9999. . autos[&quot;year_reg&quot;].describe() . count 274514.000000 mean 2003.658039 std 29.529992 min 1000.000000 25% 1999.000000 50% 2004.000000 75% 2007.000000 max 9999.000000 Name: year_reg, dtype: float64 . Logically, the years of registration should only range from when automobiles were first mass-produced, in the 1800s or 1900s (Encyclopedia Britannica), to the year when the ads were last seen by the web crawler. . We can know when the ads were last seen using the last_seen column of the dataset. The dates are shown as strings with years first, so we can sort them in descending order to find the most recent date. . autos[&quot;last_seen&quot;].sort_values(ascending = False) . 343211 2016-04-07 14:58:51 5069 2016-04-07 14:58:50 72231 2016-04-07 14:58:50 62402 2016-04-07 14:58:50 122068 2016-04-07 14:58:50 ... 244077 2016-03-05 14:35:28 177326 2016-03-05 14:25:59 136842 2016-03-05 14:15:39 275196 2016-03-05 14:15:16 311225 2016-03-05 14:15:08 Name: last_seen, Length: 274514, dtype: object . It looks like all of the ads were last seen in 2016, so the cars could not have been registered beyond that year. . Therefore, a reasonable range of years would be from 1900 to 2016. Let us keep all rows within this range. . autos = autos.loc[autos[&quot;year_reg&quot;].between(1900, 2016)] autos[&quot;year_reg&quot;].describe() . count 264977.000000 mean 2002.977877 std 6.097207 min 1910.000000 25% 1999.000000 50% 2003.000000 75% 2007.000000 max 2016.000000 Name: year_reg, dtype: float64 . Now, the years of registration range from 1910 to 2016. There are also 264,977 datapoints remaining. . Months of Registration . Lastly, we noticed that the months of registration included integers from 0 to 12, not 1 to 12. Let us make a discrete histogram in order to understand the distribution. . ax = sns.histplot( data = autos, x = &quot;month_reg&quot;, discrete = True, ) ax.set_title(&quot;Used Cars&#39; Months of Registration&quot;) ax.grid(True) plt.show() . The graph above shows that 0 is the least frequent value, appearing under 15,000 times. It seems very likely that it is a placeholder for unknown months. Therefore, we can remove rows with month 0. . autos = autos.loc[autos[&quot;month_reg&quot;] &gt;= 1] autos[&quot;month_reg&quot;].describe() . count 251198.000000 mean 6.396237 std 3.346424 min 1.000000 25% 4.000000 50% 6.000000 75% 9.000000 max 12.000000 Name: month_reg, dtype: float64 . Missing Values . Now, we shall deal with the null values in the dataset. First, how many null values are there per column? . autos.isnull().sum() . date_crawled 0 name 0 price 0 abtest 0 vehicle_type 5871 year_reg 0 gearbox 3103 power_ps 0 model 7422 mileage_km 0 month_reg 0 fuel_type 9057 brand 0 damage 28718 date_created 0 postal_code 0 last_seen 0 dtype: int64 . There are thousands of missing values for the vehicle type, gearbox, model, fuel type, and presence of damage. None of these can be easily determined from other columns. . If we were to remove all rows with these missing values, we would be left with: . autos.dropna().shape . (208538, 17) . We would have 208,538 rows left. This is just over half of the original number of datapoints we started with, 371,528. . This project is meant to showcase data cleaning, and we are assuming that all of the columns in the dataset will be used for analysis. Thus, we will have to delete all rows with missing values. . autos.dropna(inplace = True) . Below are the final descriptive statistics for the dataset after all of the cleaning. . autos.describe(include = &quot;all&quot;) . date_crawled name price abtest vehicle_type year_reg gearbox power_ps model mileage_km month_reg fuel_type brand damage date_created postal_code last_seen . count 208538 | 208538 | 208538.000000 | 208538 | 208538 | 208538.000000 | 208538 | 208538.000000 | 208538 | 208538.000000 | 208538.000000 | 208538 | 208538 | 208538 | 208538 | 208538.000000 | 208538 | . unique 177458 | 114616 | NaN | 2 | 8 | NaN | 2 | NaN | 248 | NaN | NaN | 7 | 39 | 2 | 98 | NaN | 116677 | . top 2016-03-31 16:50:28 | BMW_318i | NaN | test | limousine | NaN | manuell | NaN | golf | NaN | NaN | benzin | volkswagen | nein | 2016-04-03 00:00:00 | NaN | 2016-04-06 12:15:45 | . freq 5 | 616 | NaN | 108258 | 62894 | NaN | 166213 | NaN | 17505 | NaN | NaN | 136145 | 44157 | 189720 | 8271 | NaN | 14 | . mean NaN | NaN | 4984.662522 | NaN | NaN | 2003.205953 | NaN | 122.635031 | NaN | 128250.990227 | 6.381225 | NaN | NaN | NaN | NaN | 51907.756020 | NaN | . std NaN | NaN | 3990.028529 | NaN | NaN | 5.696916 | NaN | 50.515054 | NaN | 35781.848172 | 3.350349 | NaN | NaN | NaN | NaN | 25766.228717 | NaN | . min NaN | NaN | 500.000000 | NaN | NaN | 1937.000000 | NaN | 50.000000 | NaN | 5000.000000 | 1.000000 | NaN | NaN | NaN | NaN | 1067.000000 | NaN | . 25% NaN | NaN | 1749.000000 | NaN | NaN | 2000.000000 | NaN | 85.000000 | NaN | 125000.000000 | 3.000000 | NaN | NaN | NaN | NaN | 31319.000000 | NaN | . 50% NaN | NaN | 3750.000000 | NaN | NaN | 2004.000000 | NaN | 116.000000 | NaN | 150000.000000 | 6.000000 | NaN | NaN | NaN | NaN | 51371.000000 | NaN | . 75% NaN | NaN | 7300.000000 | NaN | NaN | 2007.000000 | NaN | 150.000000 | NaN | 150000.000000 | 9.000000 | NaN | NaN | NaN | NaN | 72649.000000 | NaN | . max NaN | NaN | 16270.000000 | NaN | NaN | 2016.000000 | NaN | 450.000000 | NaN | 150000.000000 | 12.000000 | NaN | NaN | NaN | NaN | 99998.000000 | NaN | . Data Transformation . Strings to Datetime Objects . As we saw earlier, the date_crawled, date_created, and last_seen columns contain dates and times in string format. It would be better to store these as datetime objects so that each part (year, month, etc.) can be accessed in analysis. . For this section, I used &quot;Convert the column type from string to datetime format in Pandas dataframe&quot; (2020) as a reference. . First, though, we have to make a format string so that the numbers can be parsed properly. . autos[[&quot;date_crawled&quot;, &quot;date_created&quot;, &quot;last_seen&quot;]].head() . date_crawled date_created last_seen . 3 2016-03-17 16:54:04 | 2016-03-17 00:00:00 | 2016-03-17 17:40:17 | . 4 2016-03-31 17:25:20 | 2016-03-31 00:00:00 | 2016-04-06 10:17:21 | . 5 2016-04-04 17:36:23 | 2016-04-04 00:00:00 | 2016-04-06 19:17:07 | . 6 2016-04-01 20:48:51 | 2016-04-01 00:00:00 | 2016-04-05 18:18:39 | . 10 2016-03-26 19:54:18 | 2016-03-26 00:00:00 | 2016-04-06 10:45:34 | . All 3 columns seem to follow the same format: . {4 digit year}-{2 digit month}-{2 digit day} {24-hour time hour}:{minute}:{second} . The equivalent format string is below. . format_str = &quot;%Y-%m-%d %H:%M:%S&quot; . We can use this string to parse the dates. This can be done quickly by using the pd.to_datetime() vectorized function, instead of using a for-loop with the datetime module. . autos[&quot;date_crawled&quot;] = pd.to_datetime( autos[&quot;date_crawled&quot;], format = format_str, ) autos[&quot;date_crawled&quot;] . 3 2016-03-17 16:54:04 4 2016-03-31 17:25:20 5 2016-04-04 17:36:23 6 2016-04-01 20:48:51 10 2016-03-26 19:54:18 ... 371516 2016-04-04 09:57:12 371517 2016-03-28 13:48:07 371520 2016-03-19 19:53:49 371524 2016-03-05 19:56:21 371525 2016-03-19 18:57:12 Name: date_crawled, Length: 208538, dtype: datetime64[ns] . We can see that the values look similar to how they looked before, but the dtype is now datetime64[ns]. . For example, we can now access the month and day from the datetime objects. . autos[&quot;date_crawled&quot;].iloc[0].month . 3 . autos[&quot;date_crawled&quot;].iloc[0].day . 17 . We can see that the date in the first row has a month of 3 (March) and a day of 17. . Let us do the same for the other 2 date columns. . autos[&quot;date_created&quot;] = pd.to_datetime( autos[&quot;date_created&quot;], format = format_str, ) autos[&quot;last_seen&quot;] = pd.to_datetime( autos[&quot;last_seen&quot;], format = format_str, ) autos[[&quot;date_crawled&quot;, &quot;date_created&quot;, &quot;last_seen&quot;]].describe( datetime_is_numeric = True, ) . date_crawled date_created last_seen . count 208538 | 208538 | 208538 | . mean 2016-03-21 13:39:21.839721984 | 2016-03-20 19:44:12.604321536 | 2016-03-30 09:43:36.598633984 | . min 2016-03-05 14:06:23 | 2015-08-07 00:00:00 | 2016-03-05 14:15:16 | . 25% 2016-03-13 11:06:28 | 2016-03-13 00:00:00 | 2016-03-23 20:52:41.500000 | . 50% 2016-03-21 18:46:44.500000 | 2016-03-21 00:00:00 | 2016-04-04 14:39:49.500000 | . 75% 2016-03-29 15:36:22.500000 | 2016-03-29 00:00:00 | 2016-04-06 11:45:07 | . max 2016-04-07 14:36:58 | 2016-04-07 00:00:00 | 2016-04-07 14:58:51 | . We can see that since the 3 columns are in datetime format, they are now treated as numerical values. They have a mean and a list of percentiles. . Data transformation is done, so we can move on to data analysis. . Data Analysis . Statistical analyses with hypothesis testing are outside of the scope of this project, since it is centered on data cleaning. Thus, I will perform some simple aggregates and visualize them in graphs. . Most Expensive Car Brands . Below, a bar graph shows each car brand and the median price. The mean is not used because we know that the price data are right-skewed, not normal. . plt.figure(figsize = (14, 5)) ax = sns.barplot( data = autos, x = &quot;brand&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Median Prices of Used Cars by Brand&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . The graph shows that Porsche has the highest median price of used cars, at almost 12,000 euros. This is followed by Mini, Land Rover, and Jeep. . Used Car Prices by Age . The line plot below shows the relationship between a car&#39;s year of registration (later -&gt; newer) and the median price. . plt.figure(figsize = (14, 5)) ax = sns.lineplot( data = autos, x = &quot;year_reg&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Median Prices of Used Cars by Year of Registration&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . The line plot shows that used car prices are lowest when the year of registration is in the mid-1990s. . To the right of the minimum, as the year of registration gets later, the median price steadily becomes higher. This makes sense because newer cars would have a higher value, even if they&#39;re used. . On the other hand, to the left of the minimum, the median price increases (albeit erratically) as the year of registration gets earlier. This suggests that very old cars are considered to be valuable because they are rare. It is likely that the avid car collectors are the ones who pay such high prices for these cars. . Used Car Prices by Mileage . The line plot below shows the relationship between used cars&#39; mileage in kilometers and the median price. . plt.figure(figsize = (14, 5)) ax = sns.lineplot( data = autos, x = &quot;mileage_km&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Median Prices of Used Cars by Mileage in Kilometers&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . As expected, price decreases as mileage increases. What is unexpected is that there is a sharp increase in price from around 500 km to 1000 km. This was likely caused by an outlier datapoint which had a low mileage and a low price. . Effect of Presence of Damage on Used Car Price . Lastly, we will analyze the effect of the presence of unrepaired damage on the median price of used cars. . ax = sns.barplot( data = autos, x = &quot;damage&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Presence of Damage on Median Used Car Price&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . It can be seen that the median price of used cars without damage is 4000 euros. The median price is 1500 euros for those with unrepaired damage. . Therefore, cars with damage generally have lower prices. This makes sense because if the buyer plans to drive the car, they must shoulder the expense of repairing it. . Conclusion . In this project, we did cleaning, transformation, and simple analyses on data about used cars. Below are the insights we gained with regards to each research question. Note that prices were aggregated using the median, not the mean. . Which brands of the used cars are most expensive? | . The top 4 most expensive brands of used cars are Porsche, Mini, Land Rover, and Jeep (descending order). . . Does a used car&#39;s age affect its price? | . Yes. Cars registered in the mid-1990s have the lowest prices, with a median of 1000 euros. . As cars get newer from that point, their median price increases. As cars get older from the same point, their median price also increases. Very old cars are rare and therefore valuable to collectors. . . Does a used car&#39;s mileage affect its price? | . Yes, as mileage increases, price decreases. . . Does the presence of unrepaired damage affect a car&#39;s price? | . Yes, the median price of damaged used cars is much lower than that of cars without damage. . . . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/datetime/2021/05/20/Basic-Data-Cleaning-eBay-Car-Sales-Data.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/datetime/2021/05/20/Basic-Data-Cleaning-eBay-Car-Sales-Data.html",
            "date": " • May 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Optimizing Hacker News Posts",
            "content": ". Unsplash | Clint Patterson Overview . Hacker News is a popular website about technology. Specifically, it is a community choice aggregator of tech content. Users can: . Submit tech articles that the user found online. | Submit &quot;Ask&quot; posts to ask the community a question. | Submit &quot;Show&quot; posts to show the community something that the user made. | Vote and comment on other people&#39;s posts. | . In this project, we will analyze and compare Ask posts and Show posts in order to answer the following questions: . Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post? | Can posting at a certain time of day result in getting more comments? | . This analysis can be helpful for Hacker News users who would like for their posts to reach a larger audience on the platform. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Exploring Hacker News Posts. The research questions and general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import altair as alt import datetime as dt . Dataset . The dataset for this project is the Hacker News Posts dataset on Kaggle, uploaded by Hacker News. . The following is quoted from the dataset&#39;s Description. . This data set is Hacker News posts from the last 12 months (up to September 26 2016). . It includes the following columns: . title: title of the post (self explanatory) | url: the url of the item being linked to | num_points: the number of upvotes the post received | num_comments: the number of comments the post received | author: the name of the account that made the post | created_at: the date and time the post was made (the time zone is Eastern Time in the US) | . Let us view the first 5 rows of the dataset below. . hn = pd.read_csv(&quot;./2021-05-11-OHNP-Files/HN_posts_year_to_Sep_26_2016.csv&quot;) hn.head() . id title url num_points num_comments author created_at . 0 12579008 | You have two days to comment if you want stem ... | http://www.regulations.gov/document?D=FDA-2015... | 1 | 0 | altstar | 9/26/2016 3:26 | . 1 12579005 | SQLAR the SQLite Archiver | https://www.sqlite.org/sqlar/doc/trunk/README.md | 1 | 0 | blacksqr | 9/26/2016 3:24 | . 2 12578997 | What if we just printed a flatscreen televisio... | https://medium.com/vanmoof/our-secrets-out-f21... | 1 | 0 | pavel_lishin | 9/26/2016 3:19 | . 3 12578989 | algorithmic music | http://cacm.acm.org/magazines/2011/7/109891-al... | 1 | 0 | poindontcare | 9/26/2016 3:16 | . 4 12578979 | How the Data Vault Enables the Next-Gen Data W... | https://www.talend.com/blog/2016/05/12/talend-... | 1 | 0 | markgainor1 | 9/26/2016 3:14 | . Below is the shape of the dataset. . print(hn.shape) . (293119, 7) . There are 293,119 rows and 7 columns in the dataset. . Before the data can be analyzed, it must first be cleaned. . Data Cleaning . Duplicate Rows . Below, I use pandas to delete duplicate rows except for the first instance of each duplicate. . Rows will be considered as duplicates if they are exactly alike in all features. I decided on this because it is possible for two posts to have the same title and/or url but be posted at different times or by different users. Thus, we cannot identify duplicates based on one or two features alone. . hn = hn.drop_duplicates(keep = &quot;first&quot;) print(hn.shape) . (293119, 7) . No duplicates were found. All rows were kept. . Posts without Comments . Our research questions involve the number of comments on each post. However, there are many posts with 0 comments. . To illustrate this, below a frequency table of the number of comments on each post. . def freq_comments(df = hn): &quot;&quot;&quot;Function to make a frequency table of the number of comments per post specifically for the Hacker News dataset.&quot;&quot;&quot; freq_df = df[&quot;num_comments&quot;].value_counts().reset_index() freq_df.columns = [&quot;num_comments&quot;, &quot;frequency&quot;] freq_df = freq_df.sort_values( by = &quot;num_comments&quot;, ).reset_index( drop = True, ) return freq_df freq_df1 = freq_comments() freq_df1 . num_comments frequency . 0 0 | 212718 | . 1 1 | 28055 | . 2 2 | 9731 | . 3 3 | 5016 | . 4 4 | 3272 | . ... ... | ... | . 543 1007 | 1 | . 544 1120 | 1 | . 545 1448 | 1 | . 546 1733 | 1 | . 547 2531 | 1 | . 548 rows × 2 columns . The table above shows that posts with 0 comments are most frequent. . Let us plot the table on a histogram. . def hist_comments(df, title): &quot;&quot;&quot;Function to make a histogram of the number of comments per post specifically for the Hacker News dataset.&quot;&quot;&quot; chart = alt.Chart(df).mark_bar().encode( x = alt.X( &quot;num_comments:Q&quot;, title = &quot;Number of Comments&quot;, bin = alt.Bin(step = 1) ), y = alt.Y( &quot;frequency:Q&quot;, title = &quot;Frequency&quot;, ), ).properties( title = title, width = 700, height = 400, ) return chart hist_comments(freq_df1, &quot;Histogram of Number of Comments per Post&quot;) . There are so many posts with 0 comments that we cannot see the histogram bins for other numbers of comments. . Considering that the dataset is large and most rows have 0 comments, it would be best to drop all rows with 0 comments. This would make analysis less computationally expensive and allow us to answer our research questions. . with_comments = hn[&quot;num_comments&quot;] &gt; 0 hn = hn.loc[with_comments].reset_index(drop = True) print(hn.shape) . (80401, 7) . Now, the dataset is left with only 80,401 rows. This will be easier to work with. . Below is the new histogram. . freq_df2 = freq_comments() hist_comments(freq_df2, &quot;Histogram of Number of Comments per Post&quot;) . The distribution is still heavily right-skewed since many posts have very few comments. What&#39;s important is that unnecessary data has been removed. . Missing Values . Finally, let us remove rows with missing values. In order to answer our research questions, we only need the following columns: . title | num_comments | created_at | . Thus, we will delete rows with missing values in this column. . hn.dropna( subset = [&quot;title&quot;, &quot;num_comments&quot;, &quot;created_at&quot;], inplace = True, ) print(hn.shape) . (80401, 7) . The number of rows did not change from 80401. Therefore, no missing values were found in these columns, and no rows were dropped. . Data cleaning is now done. . Filtering Posts . As mentioned earlier, the first research question involves comparing Ask posts to Show posts. In order to do this, we have to group the posts into three types: . Ask Posts | Show Posts | Other Posts | . Other posts are usually posts that share a tech article found online. . Ask and Show posts can be identified using the start of the post title. Ask posts start with &quot;Ask HN: &quot;. . ask_mask = [index for index, value in hn[&quot;title&quot;].iteritems() if value.startswith(&quot;Ask HN: &quot;) ] hn.loc[ask_mask].head() . id title url num_points num_comments author created_at . 1 12578908 | Ask HN: What TLD do you use for local developm... | NaN | 4 | 7 | Sevrene | 9/26/2016 2:53 | . 6 12578522 | Ask HN: How do you pass on your work when you ... | NaN | 6 | 3 | PascLeRasc | 9/26/2016 1:17 | . 18 12577870 | Ask HN: Why join a fund when you can be an angel? | NaN | 1 | 3 | anthony_james | 9/25/2016 22:48 | . 27 12577647 | Ask HN: Someone uses stock trading as passive ... | NaN | 5 | 2 | 00taffe | 9/25/2016 21:50 | . 41 12576946 | Ask HN: How hard would it be to make a cheap, ... | NaN | 2 | 1 | hkt | 9/25/2016 19:30 | . On the other hand, Show posts start with &quot;Show HN: &quot;. . show_mask = [index for index, value in hn[&quot;title&quot;].iteritems() if value.startswith(&quot;Show HN: &quot;) ] hn.loc[show_mask].head() . id title url num_points num_comments author created_at . 35 12577142 | Show HN: Jumble Essays on the go #PaulInYourP... | https://itunes.apple.com/us/app/jumble-find-st... | 1 | 1 | ryderj | 9/25/2016 20:06 | . 43 12576813 | Show HN: Learn Japanese Vocab via multiple cho... | http://japanese.vul.io/ | 1 | 1 | soulchild37 | 9/25/2016 19:06 | . 52 12576090 | Show HN: Markov chain Twitter bot. Trained on ... | https://twitter.com/botsonasty | 3 | 1 | keepingscore | 9/25/2016 16:50 | . 68 12575471 | Show HN: Project-Okot: Novel, CODE-FREE data-a... | https://studio.nuchwezi.com/ | 3 | 1 | nfixx | 9/25/2016 14:30 | . 88 12574773 | Show HN: Cursor that Screenshot | http://edward.codes/cursor-that-screenshot | 3 | 3 | ed-bit | 9/25/2016 10:50 | . Other posts do not start with any special label. . Below, I create a new column &quot;post_type&quot; and assign the appropriate value to each row. . hn[&quot;post_type&quot;] = &quot;Other&quot; hn.loc[ask_mask, &quot;post_type&quot;] = &quot;Ask&quot; hn.loc[show_mask, &quot;post_type&quot;] = &quot;Show&quot; hn[[&quot;title&quot;, &quot;post_type&quot;]] . title post_type . 0 Saving the Hassle of Shopping | Other | . 1 Ask HN: What TLD do you use for local developm... | Ask | . 2 Amazons Algorithms Dont Find You the Best Deals | Other | . 3 Emergency dose of epinephrine that does not co... | Other | . 4 Phone Makers Could Cut Off Drivers. So Why Don... | Other | . ... ... | ... | . 80396 My Keyboard | Other | . 80397 Google&#39;s new logo was created by Russian desig... | Other | . 80398 Why we aren&#39;t tempted to use ACLs on our Unix ... | Other | . 80399 Ask HN: What is/are your favorite quote(s)? | Ask | . 80400 Dying vets fuck you letter (2013) | Other | . 80401 rows × 2 columns . Each row has now been labeled as a type of post. . Research Question 1: Comparing Ask and Show Posts . The first research question is, &quot;Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post?&quot; . Note that the data is not normally distributed; it is right-skewed. For example, here is the distribution of the number of comments per Ask post. . ask_freq = freq_comments( df = hn.loc[hn[&quot;post_type&quot;] == &quot;Ask&quot;] ) ask_freq . num_comments frequency . 0 1 | 1383 | . 1 2 | 1238 | . 2 3 | 762 | . 3 4 | 592 | . 4 5 | 373 | . ... ... | ... | . 203 898 | 1 | . 204 910 | 1 | . 205 937 | 1 | . 206 947 | 1 | . 207 1007 | 1 | . 208 rows × 2 columns . hist_comments( ask_freq, &quot;Histogram of Number of Comments per Ask Post&quot; ) . The histogram is similar for Show posts. . show_freq = freq_comments( df = hn.loc[hn[&quot;post_type&quot;] == &quot;Show&quot;], ) show_freq . num_comments frequency . 0 1 | 1738 | . 1 2 | 814 | . 2 3 | 504 | . 3 4 | 300 | . 4 5 | 196 | . ... ... | ... | . 137 250 | 1 | . 138 257 | 1 | . 139 280 | 1 | . 140 298 | 1 | . 141 306 | 1 | . 142 rows × 2 columns . hist_comments( show_freq, &quot;Histogram of Number of Comments per Show Post&quot; ) . Therefore, the mean would not be a good measure of central tendency for the &quot;average number of comments per post.&quot; Thus, we will use the median instead. . dct = {&quot;Ask&quot;: None, &quot;Show&quot;: None} for key in dct: median = np.median( hn[&quot;num_comments&quot;].loc[hn[&quot;post_type&quot;] == key] ) dct[key] = median table = pd.DataFrame.from_dict( dct, orient = &quot;index&quot;, ).reset_index( ).rename(columns = { &quot;index&quot;: &quot;post_type&quot;, 0: &quot;median_comments&quot;, }) chart = alt.Chart(table).mark_bar().encode( y = alt.Y(&quot;post_type:N&quot;, title = &quot;Post Type&quot;), x = alt.X(&quot;median_comments:Q&quot;, title = &quot;Median Number of Comments per Post&quot;), ).properties( title = &quot;Median Number of Comments for the Two Post Types&quot;, ) chart . The bar graph shows that Show posts have a higher median number of comments per post, compared to Ask posts. . Important: The results suggest that Show posts get more comments than Ask posts. It may be easier for users to reach a larger audience via Show posts. . Research Question 2: Active Times . The second research question is, &quot;Can posting at a certain time of day result in getting more comments?&quot; . For this part of the analysis, we will only be using Show post data for simplicity. . We will divide the day into 24 one-hour periods, and then calculate the number of Show posts created in each period. . String Template for Time . Before analying, we need to inspect the &quot;created_at&quot; column of the dataset. . hn_show = hn.loc[ hn[&quot;post_type&quot;] == &quot;Show&quot; ].reset_index( drop = True, ) hn_show[[&quot;created_at&quot;]].head() . created_at . 0 9/25/2016 20:06 | . 1 9/25/2016 19:06 | . 2 9/25/2016 16:50 | . 3 9/25/2016 14:30 | . 4 9/25/2016 10:50 | . The strings in this column appear to follow the following format: . month/day/year hour:minute . With the datetime module, the following is the equivalent formatting template. . template = &quot;%m/%d/%Y %H:%M&quot; . Parsing Times . The time data can now be parsed and used for analysis. . The hours_posts dictionary will count the number of posts at certain hours. The hours_comments dictionary will count the number of comments received by posts made at certain hours. . hours_posts = {} hours_comments = {} for index, row in hn_show.iterrows(): date_str = row[&quot;created_at&quot;] num_comments = row[&quot;num_comments&quot;] # datetime object date_dt = dt.datetime.strptime( date_str, template, ) # extract hour hour = date_dt.hour # update dictionaries hours_posts.setdefault(hour, 0) hours_posts[hour] += 1 hours_comments.setdefault(hour, 0) hours_comments[hour] += num_comments . The hours were parsed and mapped to their respective counts of posts and comments. . The code below transforms the dictionaries into DataFrames for ease of use. . def hour_to_df(dct, data_label): &quot;&quot;&quot;Make a DataFrame from a dictionary that maps an &#39;hour&#39; column to another column, named by `data_label`.&quot;&quot;&quot; result = pd.DataFrame.from_dict( dct, orient = &quot;index&quot;, ).reset_index( ).rename(columns = { &quot;index&quot;: &quot;hour&quot;, 0: data_label, }).sort_values( by = &quot;hour&quot;, ).reset_index( drop = True, ) return result hours_posts_df = hour_to_df( hours_posts, data_label = &quot;num_posts&quot;, ) hours_comments_df = hour_to_df( hours_comments, data_label = &quot;num_comments&quot;, ) hours_posts_df.head() . hour num_posts . 0 0 | 141 | . 1 1 | 133 | . 2 2 | 103 | . 3 3 | 97 | . 4 4 | 90 | . hours_comments_df.head() . hour num_comments . 0 0 | 1283 | . 1 1 | 1001 | . 2 2 | 1074 | . 3 3 | 934 | . 4 4 | 978 | . The hours have been parsed, and the tables have been generated. . Additionally, another DataFrame is created below. It calculates the average number of comments per post by the hour posted. . frames = [hours_posts_df, hours_comments_df] hours_mean = pd.concat(frames, axis = 1).drop_duplicates() # Drop duplicate columns hours_mean = hours_mean.loc[:, ~hours_mean.columns.duplicated()] means = [] for index, row in hours_mean.iterrows(): hour, posts, comments = row means.append(comments / posts) hours_mean[&quot;comments_per_post&quot;] = means hours_mean . hour num_posts num_comments comments_per_post . 0 0 | 141 | 1283 | 9.099291 | . 1 1 | 133 | 1001 | 7.526316 | . 2 2 | 103 | 1074 | 10.427184 | . 3 3 | 97 | 934 | 9.628866 | . 4 4 | 90 | 978 | 10.866667 | . 5 5 | 75 | 591 | 7.880000 | . 6 6 | 95 | 904 | 9.515789 | . 7 7 | 125 | 1572 | 12.576000 | . 8 8 | 159 | 1770 | 11.132075 | . 9 9 | 158 | 1411 | 8.930380 | . 10 10 | 155 | 1225 | 7.903226 | . 11 11 | 227 | 2412 | 10.625551 | . 12 12 | 300 | 3609 | 12.030000 | . 13 13 | 334 | 3314 | 9.922156 | . 14 14 | 331 | 3839 | 11.598187 | . 15 15 | 390 | 3822 | 9.800000 | . 16 16 | 387 | 3767 | 9.733850 | . 17 17 | 368 | 3228 | 8.771739 | . 18 18 | 311 | 3242 | 10.424437 | . 19 19 | 270 | 2791 | 10.337037 | . 20 20 | 246 | 2183 | 8.873984 | . 21 21 | 209 | 1759 | 8.416268 | . 22 22 | 192 | 1450 | 7.552083 | . 23 23 | 147 | 1443 | 9.816327 | . Number of Posts by Hour of the Day . Below are the table and graph for the number of posts per hour of the day. . hours_posts_df . hour num_posts . 0 0 | 141 | . 1 1 | 133 | . 2 2 | 103 | . 3 3 | 97 | . 4 4 | 90 | . 5 5 | 75 | . 6 6 | 95 | . 7 7 | 125 | . 8 8 | 159 | . 9 9 | 158 | . 10 10 | 155 | . 11 11 | 227 | . 12 12 | 300 | . 13 13 | 334 | . 14 14 | 331 | . 15 15 | 390 | . 16 16 | 387 | . 17 17 | 368 | . 18 18 | 311 | . 19 19 | 270 | . 20 20 | 246 | . 21 21 | 209 | . 22 22 | 192 | . 23 23 | 147 | . This table is in 24-hour time. Hour 13 refers to 1:00 PM. The table shows how many posts are made for every hour in the day. . Below is a line chart that shows this visually. . chart = alt.Chart(hours_posts_df).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;num_posts:Q&quot;, title = &quot;Number of Posts&quot;), ).properties( title = &quot;Number of Posts by Hour of the Day&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . The histogram clearly shows that Hacker News users most actively make posts between 15:00 and 18:00, or from 3:00 PM to 6:00 PM. . The most active hour for posting is 3:00 PM - 4:00 PM. . Number of Comments by Hour Posted . Next, a similar analysis is done for the number of comments received by posts, grouped by the hour that they were created. Below is the table for this data. . hours_comments_df . hour num_comments . 0 0 | 1283 | . 1 1 | 1001 | . 2 2 | 1074 | . 3 3 | 934 | . 4 4 | 978 | . 5 5 | 591 | . 6 6 | 904 | . 7 7 | 1572 | . 8 8 | 1770 | . 9 9 | 1411 | . 10 10 | 1225 | . 11 11 | 2412 | . 12 12 | 3609 | . 13 13 | 3314 | . 14 14 | 3839 | . 15 15 | 3822 | . 16 16 | 3767 | . 17 17 | 3228 | . 18 18 | 3242 | . 19 19 | 2791 | . 20 20 | 2183 | . 21 21 | 1759 | . 22 22 | 1450 | . 23 23 | 1443 | . Below is the line chart that visualizes the data about the number of comments received by the hour posted. . chart = alt.Chart(hours_comments_df).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;num_comments:Q&quot;, title = &quot;Number of Comments&quot;), ).properties( title = &quot;Number of Comments by Hour Posted&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . The results in this histogram are similar to the previous one. Posts that are made from 14:00 to 17:00, or 2:00 PM - 5:00 PM, receive the most comments. . Posts made at 2:00 PM - 3:00 PM receive an especially high number of comments. . Mean Number of Comments per Post, by Hour Posted . The table below shows the mean number of comments per post, by the hour of posting. . hours_mean . hour num_posts num_comments comments_per_post . 0 0 | 141 | 1283 | 9.099291 | . 1 1 | 133 | 1001 | 7.526316 | . 2 2 | 103 | 1074 | 10.427184 | . 3 3 | 97 | 934 | 9.628866 | . 4 4 | 90 | 978 | 10.866667 | . 5 5 | 75 | 591 | 7.880000 | . 6 6 | 95 | 904 | 9.515789 | . 7 7 | 125 | 1572 | 12.576000 | . 8 8 | 159 | 1770 | 11.132075 | . 9 9 | 158 | 1411 | 8.930380 | . 10 10 | 155 | 1225 | 7.903226 | . 11 11 | 227 | 2412 | 10.625551 | . 12 12 | 300 | 3609 | 12.030000 | . 13 13 | 334 | 3314 | 9.922156 | . 14 14 | 331 | 3839 | 11.598187 | . 15 15 | 390 | 3822 | 9.800000 | . 16 16 | 387 | 3767 | 9.733850 | . 17 17 | 368 | 3228 | 8.771739 | . 18 18 | 311 | 3242 | 10.424437 | . 19 19 | 270 | 2791 | 10.337037 | . 20 20 | 246 | 2183 | 8.873984 | . 21 21 | 209 | 1759 | 8.416268 | . 22 22 | 192 | 1450 | 7.552083 | . 23 23 | 147 | 1443 | 9.816327 | . This is visualized in the line chart below, which looks quite different from the previous two charts. . chart = alt.Chart(hours_mean).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;comments_per_post:Q&quot;, title = &quot;Mean Number of Comments per Post&quot;), ).properties( title = &quot;Mean Number of Comments per Post, by Hour Posted&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . This graph shows that the mean number of comments per post is more consistent throughout the day. The statistic is highest at 7:00 AM - 8:00 AM, but the afternoon values are only slightly lower. . This brings us a new question: Why does the mean number of comments per post not peak in the afternoon? . A possible explanation is that since the site is oversaturated with new posts in the afternoon, only the very best posts receive attention. The rest are lost in the flood of new posts. . Important: Based on the results, even if users are most active in the afternoon, it may be best to post in the morning. . If you post in the morning, you will receive at least some attention since you won&#39;t have to compete with many other new posts. You may get a few upvotes and comments. Then, the many users logging in during the afternoon would see that your posted has already received comments. Thus, they would be interested in it and bring more attention to it. . Conclusion . In this project, we analyzed data about Hacker News posts, specifically regarding the number of comments that they receive. Below are the research questions, and the best answers that we could come up with from our analysis. . . Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post? | . Show posts receive a higher median number of comments per post, compared to Ask posts. In order to reach a wider audience or converse with more users, it is better to make a Show post. . . Can posting at a certain time of day result in getting more comments? | . Hacker News users are very active in the afternoon, from 2:00 PM to 6:00 PM. However, if you post in the afternoon, your post may get lost in the flood of new posts. It is better to post in the morning, like at 7:00 AM, so that some people can notice your post. Then, your post can get more attention in the afternoon. . . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/altair/datetime/2021/05/11/Optimizing-Hacker-News-Posts.html",
            "relUrl": "/python/pandas/numpy/altair/datetime/2021/05/11/Optimizing-Hacker-News-Posts.html",
            "date": " • May 11, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Profitable App Profiles for iOS and Android",
            "content": ". Unsplash | Alexander Shatov Overview . Welcome. In this project, we will be working on data about different types of apps and their corresponding number of users. The goal is to determine which apps can best attract the largest number of users. This will help a hypothetical app company make decisions regarding what apps to develop in the near future. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Profitable App Profiles for the App Store and Google Play Markets. The hypothetical app company and the general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . App Company&#39;s Context . First, we must know the context of the hypothetical app company so that we can align our analysis with it. . This company only makes free apps directed toward an English-speaking audience. They get revenue from in-app advertisements and purchases. Thus, they rely on having a large number of users so that they can generate more revenue. . Additionally, their apps should ideally be successful on both Google Play Store and the Apple App Store. The reason is that the company has the following 3-step validation strategy: . (from the Dataquest guided project) . Build a minimal Android version of the app, and add it to Google Play. | If the app has a good response from users, we develop it further. | If the app is profitable after six months, we build an iOS version of the app and add it to the App Store. | We&#39;ll take this information into consideration throughout our analysis. . Package Installs . import pandas as pd import numpy as np import altair as alt import re . App Data Overview . This project uses two datasets. . The Google Play Store dataset lists over 10,000 Android apps. | The Apple App Store dataset lists over 7,000 iOS apps. | . data_apple = pd.read_csv(&quot;./2021-05-08-PAP-Files/AppleStore.csv&quot;, header = 0) data_google = pd.read_csv(&quot;./2021-05-08-PAP-Files/googleplaystore.csv&quot;, header = 0) . Apple App Store dataset . print(data_apple.shape) . (7197, 16) . The dataset has 7197 rows (1 row per app), and 16 columns which describe these apps. . According to the Kaggle documentation (Mobile App Store ( 7200 apps)), the following are the columns and their meanings. . &quot;id&quot; : App ID | &quot;track_name&quot;: App Name | &quot;size_bytes&quot;: Size (in Bytes) | &quot;currency&quot;: Currency Type | &quot;price&quot;: Price amount | &quot;ratingcounttot&quot;: User Rating counts (for all version) | &quot;ratingcountver&quot;: User Rating counts (for current version) | &quot;user_rating&quot; : Average User Rating value (for all version) | &quot;userratingver&quot;: Average User Rating value (for current version) | &quot;ver&quot; : Latest version code | &quot;cont_rating&quot;: Content Rating | &quot;prime_genre&quot;: Primary Genre | &quot;sup_devices.num&quot;: Number of supporting devices | &quot;ipadSc_urls.num&quot;: Number of screenshots showed for display | &quot;lang.num&quot;: Number of supported languages | &quot;vpp_lic&quot;: Vpp Device Based Licensing Enabled | . A sample of the first 5 rows of the dataset is shown below. . data_apple.head() . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 0 284882215 | Facebook | 389879808 | USD | 0.0 | 2974676 | 212 | 3.5 | 3.5 | 95.0 | 4+ | Social Networking | 37 | 1 | 29 | 1 | . 1 389801252 | Instagram | 113954816 | USD | 0.0 | 2161558 | 1289 | 4.5 | 4.0 | 10.23 | 12+ | Photo &amp; Video | 37 | 0 | 29 | 1 | . 2 529479190 | Clash of Clans | 116476928 | USD | 0.0 | 2130805 | 579 | 4.5 | 4.5 | 9.24.12 | 9+ | Games | 38 | 5 | 18 | 1 | . 3 420009108 | Temple Run | 65921024 | USD | 0.0 | 1724546 | 3842 | 4.5 | 4.0 | 1.6.2 | 9+ | Games | 40 | 5 | 1 | 1 | . 4 284035177 | Pandora - Music &amp; Radio | 130242560 | USD | 0.0 | 1126879 | 3594 | 4.0 | 4.5 | 8.4.1 | 12+ | Music | 37 | 4 | 1 | 1 | . Google Play Store dataset . print(data_google.shape) . (10841, 13) . The dataset has 10841 rows and 13 columns. . The column names are self-explanatory, so the Kaggle documentation (Google Play Store Apps) does not describe them. . print(list(data_google.columns)) . [&#39;App&#39;, &#39;Category&#39;, &#39;Rating&#39;, &#39;Reviews&#39;, &#39;Size&#39;, &#39;Installs&#39;, &#39;Type&#39;, &#39;Price&#39;, &#39;Content Rating&#39;, &#39;Genres&#39;, &#39;Last Updated&#39;, &#39;Current Ver&#39;, &#39;Android Ver&#39;] . Below is a sample of the dataset. . data_google.head() . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 1 Coloring book moana | ART_AND_DESIGN | 3.9 | 967 | 14M | 500,000+ | Free | 0 | Everyone | Art &amp; Design;Pretend Play | January 15, 2018 | 2.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . Data Cleaning . Before analysis, the data must first be cleaned of unwanted datapoints. . Inaccurate Data . This Kaggle discussion about the Google Play dataset indicates that row 10472 (excluding the header) has an error. . Below, I have printed row 0 and row 10472 so that these can be compared. . data_google.iloc[[0, 10472]] . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 10472 Life Made WI-Fi Touchscreen Photo Frame | 1.9 | 19.0 | 3.0M | 1,000+ | Free | 0 | Everyone | NaN | February 11, 2018 | 1.0.19 | 4.0 and up | NaN | . As we look at row 10472 in the context of the column headers and row 0, the following things become clear. . The &quot;Category&quot; value is not present. Thus, all values to the right of it have been shifted leftward. | The &quot;Android Ver&quot; column was left with a missing value. | . Thus, this row will be removed. . if data_google.iloc[10472, 0] == &#39;Life Made WI-Fi Touchscreen Photo Frame&#39;: # This if-statement prevents more rows from being deleted # if the cell is run again. data_google.drop(10472, inplace = True) print(&quot;The inaccurate row was deleted.&quot;) . The inaccurate row was deleted. . Duplicate Data . There are also duplicate app entries in the Google Play dataset. We can consider a row as a duplicates if another row exists that has the same &quot;App&quot; value. . Here, I count the total number of duplicate rows. This turns out to be 1979 rows. . def count_duplicates(df, col_name): &quot;&quot;&quot;Count the number of duplicate rows in a DataFrame. `col_name` is the name of the column to be used as a basis for duplicate values.&quot;&quot;&quot; all_apps = {} for index, row in df.iterrows(): name = row[col_name] all_apps.setdefault(name, []).append(index) duplicate_inds = [ind for lst in all_apps.values() for ind in lst if len(lst) &gt; 1] n_duplicates = &quot;Duplicates: {}&quot;.format(len(duplicate_inds)) duplicate_rows = df.iloc[duplicate_inds] return n_duplicates, duplicate_rows google_dupes = count_duplicates(data_google, &quot;App&quot;) print(google_dupes[0]) . Duplicates: 1979 . As an example, there are 4 rows for Instagram: . ig_filter = data_google[&quot;App&quot;] == &quot;Instagram&quot; ig_rows = data_google.loc[ig_filter] . ig_rows . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 2545 Instagram | SOCIAL | 4.5 | 66577313 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 2604 Instagram | SOCIAL | 4.5 | 66577446 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 2611 Instagram | SOCIAL | 4.5 | 66577313 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 3909 Instagram | SOCIAL | 4.5 | 66509917 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . Looking closely, we can see that duplicate rows are not exactly identical. The &quot;Reviews&quot; column, which shows the total number of reviews of the app, has different values. . It can be inferred that the row with the largest value is the newest entry for the app. Therefore, all duplicate rows will be dropped except for the ones with the largest &quot;Reviews&quot; values. . def remove_duplicates(df, name_col, reviews_col): # Each key-value pair will follow the format: # {&quot;App Name&quot;: maximum number of reviews among all duplicates} reviews_max = {} for index, row in df.iterrows(): name = row[name_col] n_reviews = int(row[reviews_col]) if n_reviews &gt; reviews_max.get(name, -1): reviews_max[name] = n_reviews # List of duplicate indices to drop, # excluding the row with the highest number of reviews # among that app&#39;s duplicate rows. indices_to_drop = [] # Rows with names that have already been added into this list # will be dropped. already_added = [] for index, row in df.iterrows(): name = row[name_col] n_reviews = int(row[reviews_col]) if (name not in already_added) and (n_reviews == reviews_max[name]): already_added.append(name) else: indices_to_drop.append(index) # Remove duplicates and return the clean dataset. clean = df.drop(indices_to_drop) return clean android_clean = remove_duplicates(data_google, &quot;App&quot;, &quot;Reviews&quot;) print(android_clean.shape) . (9659, 13) . After duplicates were removed, the Google Play dataset was left with 9659 rows. . As for the Apple App Store dataset, there are 4 duplicate rows. . apple_dupes = count_duplicates(data_apple, &quot;track_name&quot;) print(apple_dupes[0]) apple_dupes[1] . Duplicates: 4 . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 2948 1173990889 | Mannequin Challenge | 109705216 | USD | 0.0 | 668 | 87 | 3.0 | 3.0 | 1.4 | 9+ | Games | 37 | 4 | 1 | 1 | . 4463 1178454060 | Mannequin Challenge | 59572224 | USD | 0.0 | 105 | 58 | 4.0 | 4.5 | 1.0.1 | 4+ | Games | 38 | 5 | 1 | 1 | . 4442 952877179 | VR Roller Coaster | 169523200 | USD | 0.0 | 107 | 102 | 3.5 | 3.5 | 2.0.0 | 4+ | Games | 37 | 5 | 1 | 1 | . 4831 1089824278 | VR Roller Coaster | 240964608 | USD | 0.0 | 67 | 44 | 3.5 | 4.0 | 0.81 | 4+ | Games | 38 | 0 | 1 | 1 | . The &quot;rating_count_tot&quot; column in the Apple App Store dataset is like the &quot;Reviews&quot; column in the Google Play dataset. It tells the total number of reviews so far. Therefore, Apple App Store dataset duplicates can be removed by keeping the rows with the highest rating count totals. . ios_clean = remove_duplicates(data_apple, &quot;track_name&quot;, &quot;rating_count_tot&quot;) print(ios_clean.shape) . (7195, 16) . From 7197 rows, there are now 7195 rows in the Apple App Store dataset. . Non-English Apps . The hypothetical app company who will use this analysis is a company that only makes apps in English. Thus, all apps with non-English titles shall be removed from the datasets. . The task now is to identify titles which are not in English. It is known that in the ASCII table, the characters most commonly used in English are within codes 0 to 127. Some English app titles may have special characters or emojis, though, so I will only remove titles which have more than 3 characters outside of the normal range. . def is_english(text): unicode = [ord(char) for char in text] normal = [(code &gt;= 0 and code &lt;= 127) for code in unicode] non_english = len(text) - sum(normal) return non_english &lt;= 3 def keep_english(df, name_col): &quot;&quot;&quot;Return a new DataFrame containing only rows with English names.&quot;&quot;&quot; remove_indices = [] for index, row in df.iterrows(): name = row[name_col] if not is_english(name): remove_indices.append(index) return df.drop(remove_indices) android_clean = keep_english(android_clean, &quot;App&quot;) ios_clean = keep_english(ios_clean, &quot;track_name&quot;) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (9614, 13) Apple App Store Dataset: (6181, 16) . Now, there are only English apps in both datasets. . Paid Apps . As mentioned earlier, the app company only makes free apps. Therefore, data on paid apps is irrelevant to this analysis. Paid apps shall be identified and removed from both datasets. . def remove_paid(df, price_col): &quot;&quot;&quot;Return a new DataFrame without paid apps.&quot;&quot;&quot; remove_indices = [] for index, row in df.iterrows(): price = str(row[price_col]) # Keep characters that are numeric or periods. price = float(re.sub(&quot;[^0-9.]&quot;, &quot;&quot;, price)) if price != 0.0: remove_indices.append(index) return df.drop(remove_indices) android_clean = remove_paid(android_clean, &quot;Price&quot;) ios_clean = remove_paid(ios_clean, &quot;price&quot;) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (8864, 13) Apple App Store Dataset: (3220, 16) . The datasets were left with 8864 apps in Google Play and 3220 apps in the App Store. . Missing Data . Lastly, let us remove rows with missing data. Note that it would be wasteful to remove rows with missing data in columns that we will not inspect. Therefore, we will only remove rows with missing data in relevant columns. (Why these are relevant will be explained later.) These would be the following. . Google Play Store dataset . App | Category | Installs | Genres | . Apple App Store dataset . track_name | prime_genre | rating_count_tot | . I will now remove all rows with missing values in these columns. . android_clean.dropna( subset = [&quot;App&quot;, &quot;Category&quot;, &quot;Installs&quot;, &quot;Genres&quot;], inplace = True, ) ios_clean.dropna( subset = [&quot;track_name&quot;, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;], inplace = True, ) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (8864, 13) Apple App Store Dataset: (3220, 16) . These are the same shapes as before. Therefore, there were no missing values in the relevant columns. No datapoints were removed at this step. . Data cleaning is done, so now we can move on to the analysis. . Common App Genres . Now that the data has been cleaned, let us find out which genres of apps are most common in both app markets. If an app genre is common, then there may be high demand for it among users. . Which columns in the datasets can give information about the app genres? . print(&quot;Google Play Store&quot;) android_clean.head() . Google Play Store . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . 5 Paper flowers instructions | ART_AND_DESIGN | 4.4 | 167 | 5.6M | 50,000+ | Free | 0 | Everyone | Art &amp; Design | March 26, 2017 | 1.0 | 2.3 and up | . print(&quot; nApple App Store&quot;) ios_clean.head() . Apple App Store . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 0 284882215 | Facebook | 389879808 | USD | 0.0 | 2974676 | 212 | 3.5 | 3.5 | 95.0 | 4+ | Social Networking | 37 | 1 | 29 | 1 | . 1 389801252 | Instagram | 113954816 | USD | 0.0 | 2161558 | 1289 | 4.5 | 4.0 | 10.23 | 12+ | Photo &amp; Video | 37 | 0 | 29 | 1 | . 2 529479190 | Clash of Clans | 116476928 | USD | 0.0 | 2130805 | 579 | 4.5 | 4.5 | 9.24.12 | 9+ | Games | 38 | 5 | 18 | 1 | . 3 420009108 | Temple Run | 65921024 | USD | 0.0 | 1724546 | 3842 | 4.5 | 4.0 | 1.6.2 | 9+ | Games | 40 | 5 | 1 | 1 | . 4 284035177 | Pandora - Music &amp; Radio | 130242560 | USD | 0.0 | 1126879 | 3594 | 4.0 | 4.5 | 8.4.1 | 12+ | Music | 37 | 4 | 1 | 1 | . For Google Play, some columns that seem relevant are &quot;Category&quot; and &quot;Genres&quot;. For the Apple App Store, the relevant column is &quot;prime_genre&quot;. . We can determine the most common genres by using frequency tables of the mentioned columns. . def freq_table(df, label): &quot;&quot;&quot;Return a frequency table of the values in a column of a DataFrame.&quot;&quot;&quot; col = df[label] freq = {} for value in col: freq.setdefault(value, 0) freq[value] += 1 for key in freq: freq[key] /= len(df) / 100 freq_series = pd.Series(freq).sort_values(ascending = False) return freq_series def sr_to_df(sr, col_name = &quot;number&quot;, n_head = None): &quot;&quot;&quot;Return a DataFrame by resetting the index of a Series.&quot;&quot;&quot; df = sr.rename(col_name).reset_index().rename(columns = {&quot;index&quot;:&quot;name&quot;}) if n_head is not None: df = df.head(n_head) return df google_categories = freq_table(android_clean, &quot;Category&quot;) google_genres = freq_table(android_clean, &quot;Genres&quot;) apple_genres = freq_table(ios_clean, &quot;prime_genre&quot;) . The frequency tables will be inspected in the sections below. Only the top positions in each table will be shown, for brevity. . Apple App Store: Prime Genres . First, the frequency table of Apple App Store prime genres shall be analyzed. Below, I have ordered the table by frequency, descending. I have also made bar graphs showing the top 10 positions in each frequency table. . sr_to_df(apple_genres, &quot;percentage&quot;, n_head = 5) . name percentage . 0 Games | 58.136646 | . 1 Entertainment | 7.888199 | . 2 Photo &amp; Video | 4.968944 | . 3 Education | 3.664596 | . 4 Social Networking | 3.291925 | . def bar_n(series, chart_title, ylabel, n = 10, perc = False): &quot;&quot;&quot;Takes a series and outputs a bar graph of the first n items.&quot;&quot;&quot; series.index.name = &quot;name&quot; df = series.rename(&quot;number&quot;).reset_index() df[&quot;number&quot;] = [round(i, 2) for i in df[&quot;number&quot;]] df = df[:n] bar = alt.Chart(df).mark_bar().encode( x = alt.X(&quot;name&quot;, title = &quot;Name&quot;, sort = &quot;-y&quot;), y = alt.Y(&quot;number&quot;, title = ylabel), ) text = bar.mark_text( align = &#39;center&#39;, baseline = &#39;middle&#39;, dy = -5, # Nudge text upward ).encode( text = &#39;number:Q&#39; ) chart = (bar + text).properties( title = chart_title, width = 700, height = 400, ) return chart bar_n( apple_genres, &quot;Top 10 Most Common Prime Genres of iOS Apps&quot;, &quot;Percentage of Apps&quot;, perc = True, ) . The top 5 most common prime genres in the Apple App Store are Games, Entertainment, Photo &amp; Video, Education, and Social Networking. Games are at the top, occupying over 58% of all apps. This is a much higher percentage than any other single genre occupies. . Important: The general impression is that there are many more iOS apps that are entertainment-related apps compared to practical apps. . Google Play Store: Categories . Next, below is the frequency table for Google Play Store app categories. . sr_to_df(google_categories, &quot;percentage&quot;, 5) . name percentage . 0 FAMILY | 18.907942 | . 1 GAME | 9.724729 | . 2 TOOLS | 8.461191 | . 3 BUSINESS | 4.591606 | . 4 LIFESTYLE | 3.903430 | . bar_n( google_categories, &quot;Top 10 Most Common Categories of Android Apps&quot;, &quot;Percentage of Apps&quot;, perc = True, ) . The picture here seems to be different. The most common category is Family occupying almost 19% of all apps, followed by Game, Tools, Business, and Lifestyle. . Important: The table suggests that practical app categories are more common in Google Play than in the Apple App Store. . Google Play Store: Genres . Lastly, below is the frequency table for Google Play Store app genres. . sr_to_df(google_genres, &quot;percentage&quot;, 5) . name percentage . 0 Tools | 8.449910 | . 1 Entertainment | 6.069495 | . 2 Education | 5.347473 | . 3 Business | 4.591606 | . 4 Productivity | 3.892148 | . bar_n( google_genres, &quot;Top 10 Most Common Genres of Android Apps&quot;, &quot;Percentage of Apps&quot;, ) . There are 114 genres in this table, so it is not fully displayed. However, it would appear that the top 5 genres are Tools (8%), Entertainment, Education, Business, and Lifestyle. Like with the categories, practical apps are very common. . However, I noticed something special about this frequency table. Some genres are actually combinations of multiple genres, separated by semi-colons. If I can extract and count individual genres from these combined genres, then I can get a more accurate idea of app genres in the Google Play Store. . Note: This frequency table will show numbers instead of percentages. Since the genres overlap, the percentages would add up to greater than 100%. . freq = {} for value in android_clean[&quot;Genres&quot;]: genres = value.split(&quot;;&quot;) for genre in genres: freq.setdefault(genre, 0) freq[genre] += 1 google_genres_split = pd.Series(freq).sort_values(ascending = False) sr_to_df(google_genres_split, n_head = 5) . name number . 0 Tools | 750 | . 1 Education | 606 | . 2 Entertainment | 569 | . 3 Business | 407 | . 4 Lifestyle | 347 | . bar_n( google_genres_split, &quot;Top 10 Most Common Genres of Android Apps (Split Up)&quot;, &quot;Number of Apps&quot;, ) . It can be seen that the frequency table has slightly different placements now. However, the top genres are still Tools, Education, Entertainment, Business, and Lifestyle. Practical app genres are very common in the Google Play Store. They are more common here than in the Apple App Store. . Important: Based on the results, the Google Play Store has a selection of apps that is more balanced between entertainment and practicality. . . Going back to the the frequency table of Categories, since it seems that each Category represents a group of Genres. For example, one would expect apps in the Simulation, Arcade, Puzzle, Strategy, etc. genres to be under the Game category. It was shown earlier that this category is the 2nd most common in the Google Play Store. . The Categories column is more general and gives a more accurate picture of the common types of apps. Thus, from here on, I will be analyzing only the &quot;Category&quot; column and not the &quot;Genres&quot; column. . Note: I will now use &quot;app type&quot; to generally refer to the Apple App Store&#8217;s &quot;prime_genre&quot; values or the Google Play Store&#8217;s &quot;Category&quot; values. . App Types by Number of Users . We first looked at app types in terms of how common they are in the two app markets. Now, we shall see how many users there are for each app type. . Apple App Store: Rating Counts . In the Apple App Store dataset, there is no column that indicates the number of users. . print(list(ios_clean.columns)) . [&#39;id&#39;, &#39;track_name&#39;, &#39;size_bytes&#39;, &#39;currency&#39;, &#39;price&#39;, &#39;rating_count_tot&#39;, &#39;rating_count_ver&#39;, &#39;user_rating&#39;, &#39;user_rating_ver&#39;, &#39;ver&#39;, &#39;cont_rating&#39;, &#39;prime_genre&#39;, &#39;sup_devices.num&#39;, &#39;ipadSc_urls.num&#39;, &#39;lang.num&#39;, &#39;vpp_lic&#39;] . However, the &quot;rating_count_tot&quot; column exists. It indicates the total number of ratings given to each app. We can use it as a proxy for the number of users of each app. . The function below will return a Series showing the average number of users per app within each type. (Not the total number of users per type.) . def users_by_type(df, type_col, users_col, moct = &quot;mean&quot;): &quot;&quot;&quot;Return a Series that maps each app type to the average number of users per app for that type. Specify &#39;mean&#39; or &#39;median&#39; for the measure of central tendency.&quot;&quot;&quot; dct = {} for index, row in df.iterrows(): app_type = row[type_col] users = row[users_col] dct.setdefault(app_type, []).append(users) dct2 = {} for app_type in dct: counts = dct[app_type] if moct == &quot;mean&quot;: dct2[app_type] = np.mean(counts) elif moct == &quot;median&quot;: dct2[app_type] = np.median(counts) result = pd.Series(dct2).sort_values(ascending = False) return result ios_users = users_by_type(ios_clean, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;) sr_to_df(ios_users, n_head = 5) . name number . 0 Navigation | 86090.333333 | . 1 Reference | 74942.111111 | . 2 Social Networking | 71548.349057 | . 3 Music | 57326.530303 | . 4 Weather | 52279.892857 | . bar_n( ios_users, &quot;Top 10 Most Popular iOS App Types&quot;, &quot;Mean Number of Users per App&quot;, ) . The top 5 iOS app types with the highest mean average number of users per app are Navigation, Reference, Social Networking, Music, and Weather. . However, these mean averages may be skewed by a few particularly popular apps. For example, let us look at the number of users of the top 5 Navigation apps. . ios_nav = ios_clean[[ &quot;track_name&quot;, &quot;rating_count_tot&quot;, ]].loc[ ios_clean[&quot;prime_genre&quot;] == &quot;Navigation&quot; ].sort_values( by = &quot;rating_count_tot&quot;, ascending = False, ).set_index( &quot;track_name&quot;, ) # `ios_nav` is still a DataFrame at this point. # It becomes a Series below. ios_nav = ios_nav[&quot;rating_count_tot&quot;] sr_to_df(ios_nav, n_head = 5) . track_name number . 0 Waze - GPS Navigation, Maps &amp; Real-time Traffic | 345046 | . 1 Google Maps - Navigation &amp; Transit | 154911 | . 2 Geocaching® | 12811 | . 3 CoPilot GPS – Car Navigation &amp; Offline Maps | 3582 | . 4 ImmobilienScout24: Real Estate Search in Germany | 187 | . bar_n( ios_nav, &quot;iOS Navigation Apps by Popularity&quot;, &quot;Number of Users&quot;, ) . Clearly, the distribution is skewed because Waze has such a high number of users. Therefore, a better measure of central tendency to use would be the median, not the mean. . Let us repeat the analysis using the median this time: . ios_users = users_by_type( ios_clean, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;, moct = &quot;median&quot;, ) sr_to_df(ios_users, n_head = 5) . name number . 0 Productivity | 8737.5 | . 1 Navigation | 8196.5 | . 2 Reference | 6614.0 | . 3 Shopping | 5936.0 | . 4 Social Networking | 4199.0 | . bar_n( ios_users, &quot;Top 10 Most Popular iOS App Types&quot;, &quot;Median Number of Users per App&quot;, ) . The top 5 most popular iOS apps by median number of users per app are: . Productivity | Navigation | Reference | Shopping | Social Networking | . These placements are quite different from the top 5 most common iOS apps (Games, Entertainment, Photo &amp; Video, Education, and Social Networking). . . . Important: We can say the following about the Apple App Store. . Apps for entertainment and fun, notably Games, are the most common apps. | Apps for practical purposes, notably Productivity, are the most popular apps. | . Google Play Store: Installs . Let us see which columns in the Google Play Store dataset can tell us about the number of users per app. . android_clean.head() . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . 5 Paper flowers instructions | ART_AND_DESIGN | 4.4 | 167 | 5.6M | 50,000+ | Free | 0 | Everyone | Art &amp; Design | March 26, 2017 | 1.0 | 2.3 and up | . The &quot;Installs&quot; column seems like the best indicator of the number of users. . android_clean[[&quot;App&quot;, &quot;Installs&quot;]] . App Installs . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | 10,000+ | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | 5,000,000+ | . 3 Sketch - Draw &amp; Paint | 50,000,000+ | . 4 Pixel Draw - Number Art Coloring Book | 100,000+ | . 5 Paper flowers instructions | 50,000+ | . ... ... | ... | . 10836 Sya9a Maroc - FR | 5,000+ | . 10837 Fr. Mike Schmitz Audio Teachings | 100+ | . 10838 Parkinson Exercices FR | 1,000+ | . 10839 The SCP Foundation DB fr nn5n | 1,000+ | . 10840 iHoroscope - 2018 Daily Horoscope &amp; Astrology | 10,000,000+ | . 8864 rows × 2 columns . The column contains strings which indicate the general range of how many users installed the apps. Since we cannot find the exact number of installs, we will simply remove the &quot;+&quot; signs and convert the numbers into integers. . android_clean[&quot;Installs&quot;] = [int(re.sub(&quot;[,+]&quot;, &quot;&quot;, text)) for text in android_clean[&quot;Installs&quot;]] android_clean[[&quot;Installs&quot;]] . Installs . 0 10000 | . 2 5000000 | . 3 50000000 | . 4 100000 | . 5 50000 | . ... ... | . 10836 5000 | . 10837 100 | . 10838 1000 | . 10839 1000 | . 10840 10000000 | . 8864 rows × 1 columns . Let us now see which app categories are most popular. We will use the median average here, as we did for iOS apps. . android_users = users_by_type( android_clean, &quot;Category&quot;, &quot;Installs&quot;, moct = &quot;median&quot;, ) sr_to_df(android_users, n_head = 10) . name number . 0 ENTERTAINMENT | 1000000.0 | . 1 EDUCATION | 1000000.0 | . 2 GAME | 1000000.0 | . 3 PHOTOGRAPHY | 1000000.0 | . 4 SHOPPING | 1000000.0 | . 5 WEATHER | 1000000.0 | . 6 VIDEO_PLAYERS | 1000000.0 | . 7 COMMUNICATION | 500000.0 | . 8 FOOD_AND_DRINK | 500000.0 | . 9 HEALTH_AND_FITNESS | 500000.0 | . bar_n( android_users, &quot;Top 10 Most Popular Android App Types&quot;, &quot;Median Number of Users per App&quot;, n = 10, ) . Since the top 5 spots all had the same median number of users per app (1000000), the graph was expanded to include the top 10 spots. . It appears that the types of Android apps with the highest median number of users per app are: . GAME | VIDEO_PLAYERS | WEATHER | EDUCATION | ENTERTAINMENT | PHOTOGRAPHY | SHOPPING | . . . Important: We can say the following about the Google Play Store. . Both fun apps and practical apps are very common. | The most popular apps are also a mix of fun apps and practical apps. | . App Profile Ideas . Based on the results, we can now determine a profitable app profile for the hypothetical app company. . Here is a summary of the findings on the 2 app stores. . The Google Play Store has a balanced mix of fun and practical apps, so we can pick either kind. | On the other hand, the Apple App Store appears to be oversaturated with game apps, and practical apps are more popular. | . Therefore, in order to get the most users, the app company can set themselves apart in the Apple App Store by developing a useful practical app. . The most popular types of practical apps for the Apple App Store would be: . Productivity | Navigation | Reference | Shopping | . For the Google Play Store, these would be: . Weather | Education | Photography | Shopping | . Shopping appears in both lists, so it may be the most profitable type of app. However, the app company would have to make a unique app that has an edge over existing popular shopping apps. The same would apply for making a navigation app. . Considering that Reference and Education apps are popular, perhaps these two types could be combined into one app. First, let us find out the titles of the most popular apps in these genres. . reference_popularity = ios_clean[[ &quot;track_name&quot;, &quot;rating_count_tot&quot; ]].loc[ ios_clean[&quot;prime_genre&quot;] == &quot;Reference&quot; ].dropna( ).sort_values( &quot;rating_count_tot&quot;, ascending = False, ).set_index( &quot;track_name&quot;, )[&quot;rating_count_tot&quot;] sr_to_df(reference_popularity, n_head = 10) . track_name number . 0 Bible | 985920 | . 1 Dictionary.com Dictionary &amp; Thesaurus | 200047 | . 2 Dictionary.com Dictionary &amp; Thesaurus for iPad | 54175 | . 3 Google Translate | 26786 | . 4 Muslim Pro: Ramadan 2017 Prayer Times, Azan, Q... | 18418 | . 5 New Furniture Mods - Pocket Wiki &amp; Game Tools ... | 17588 | . 6 Merriam-Webster Dictionary | 16849 | . 7 Night Sky | 12122 | . 8 City Maps for Minecraft PE - The Best Maps for... | 8535 | . 9 LUCKY BLOCK MOD ™ for Minecraft PC Edition - T... | 4693 | . bar_n( reference_popularity, &quot;Top 10 Most Popular iOS Reference Apps&quot;, &quot;Number of Users&quot;, ) . education_popularity = android_clean[[ &quot;App&quot;, &quot;Installs&quot; ]].loc[ android_clean[&quot;Category&quot;] == &quot;EDUCATION&quot; ].dropna( ).sort_values( &quot;Installs&quot;, ascending = False, ).set_index( &quot;App&quot;, )[&quot;Installs&quot;] sr_to_df(education_popularity, n_head = 5) . App number . 0 Quizlet: Learn Languages &amp; Vocab with Flashcards | 10000000 | . 1 Learn languages, grammar &amp; vocabulary with Mem... | 10000000 | . 2 Learn English with Wlingua | 10000000 | . 3 Remind: School Communication | 10000000 | . 4 Math Tricks | 10000000 | . bar_n( education_popularity, &quot;Top 10 Most Popular Android Education Apps&quot;, &quot;Number of Users&quot;, ) . The most popular Reference apps are the Bible and some dictionary and translation apps. The most popular Education apps teach languages (especially English), or Math. . Therefore, the following are some ideas of a profitable app: . An app containing the Bible, another religious text, or another well-known text. The app can additionally include reflections, analyses, or quizzes about the text. | An app that contains an English dictionary, a translator, and some quick guides on English vocabulary and grammar. An app like the above, but for a different language that is spoken by many people. | . | An app that teaches English and Math lessons. Perhaps it could be marketed as a practice app for an entrance exam. | . Conclusion . In this project, we analyzed app data from a Google Play Store dataset and an Apple App Store dataset. Apps were limited to free apps targeted towards English speakers, because the hypothetical app company makes these kinds of apps. The most common and popular app genres were determined. . In the end, several ideas of profitable apps were listed. The app company may now review the analysis and consider the suggestions. This may help them make an informed, data-driven decision regarding the next app that they will develop. .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/altair/2021/05/08/Profitable-App-Profiles.html",
            "relUrl": "/python/pandas/numpy/altair/2021/05/08/Profitable-App-Profiles.html",
            "date": " • May 8, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Miguel Antonio H. Germar, but you can call me Migs. I am currently a high school student in the Philippines. I am interested in pursuing a college degree and career in Data Science. Other than this, my interests also include anime and archery. . . Contact . Email: migs.germar@gmail.com . Facebook: https://www.facebook.com/miguelantonio.germar/ . My Learning Journey . I set up this website using fastpages as a blog and portfolio for Data Science projects. Below, I outline my learning journey in Data Science. Eventually, I will make blog posts about most of the projects that I have done along the way. . How I started in Data Science . I first found out about data science in 2020, around the time that the COVID-19 pandemic started. I decided to take online courses about data science in the summer, in order to see if it was interesting. First, I took the Python Core, Data Science, and Machine learning courses on Sololearn. . Here, I learned basic skills in the following: . Python | Spyder IDE | Jupyter Noteboook | Numpy | Pandas | Matplotlib | Scikit-learn | SQL basics | . Data Science in Practical Research . When classes started again, I prioritized schoolwork. However, I was able to apply my data science skills in my group’s Practical Research project. Our research paper was entitled “The Effect of COVID-19’s Consequences on Philippine Frontliners on their Mental Health: A Descriptive, Correlational Study.” We collected survey responses from 196 frontliners. I wrote the entire analysis in Python, from data cleaning to transformation to modeling. . I had to learn new things in order to do this, including: . Statsmodels | Dummy-coding categorical variables | Multiple linear regression | Testing the assumptions of OLS regression | Interpreting model results | . The Present: Summer of 2021 . Currently, I am working on the Data Scientist in Python course on Dataquest. The course includes many guided projects in Jupyter Notebook, and I will post each project on my blog as I go along. . .",
          "url": "https://miguelahg.github.io/mahg-data-science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://miguelahg.github.io/mahg-data-science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}