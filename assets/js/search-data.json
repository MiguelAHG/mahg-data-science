{
  
    
        "post0": {
            "title": "Using a Neural Network to Classify Handwritten Digits",
            "content": "Unsplash | Clint Adair Introduction . Neural networks are a class of machine learning models that take inspiration from biological neurons. One network is made up of multiple neurons, which can be visualized as nodes connected to each other with lines. In order to make a prediction, data is fed into input neurons, then various calculations are performed as it passes through the other neurons, until a final output value is reached. . One main advantage of using a neural network is that it uses nonlinear activation functions. These are functions are &quot;nonlinear&quot; in the sense that the relationship between x (the input) and y (the output) does not form a straight line. For example, the logistic or sigmoid ($ sigma$) function, as well as the $ tanh$ function, have an S-shaped curve which makes them suited for classification tasks. The ReLU (Rectified Linear Unit) function, $ReLU(x) = max(0, x)$, returns 0 if the input is less than or equal to 0, thus making the shape of the function look like a bent line. . Because these functions are nonlinear, the model is able to find nonlinear relationships between the features (the variables used to make a prediction) and the target (the variable that we want to predict). Thus, it can be more accurate than a linear regression in many scenarios. . Furthermore, a neural network can be built with &quot;hidden layers&quot;: layers of neurons between the input neurons and the output neurons. These add additional depth to the model. Thus, the neural network can potentially make more accurate predictions than other models that also use nonlinear functions. . In this project, I will compare the performance of a neural network and another type of model (K Nearest Neighbors) in predicting the labels of images of handwritten digits (as in, digits from 0 to 9). Furthermore, I will demonstrate how scikit-learn&#39;s Grid Search feature can be used for hyperparameter optimization for both models. . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically the Guided Project: Building a Handwritten Digits Classifier. The general project flow and research questions were guided by Dataquest. Other than what was instructed, I also added my own steps. You can visit the official solution to compare it to my project. . Below are the packages used in this project. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import load_digits from sklearn.neighbors import KNeighborsClassifier from sklearn.neural_network import MLPClassifier from sklearn.model_selection import StratifiedKFold, cross_validate from sklearn.model_selection import GridSearchCV . The Dataset . The dataset for this project is the Optical Recognition of Handwritten Digits dataset. This dataset comes with the scikit-learn package in Python, though it can also be downloaded from the UCI Machine Learning Repository, where it was donated by Alpaydin and Kaynak in 1998. It contains 1797 different images of handwritten digits. . Let us inspect the first 5 rows of the features. . X, y = load_digits( n_class = 10, return_X_y = True, as_frame = True, ) X.head() . . pixel_0_0 pixel_0_1 pixel_0_2 pixel_0_3 pixel_0_4 pixel_0_5 pixel_0_6 pixel_0_7 pixel_1_0 pixel_1_1 ... pixel_6_6 pixel_6_7 pixel_7_0 pixel_7_1 pixel_7_2 pixel_7_3 pixel_7_4 pixel_7_5 pixel_7_6 pixel_7_7 . 0 0.0 | 0.0 | 5.0 | 13.0 | 9.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 6.0 | 13.0 | 10.0 | 0.0 | 0.0 | 0.0 | . 1 0.0 | 0.0 | 0.0 | 12.0 | 13.0 | 5.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 11.0 | 16.0 | 10.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 0.0 | 4.0 | 15.0 | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 5.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 11.0 | 16.0 | 9.0 | 0.0 | . 3 0.0 | 0.0 | 7.0 | 15.0 | 13.0 | 1.0 | 0.0 | 0.0 | 0.0 | 8.0 | ... | 9.0 | 0.0 | 0.0 | 0.0 | 7.0 | 13.0 | 13.0 | 9.0 | 0.0 | 0.0 | . 4 0.0 | 0.0 | 0.0 | 1.0 | 11.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 16.0 | 4.0 | 0.0 | 0.0 | . 5 rows × 64 columns . Each row represents a different black-and-white image of a handwritten digit. Each column represents the value of a pixel. For example, if you imagine the pixels arranged in a grid, pixel_0_0 represents the value of the pixel in the top left of the image (first row, first column). Then, pixel_0_1 represents the pixel in the first row and second column, and so on. . Below, I have reconstructed the first 10 images in the dataset using the pixel data. . sample_indices = range(10) fig, axes = plt.subplots(2, 5, figsize = (10, 5)) # Flatten the 2darray into a 1darray so that I can index it with just one axis. axes = axes.flatten() for i in range(10): # Convert a row of data into a 1darray, then reshape it into a 2darray. sample_image = X.iloc[i].to_numpy().reshape(8, 8) sample_label = y.iloc[i] # current subplot sp = axes[i] sp.imshow(sample_image, cmap = &quot;gray_r&quot;) sp.set_title(f&quot;Label: {sample_label}&quot;) # Remove all ticks except one, at y = 0, and make its label blank. # This is done because ticks and tick labels are unnecessary. # I had to leave one tick in place to avoid an error where the background becomes transparent. sp.set_xticks([]) sp.set_yticks([0.0]) sp.set_yticklabels([&quot;&quot;]) # Make the length of the remaining tick 0 so it is invisible. sp.tick_params(axis = &quot;both&quot;, length = 0.0) plt.show() . . Each image has a low resolution because it only contains 64 pixels in an 8-by-8 grid. Above each image, I displayed its label (in other words, the actual value of the handwritten digit). . Later on in this project, the goal of the machine learning models will be to look at the images and predict their labels. . Below, I have displayed a frequency table of the labels. . print(&quot;Frequency Table of Labels&quot;) y.value_counts().sort_index() . . Frequency Table of Labels . 0 178 1 182 2 177 3 183 4 181 5 182 6 181 7 179 8 174 9 180 Name: target, dtype: int64 . All digits from 0 to 9 are present in the dataset. Furthermore, there are around 180 images for each label. The dataset seems balanced; that is, the labels are equally distributed. . Now that I understand the contents of the dataset, I can move on to the machine learning models. . K Nearest Neighbors . Before I use neural networks, I will use the K Nearest Neighbors (KNN) algorithm for comparison. This algorithm determines the similarity of a test observation to the training observations, then uses the most similar training observations (&quot;nearest neighbors&quot;) to make a prediction. For more details on this algorithm, you can visit my past project, Predicting Car Prices using the K Nearest Neighbors Algorithm. . Before evaluating the model, I will first perform hyperparameter optimization. This means that I will try to find the optimal &quot;settings&quot; for the model, which will make it perform better. These settings are unrelated to the data being used to train the model. . In the case of KNN, I am concerned with the n_neighbors hyperparameter, which determines how many of the &quot;nearest neighbors&quot; are used to make a prediction. If this value is too low, the model may not have enough information to make a good prediction. If the value is too high, the model may make a prediction that is very far from the real value. Thus, I have to test various values. . Below is a list of the values that I will test. . # 1 to 10, then 10 to 200 with skips of 10 k_list = list(range(1, 10, 1)) + list(range(10, 201, 10)) knn_param_grid = [{ &quot;n_neighbors&quot;: k_list }] print(k_list) . . [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200] . The code below uses scikit-learn&#39;s GridSearchCV feature. It takes the grid of the parameter values that I want to try. Then, it tests the model using the different values, and records the model performance. It then identifies the best model based on performance. . In this case, I will use accuracy as my performance metric. This refers to the number of correct predictions divided by the total number of predictions. . # grid search for knn skf = StratifiedKFold( n_splits = 5, shuffle = True, random_state = 0, ) gs_knn = GridSearchCV( KNeighborsClassifier(), knn_param_grid, scoring = &quot;accuracy&quot;, cv = skf, ) gs_knn.fit(X, y) gs_knn.best_params_ . . {&#39;n_neighbors&#39;: 3} . The result above tells me that the Grid Search determined that the model with n_neighbors = 3 had the best accuracy compared to models with other values. . Below is a graph that shows how the accuracy changed as the n_neighbors parameter changed. . gs_knn_results = pd.DataFrame(gs_knn.cv_results_) sns.lineplot( data = gs_knn_results, x = &quot;param_n_neighbors&quot;, y = &quot;mean_test_score&quot;, ) plt.title(&quot;KNN: Accuracy against n_neighbors Parameter&quot;) plt.xlabel(&quot;n_neighbors&quot;) plt.ylabel(&quot;Accuracy&quot;) plt.show() . . Clearly, the general trend is that as the number of neighbors increased, the accuracy decreased. This makes sense because if too many neighbors are used, some of the neighbors may be very different from the observation for which we are trying to make a prediction. . Next, I will use scikit-learn&#39;s cross_validate feature to obtain multiple performance metrics about the best KNN model that Grid Search found. . def stratified_5fcv(model, X, y, skf): metrics = [ &quot;accuracy&quot;, &quot;recall_weighted&quot;, &quot;precision_weighted&quot;, &quot;f1_weighted&quot;, ] scores_dict = cross_validate( estimator = model, X = X, y = y, scoring = metrics, cv = skf, ) scores_df = pd.DataFrame(scores_dict) mean_results = ( scores_df .loc[:, [col for col in scores_df if col.startswith(&quot;test&quot;)]] .mean(axis = 0) ) return mean_results # This will use the best estimator found. scores_knn = stratified_5fcv(gs_knn.best_estimator_, X, y, skf) print(&quot;K Nearest Neighbors&quot;) scores_knn . . K Nearest Neighbors . test_accuracy 0.987758 test_recall_weighted 0.987758 test_precision_weighted 0.988019 test_f1_weighted 0.987729 dtype: float64 . Above, I have displayed the mean scores of the KNN model from cross-validation. All of the scores were over 98%, which means that the model performed very well. . I used multiple metrics for the sake of displaying the ability of cross_validate to measure multiple metrics. However, I will not go over these individually. For more information on recall, precision, and F1 score, visit my past project, Naive Bayes Algorithm for Detecting Spam Messages, and scroll down to &quot;Recall, Precision, F1.&quot; . Note that I used the &quot;weighted&quot; versions of some of the metrics. This is helpful when the labels are imbalanced. In the case of this project, there is slight imbalance, as each digit has around 175 to 185 images in the dataset. . For a general guide on scikit-learn metrics, visit the documentation. . Moving on, let&#39;s compare the KNN model to a neural network. . Neural Network . Like with KNN, I will first perform hyperparameter optimization. I will focus on three hyperparameters of interest: . hidden_layer_sizes: This represents the number of hidden layers in the model, as well as their size (number of nodes). | activation: The activation function. I have chosen to only test the &quot;logistic&quot; or sigmoid function in order to save time during Grid Search. | max_iter: The maximum number of iterations during model fitting. I have set this to a somewhat high number (1000). | . The only hyperparameter for which I will test multiple values is hidden_layer_sizes. Below is a list of the values that I will try. . single_layer_list = [(8,), (16,), (32,), (64,), (128,), (256,)] multi_layer_list = [ (64, 64), (10, 10, 10), (64, 64, 64), (128, 128, 128), ] all_layer_list = single_layer_list + multi_layer_list all_layer_list . . [(8,), (16,), (32,), (64,), (128,), (256,), (64, 64), (10, 10, 10), (64, 64, 64), (128, 128, 128)] . Each set of parentheses is a unique value that I will try. The number of items between the parentheses represents the number of hidden layers. For example, with (8,), there is only one item, so there is one hidden layer. Also, this hidden layer&#39;s size is 8, meaning it has 8 neurons. . The code cell below performs the Grid Search to find the optimal setup of hidden layers. . # Grid search for MLP # Hide warnings about optimization not converging yet. import warnings warnings.filterwarnings(&#39;ignore&#39;) mlp_param_grid = { &quot;hidden_layer_sizes&quot;: all_layer_list, &quot;activation&quot;: [&quot;logistic&quot;], &quot;max_iter&quot;: [1000], } gs_mlp = GridSearchCV( MLPClassifier(), mlp_param_grid, scoring = &quot;accuracy&quot;, cv = skf, ) gs_mlp.fit(X, y) gs_mlp.best_params_ . . {&#39;activation&#39;: &#39;logistic&#39;, &#39;hidden_layer_sizes&#39;: (256,), &#39;max_iter&#39;: 1000} . The result above shows that the model with hidden_layer_sizes = (256,) performed the best. This model had only one hidden layer, which contained 256 neurons. . I performed cross-validation using the best model. Below are the mean metric scores. . scores_mlp = stratified_5fcv(gs_mlp.best_estimator_, X, y, skf) print(&quot;Neural network&quot;) scores_mlp . . Neural network . test_accuracy 0.983863 test_recall_weighted 0.983863 test_precision_weighted 0.984525 test_f1_weighted 0.983880 dtype: float64 . Like the KNN model, the neural network also performed very well. All of its scores are over 98%. . Let us compare the results of the KNN model and the neural network. . scores_mlp &gt; scores_knn . . test_accuracy False test_recall_weighted False test_precision_weighted False test_f1_weighted False dtype: bool . The table above shows True where the neural network outperformed the KNN, and False where the KNN performed better. . It seems that the KNN performed slightly better than the neural network on all metrics: accuracy, recall, precision, and F1 score. . However, this does not mean that KNN is always inherently better than a neural network. There are certain tradeoffs to consider. . In the case of KNN, the advantage is that it takes no time to train, since it does not try to fit coefficients. Thus, it is much faster to use when performing Grid Search or cross-validation. However, the disadvantage is that it can take a lot of time to make predictions, since it uses the entire training set to make predictions. Furthermore, if we obtain more data later on and add it to the model, the accuracy might increase, but the amount of time needed to make a prediction will definitely increase. . On the other hand, a neural network&#39;s advantage is that it can have multiple neurons and hidden layers, so it can form a deep understanding of variable relationships. What&#39;s unfortunate is that it takes a long time to train because it tries to find the optimal coefficent for each connection between two neurons. However, it takes a shorter time than KNN to make predictions. Furthermore, if we obtain more data later on and add it to the model, the amount of time needed to make a prediction will still remain the same (because the network will still have the same size). . Summary . In summary, I briefly discussed how a neural network works, performed hyperparameter optimization on a KNN model and a neural network, compared the two models&#39; performance, and discussed tradeoffs that must be considered when choosing a model to use. . Thanks for reading! . Bibliography . Data Source . Alpaydin, E., &amp; Kaynak, C. (1998). UCI Machine Learning Repository: Optical Recognition of Handwritten Digits Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits . The data was obtained via the following scikit-learn feature: . scikit-learn developers. (2021). Sklearn.datasets.load_digits. Scikit-Learn. https://scikit-learn/stable/modules/generated/sklearn.datasets.load_digits.html . Information Sources . Dataquest. (n.d.). Building A Handwritten Digits Classifier—Introduction. Dataquest. Retrieved January 21, 2022, from https://app.dataquest.io/c/50/m/244/guided-project%3A-building-a-handwritten-digits-classifier/1 . scikit-learn developers. (2021a). 3.3. Metrics and scoring: Quantifying the quality of predictions. Scikit-Learn. https://scikit-learn/stable/modules/model_evaluation.html . scikit-learn developers. (2021b). Sklearn.model_selection.GridSearchCV. Scikit-Learn. https://scikit-learn/stable/modules/generated/sklearn.model_selection.GridSearchCV.html . Image Source . Adair, C. (2016, February 26). Photo by Clint Adair on Unsplash. Unsplash. https://unsplash.com/photos/BW0vK-FA3eg .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/scikit-learn/2022/01/21/Neural-Network-Classify-Handwritten-Digits.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/scikit-learn/2022/01/21/Neural-Network-Classify-Handwritten-Digits.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Comparison of Regression Models for Predicting Bike Rentals",
            "content": "Unsplash | Stéphane Mingot Introduction . Regression models refer to a class of machine learning models that predict continuous variables. That is, they predict quantities such as the count of an object, its price, its size, its mass, etc. This is what makes regression models different from classification models, which predict discrete categories such as positive and negative test results, survival or death, place of origin, etc. . In this project, I will compare three different regression models: linear regression, decision tree, and random forest. I will use these to predict the number of people who will rent a bike from the bike sharing stations in Washington, D.C. at a given hour of a given day. I will then compare the performance of these models and discuss the factors that may have affected their performance. . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically the Guided Project: Predicting Bike Rentals. The general project flow and research questions were guided by Dataquest. Other than what was instructed, I also added my own steps. You can visit the official solution to compare it to my project. . Below are the packages imported for this project. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sms from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Custom modules from custom_modules import linreg_tools as lrt . Note that some of the imported modules are custom ones that I wrote. To view these modules, visit this directory in my website&#39;s repository. . The Dataset . The &quot;Bike Sharing Dataset&quot; was donated to the UCI Machine Learning Repository by Hadi Fanaee-T in 2013. It contains data on bike rentals in Washington, D.C. from 2011-2012. The first 5 rows of the data are shown below. . data = pd.read_csv(&quot;./private/Bike-Rentals-Files/hour.csv&quot;) data.dteday = pd.to_datetime(data.dteday) data.head() . . instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt . 0 1 | 2011-01-01 | 1 | 0 | 1 | 0 | 0 | 6 | 0 | 1 | 0.24 | 0.2879 | 0.81 | 0.0 | 3 | 13 | 16 | . 1 2 | 2011-01-01 | 1 | 0 | 1 | 1 | 0 | 6 | 0 | 1 | 0.22 | 0.2727 | 0.80 | 0.0 | 8 | 32 | 40 | . 2 3 | 2011-01-01 | 1 | 0 | 1 | 2 | 0 | 6 | 0 | 1 | 0.22 | 0.2727 | 0.80 | 0.0 | 5 | 27 | 32 | . 3 4 | 2011-01-01 | 1 | 0 | 1 | 3 | 0 | 6 | 0 | 1 | 0.24 | 0.2879 | 0.75 | 0.0 | 3 | 10 | 13 | . 4 5 | 2011-01-01 | 1 | 0 | 1 | 4 | 0 | 6 | 0 | 1 | 0.24 | 0.2879 | 0.75 | 0.0 | 0 | 1 | 1 | . Each row contains data for one hour of a certain day. The dteday column shows the date, and the hr column shows the hour from 0 (midnight) to 23 (11:00 PM). Also, the instant column provides an identification number for each row. . The cnt variable shows the total number of people who rented bikes during a given hour. It is the sum of the casual (casual renters) and registered (registered renters) variables. The other variables describe conditions like the weather. . For more information on the variables, one can visit the data documentation and scroll down to &quot;Attribute Information&quot;. Note that this link is also the link from which I downloaded the dataset. . Let us look at a summary of the variables. . data.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17379 entries, 0 to 17378 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 instant 17379 non-null int64 1 dteday 17379 non-null datetime64[ns] 2 season 17379 non-null int64 3 yr 17379 non-null int64 4 mnth 17379 non-null int64 5 hr 17379 non-null int64 6 holiday 17379 non-null int64 7 weekday 17379 non-null int64 8 workingday 17379 non-null int64 9 weathersit 17379 non-null int64 10 temp 17379 non-null float64 11 atemp 17379 non-null float64 12 hum 17379 non-null float64 13 windspeed 17379 non-null float64 14 casual 17379 non-null int64 15 registered 17379 non-null int64 16 cnt 17379 non-null int64 dtypes: datetime64[ns](1), float64(4), int64(12) memory usage: 2.3 MB . The summary above shows that there are 17,379 rows and 17 columns. None of the rows have null values. Furthermore, all of the columns have numeric data types, with the exception of dteday, which contains datetime values. . However, according to the documentation, the following variables are nominal or ordinal despite being numeric: . instant provides an ID number to each entry. | season ranges from winter (1) to fall (4). | holiday contains values of 1 if the day is a holiday, else 0. | workingday contains values of 1 if the day is neither a holiday nor weekday, else 0. | weathersit ranges from 1 (mild weather) to 4 (certain extreme weather conditions). | . instant should not be used as a predictor variable, but the others may be used. I must keep in mind, however, that linear regression treats all variables as continuous. It assumes that each one-unit change in a predictor variable increases or decreases the target variable by a set amount. . Another concern is the hr variable, which indicates the hour of the day from 0 (midnight) to 23 (11:00 PM). This variable is rather fine-grained, and it would be interesting to experiment with creating a new variable that groups certain hours together. That is, 1 would refer to the morning, 2 to the afternoon, 3 to the evening, and 4 to night time. . This variable has been created below, and it has been named time_label. . # Make time_label column def hour_to_label(h): &quot;&quot;&quot;Convert an integer representing an hour of the day to an integer representing a section of the day-night cycle. 1: morning, 2: afternoon, 3: evening, 4: night.&quot;&quot;&quot; if h &gt;= 6 and h &lt; 12: return 1 elif h &gt;= 12 and h &lt; 18: return 2 elif h &gt;= 18 and h &lt; 24: return 3 elif h &gt;= 0 and h &lt; 6: return 4 data[&quot;time_label&quot;] = data.hr.apply(hour_to_label) data[[&quot;hr&quot;, &quot;time_label&quot;]].head() . . hr time_label . 0 0 | 4 | . 1 1 | 4 | . 2 2 | 4 | . 3 3 | 4 | . 4 4 | 4 | . The table above shows the first 5 rows and the hr and time_label columns. Since the hours from 00:00 to 04:00 occur in the middle of the night, the time_label values are 4. . Now, data inspection and cleaning are done, so I can move on to the models. . Linear Regression . The first of the three models is linear regression. It attempts to determine how predictors affect the target variable by assigning each predictor a coefficient. The coefficients are then used in an equation to come up with a predicted value for a new observation. . Note that I may not go into full detail about the steps that I am performing. To know more about the details, view my other post on linear regression for predicting house sale prices. . Feature Selection . I must select features that have a useful linear relationship with the number of bike rentals (cnt). The code cell below lists the initial list of features that I will consider. I will not use casual and registered since these are systematically related to cnt. . considered_features = pd.Series([ &quot;season&quot;, &quot;yr&quot;, &quot;mnth&quot;, &quot;hr&quot;, &quot;time_label&quot;, &quot;holiday&quot;, &quot;weekday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;, ]) . First, I can find the Pearson&#39;s correlation coefficient between each variable and the target. . target_col = &quot;cnt&quot; # Correlation coefficient of each variable with cnt cnt_corr = ( data .loc[:, considered_features.tolist() + [target_col]] .corr() .loc[:, &quot;cnt&quot;] .drop(index = &quot;cnt&quot;) # Sort by distance from 0 .sort_values(key = np.abs, ascending = False) ) print(&quot;Correlation Coefficient of each Variable with `cnt`&quot;) cnt_corr . . Correlation Coefficient of each Variable with `cnt` . temp 0.404772 atemp 0.400929 hr 0.394071 time_label -0.378318 hum -0.322911 yr 0.250495 season 0.178056 weathersit -0.142426 mnth 0.120638 windspeed 0.093234 holiday -0.030927 workingday 0.030284 weekday 0.026900 Name: cnt, dtype: float64 . The coefficients have been ordered by distance from 0, descending. Coefficients farther from 0 indicate stronger correlation. . It appears that the temperature (temp) and feeling temperature (atemp) have the strongest correlation with bike rentals, so it is likely that these will be significant predictors. . As for the other variables, I will not drop any of them based solely on their correlation coefficient. This decision may seem strange since some variables have very weak correlations, even less than 0.10. However, one must note that these coefficients are the results of univariate tests. It is still possible for a variable to become significant when the effects of other variables are taken into consideration in multiple linear regression. Furthermore, the sample size is large (n = 17379), so there is little risk of overfitting from having too many predictors. . However, I do have to drop predictors if they have multicollinearity issues. Thus, I have generated a correlation heatmap to inspect correlations among predictors. . lrt.correlation_heatmap(data[considered_features.tolist() + [target_col]].corr()) . . In the heatmap above, blue represents positive correlations, red represents negative correlations, and darker shades represent stronger correlations. . It appears that temp and atemp are highly correlated (0.99). Thus, I will keep atemp since it is the &quot;feeling&quot; temperature (temperature perceived by humans). . Furthermore, mnth and season have a coefficient of 0.83. Thus, I will drop season because it is less specific compared to mnth. . Lastly, weathersit and hum (humidity) have a coefficient of 0.42. Thus, I will drop weathersit because hum has a higher correlation with the target, cnt. . By dropping the mentioned variables, I will avoid multicollinearity in the model. . considered_features = considered_features.loc[~considered_features.isin([&quot;temp&quot;, &quot;season&quot;, &quot;weathersit&quot;])] considered_features . . 1 yr 2 mnth 3 hr 4 time_label 5 holiday 6 weekday 7 workingday 10 atemp 11 hum 12 windspeed dtype: object . Above is the final list of variables that I will use in linear regression. . Statistical Inference . Now, I will perform statistical inference with the statsmodels package to check for significance values and other assumptions of linear regression. . X = sms.add_constant(data[considered_features]) y = data[target_col] vif_df = lrt.get_vif(X) model = sms.OLS(y, X) results = model.fit() summary = results.summary() tables = lrt.extract_summary(summary, vif_df) tables[0] . . C: Users migs anaconda3 envs new_streamlit_env2 lib site-packages statsmodels tsa tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument &#39;objs&#39; will be keyword-only x = pd.concat(x[::order], 1) . 0 1 2 3 . 0 Dep. Variable: | cnt | R-squared: | 0.463 | . 1 Model: | OLS | Adj. R-squared: | 0.463 | . 2 Method: | Least Squares | F-statistic: | 1497.000 | . 3 Date: | Sun, 16 Jan 2022 | Prob (F-statistic): | 0.000 | . 4 Time: | 13:13:59 | Log-Likelihood: | -109640.000 | . 5 No. Observations: | 17379 | AIC: | 219300.000 | . 6 Df Residuals: | 17368 | BIC: | 219400.000 | . 7 Df Model: | 10 | NaN | NaN | . 8 Covariance Type: | nonrobust | NaN | NaN | . Above is the first table of results provided by the statsmodels package. Unfortunately, though the model&#39;s F-statistic is significant (p &lt; 0.05), the R-squared value is 0.463. Therefore, the model only explains 46.3% of the variance in the data. This ideally should be close to 100%. . tables[1] . . coef std err t P&gt;|t| [0.025 0.975] VIF . feature . const 104.2364 | 6.885 | 15.139 | 0.000 | 90.741 | 117.732 | 46.593076 | . yr 81.9409 | 2.028 | 40.405 | 0.000 | 77.966 | 85.916 | 1.010528 | . mnth 4.9372 | 0.306 | 16.130 | 0.000 | 4.337 | 5.537 | 1.088734 | . hr 6.4545 | 0.155 | 41.600 | 0.000 | 6.150 | 6.759 | 1.131082 | . time_label -46.7594 | 0.928 | -50.394 | 0.000 | -48.578 | -44.941 | 1.052451 | . holiday -24.0548 | 6.269 | -3.837 | 0.000 | -36.343 | -11.767 | 1.079365 | . weekday 1.9334 | 0.506 | 3.819 | 0.000 | 0.941 | 2.926 | 1.013161 | . workingday 3.8622 | 2.243 | 1.722 | 0.085 | -0.535 | 8.259 | 1.071211 | . atemp 329.9434 | 6.103 | 54.058 | 0.000 | 317.980 | 341.907 | 1.081206 | . hum -179.8373 | 5.760 | -31.224 | 0.000 | -191.127 | -168.548 | 1.213499 | . windspeed 9.7759 | 8.717 | 1.121 | 0.262 | -7.310 | 26.862 | 1.117727 | . Next, above is the second table of model results, showing each predictor and its coefficients and other statistics. Notably, all of the variables were significant (p &lt; 0.05), except for workingday and windspeed. . It is also good that all of the predictors&#39; variance inflation factors (VIF) were close to 1 and lower than 5. (Do not mind the VIF of const, since this represents the constant term, not a predictor.) These values indicate low multicollinearity among the predictors. . tables[2] . . 0 1 2 3 . 0 Omnibus: | 3471.487 | Durbin-Watson: | 0.663 | . 1 Prob(Omnibus): | 0.000 | Jarque-Bera (JB): | 7064.939 | . 2 Skew: | 1.192 | Prob(JB): | 0.000 | . 3 Kurtosis: | 5.018 | Cond. No. | 152.000 | . Lastly, the Jarque-Bera test statistic is significant, so the model violates the assumption of normality of residuals. Furthermore, the Durbin-Watson test statistic (0.663) is below the ideal range (1.5-2.5), indicating strong positive autocorrelation among residuals. . Overall, due to the low R-squared value mentioned earlier and the violation of certain assumptions of linear regression, this model may not work very well for prediction. This will be tested in the next part. . Predictive Modelling . Before making a predictive model, I must make training and testing sets. Given that the data is time-series data, it would be appropriate to ensure that all test set observations occur after the training set observations. This will allow me to test the models in a similar way to how they would be used in the real world: to predict the number of bike rentals at a given time in the future. . Thus, before I perform a train-test split, I must order the observations by dteday. This has been done below. . from sklearn.model_selection import train_test_split data = data.sort_values(&quot;dteday&quot;, ascending = True) X = sms.add_constant(data[considered_features]) y = data[target_col] X_train, X_test, y_train, y_test = train_test_split( X, y, # When shuffle = False, the second part of the dataset is used as the test set. shuffle = False, test_size = 0.2, ) # KDE plots sns.kdeplot(y_train) sns.kdeplot(y_test) plt.title(&quot;Comparison of Target Distributions&quot;) plt.xlabel(&quot;Hourly Count of Bike Rentals&quot;) plt.ylabel(&quot;Probability Density&quot;) plt.legend([&quot;y_train&quot;, &quot;y_test&quot;]) plt.show() . . C: Users migs anaconda3 envs new_streamlit_env2 lib site-packages statsmodels tsa tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument &#39;objs&#39; will be keyword-only x = pd.concat(x[::order], 1) . After the train-test split, it seems that the two sets have roughly similar distributions. Both are right-skewed, as the low values appear frequently and the outliers are high values. . Now, we can use RMSE (Root Mean Squared Error) to evaluate the predictive model. . lr = LinearRegression() lr.fit(X_train, y_train) def compare_train_test_rmse(model, X_train, X_test, y_train, y_test): y_pred = model.predict(X_train) mse = mean_squared_error(y_train, y_pred) rmse = np.sqrt(mse) print(f&quot;Train set: RMSE = {rmse}&quot;) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) rmse = np.sqrt(mse) print(f&quot;Test set: RMSE = {rmse}&quot;) print(&quot;Linear Regression&quot;) compare_train_test_rmse(lr, X_train, X_test, y_train, y_test) . . Linear Regression Train set: RMSE = 121.87152363617122 Test set: RMSE = 171.70884445091582 . Before I interpret these results, please note the following terminology: . RMSE: Root Mean Squared Error. It can be interpreted as the average distance of the predicted values from the real values. A lower value indicates better performance. | Test set RMSE: The RMSE resulting from evaluating the model on the testing set. We use this to assess how well the model performs on previously unseen data. | Train set RMSE: The RMSE resulting from evaluating the model on the training set. Since the model already saw the training set while it was being trained, this value is not a useful metric of performance. However, it is helpful for determining whether overfitting may have occurred. | Overfitting: The event in which the model has become too sensitive to the small changes in the training data. It is unable to identify the general patterns that help make accurate predictions. If the model performs much worse (higher RMSE) on the test set compared to the training set, this may indicate overfitting. | . The test set RMSE is roughly 172, which means that on average, the model&#39;s predicted counts of bike rentals were 172 off from the true counts. Let&#39;s put that into perspective by looking at the distribution of the target variable. . sns.histplot(data = data, x = &quot;cnt&quot;) plt.title(&quot;Distribution of Target Variable in the Whole Dataset&quot;) plt.xlabel(&quot;Hourly Number of Bike Rentals&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() . The chart shows that the most common values in the distribution range between 0 and 400, and there are some outlying high values. . Let&#39;s say that the true value of one observation is equal to 200 bike rentals. Since the RMSE is 172, the predicted value may usually end up being around 28 or 372. Each of these predictions is very low or high given the range of the distribution. Therefore, an RMSE of 172 indicates very poor predictive ability. . Also, the test set RMSE is higher than the train set RMSE by around 50. Thus, the model has somewhat overfitted. It would be better if the test set RMSE is almost as low as the train set RMSE. This would indicate that the model is able to recognize general patterns. . Later on, we&#39;ll see how this model compares to the other two models. . Decision Tree . A High-Level Explanation . Here, I will briefly explain how decision trees work for regression problems, based on the Dataquest course and some articles by Sriram (2020) and Sayad (2022). Take note of the following terms: . feature, predictor: A variable used to make a prediction. Multiple features may be used in one model. | target, response variable: The variable that the model attempts to predict. | . Before a DT can make predictions, the tree must first be built. We start with one node, and each node has a conditional statement about a feature. For example, if the feature is temperature, a node may check whether the temperature is lower than 25 degrees Celsius. The &quot;False&quot; case splits off to the left branch, whereas the &quot;True&quot; case splits off to the right branch. . To determine the best feature to use in a split, the model uses an error metric. For example, one may use Mean Squared Error, Mean Absolute Error, Standard Deviation, etc. At each split, the model determines the split that will maximize error reduction. Reducing the error means reducing the spread of the target values, so that the mean value is close to the real values. . The tree keeps splitting nodes and branching off. Eventually, the tree reaches nodes where the error is very low. Thus, these nodes become leaf nodes. The mean target value of each node is used as the predicted value. . When predictions are made on a new observation, the model starts at the root node, checks the conditional statement, moves to the appropriate branch, and repeats this process until a leaf node is reached. Then, the leaf node&#39;s predicted value is the output. . The implication is that, unlike linear regression, the model does not assume that there are linear relationships between the predictors and the target. Rather, it narrows down the possible target values through process-of-elimination. Thus, it is able to find non-linear relationships. This makes DTs potentially more accurate than linear regression in some scenarios. . Evaluating a DT Regressor . Let&#39;s now create and evaluate a DT regressor. For the features of the model, I decided to use the ones listed below. . considered_features = pd.Series([ &quot;season&quot;, &quot;yr&quot;, &quot;mnth&quot;, &quot;hr&quot;, &quot;time_label&quot;, &quot;holiday&quot;, &quot;weekday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;, ]) . Earlier, I removed some features due to collinearity issues. However, collinearity is not an issue for Decision Trees because these are not like linear regression. That is why I have not removed the collinear features before performing Decision Tree regression. . Continuing on, let us fit and evaluate the model. . data = data.sort_values(&quot;dteday&quot;, ascending = True) X = data[considered_features] y = data[target_col] X_train, X_test, y_train, y_test = train_test_split( X, y, # When shuffle = False, the second part of the dataset is used as the test set. shuffle = False, test_size = 0.2, ) tree = DecisionTreeRegressor(random_state = 0) tree.fit(X_train, y_train) print(&quot;Decision Tree&quot;) compare_train_test_rmse(tree, X_train, X_test, y_train, y_test) . . Decision Tree Train set: RMSE = 0.514411456857934 Test set: RMSE = 86.61283689808975 . Interestingly, the test set RMSE is now roughly 87. This means that the predicted counts of bike rentals are around 87 away from the true counts. This error value is much lower (and therefore better) than that of the linear regression model shown earlier. . However, the DT model also seems to suffer from overfitting. The test set RMSE is much higher than the train set RMSE (around 0.5). . Thus, we may need to adjust the parameters of the tree to keep it from growing too large. In the code cell below, I have built the tree again, but with constraints on: . The maximum depth (number of splits from root to leaf nodes) | The minimum number of samples required to split a node | . tree = DecisionTreeRegressor( max_depth = 20, min_samples_split = 20, random_state = 0, ) tree.fit(X_train, y_train) print(&quot;Decision Tree&quot;) compare_train_test_rmse(tree, X_train, X_test, y_train, y_test) . . Decision Tree Train set: RMSE = 33.94259741687692 Test set: RMSE = 78.39525816583571 . These results are slightly better than before. The test set RMSE is around 79, which is better than the previous value of 87. Furthermore, The train set RMSE and test set RMSE are closer to each other, so the model suffers less overfitting. . (Note that the train set RMSE increased. This is not a bad thing; it only means that the model is less sensitive to small variations in the training data.) . Though these results are decent, they can be improved through the use of a Random Forest. . Random Forest . A Random Forest (RF) is an ensemble. This means that it contains multiple individual models. When the ensemble is used to make a prediction, each model first makes its own prediction, then the predictions are combined to make a final prediction. For example, the mean of the models&#39; predictions is taken as the final prediction. . The &quot;Forest&quot; in RF&#39;s name refers to the fact that each model in the ensemble is a Decision Tree. The &quot;Random&quot; in RF&#39;s name refers to the fact that some random decisions are made while each tree is built, such that each tree is different from the others in the forest. . The processes that introduce randomness are bootstrap aggregation and feature subsets. However, I won&#39;t go into detail about how those work. The important point is that since each tree is different, it takes a different approach to coming up with a predicted value. By combining varied approaches, we can make a more accurate final prediction. . In the code cell below, I have constructed a Random Forest with 100 Decision Trees. . rf = RandomForestRegressor( n_estimators = 100, max_depth = 30, min_samples_split = 15, random_state = 0, bootstrap = True, ) rf.fit(X_train, y_train) print(&quot;Random Forest&quot;) compare_train_test_rmse(rf, X_train, X_test, y_train, y_test) . . Random Forest Train set: RMSE = 29.62524751679492 Test set: RMSE = 70.73988695365036 . Now, the test set RMSE is around 71, and this is better than the previous value of 78. . Therefore, using a Random Forest provided an advantage over just one Decision Tree. Note, however, that the Random Forest took over 2 seconds to fit, whereas the Decision Tree took less than a second. As the number of trees in the forest increases, the amount of time required for fitting also increases. One must consider this tradeoff in order to save time and make an accurate model. . Summary . In summary, we compared the performance of these three models in predicting the number of bike rentals that may occur during a particular hour. . Linear Regression had the poorest performance. The model only explained 46.3% of the variance in the data, and the test set RMSE was around 172. It also seemed to be overfit. | The Decision Tree initially suffered from much overfitting, but it performed better when restrictions on the size of the tree were put into place. The test set RMSE was around 78. | The Random Forest with 100 trees was the best-performing model. The test set RMSE was around 71. | . It is important to compare the RMSEs of a model when evaluated on the training set and the testing set so that potential overfitting can be identified and addressed. Decision Trees can be more accurate than Linear Regression because trees can find non-linear relationships. Random Forests can be more accurate than a single Decision Tree because forests combine the predictions of multiple models together. . Thanks for reading! . Bibliography . Data Source . Fanaee-T, H. (2013, December 20). Bike Sharing Dataset. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset . Information Sources . Dataquest. (n.d.). Guided Project: Predicting Bike Rentals. Dataquest. https://app.dataquest.io/c/22/m/213/guided-project%3A-predicting-bike-rentals/1/introduction-to-the-dataset . Sayad, S. (2022). Decision Tree—Regression. SaedSayad.Com. https://saedsayad.com/decision_tree_reg.htm . scikit-learn developers. (2021). Sklearn.tree.DecisionTreeRegressor. Scikit-Learn. https://scikit-learn/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html . Sriram, A. (2020, June 5). Decision Tree for Regression—The Recipe. Analytics Vidhya. https://medium.com/analytics-vidhya/decision-tree-for-regression-the-recipe-74f7628b8a0 . Image Source . Mingot, S. (2020, January 14). Photo by Stéphane Mingot on Unsplash. Unsplash. https://unsplash.com/photos/e8msPzLTXxU .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/sklearn/statsmodels/2022/01/16/Comparison-Regression-Models-Predicting-Bike-Rentals.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/sklearn/statsmodels/2022/01/16/Comparison-Regression-Models-Predicting-Bike-Rentals.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Using Linear Regression to Predict House Sale Prices",
            "content": "Unsplash | Breno Assis Introduction . The Ames, Iowa housing dataset was formed by De Cock in 2011 as a high-quality dataset for regression projects. It contains data on 80 features of 2930 houses. The target variable is the sale price of each house. . In order to predict the target, I will use linear regression for both statistical inference and machine learning. To each feature (or independent variable), the model will assign a coefficient that shows how the feature affects the target (or dependent variable). I will use the model&#39;s $p$ values to determine the features with statistically significant effects, then use these features in our final model for predicting prices from new data. . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically the Guided Project: Predicting House Sale Prices. The general project flow and research questions were guided by Dataquest. Other than what was instructed, I also added my own steps. You can visit the official solution to compare it to my project. . Below are the packages used in this project. . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import statsmodels.api as sms from sklearn.linear_model import LinearRegression # Import custom modules that I wrote. from custom_modules import linreg_tools as lrt, stratified_kfcv as skf . Note that some of the imported modules are custom ones that I wrote. To view these modules, visit this directory in my website&#39;s repository. . Data Inspection and Cleaning . The journal article that introduced the Ames, Iowa housing dataset is linked here. You may click the &quot;PDF&quot; button on the webpage to access the article, which contains the links to download the data file. . A summary of the columns and their data types is shown below. . houses = pd.read_excel(&quot;./private/Linear-Regression-House-Prices-Files/AmesHousing.xls&quot;) houses.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2930 entries, 0 to 2929 Data columns (total 82 columns): # Column Non-Null Count Dtype -- -- 0 Order 2930 non-null int64 1 PID 2930 non-null int64 2 MS SubClass 2930 non-null int64 3 MS Zoning 2930 non-null object 4 Lot Frontage 2440 non-null float64 5 Lot Area 2930 non-null int64 6 Street 2930 non-null object 7 Alley 198 non-null object 8 Lot Shape 2930 non-null object 9 Land Contour 2930 non-null object 10 Utilities 2930 non-null object 11 Lot Config 2930 non-null object 12 Land Slope 2930 non-null object 13 Neighborhood 2930 non-null object 14 Condition 1 2930 non-null object 15 Condition 2 2930 non-null object 16 Bldg Type 2930 non-null object 17 House Style 2930 non-null object 18 Overall Qual 2930 non-null int64 19 Overall Cond 2930 non-null int64 20 Year Built 2930 non-null int64 21 Year Remod/Add 2930 non-null int64 22 Roof Style 2930 non-null object 23 Roof Matl 2930 non-null object 24 Exterior 1st 2930 non-null object 25 Exterior 2nd 2930 non-null object 26 Mas Vnr Type 2907 non-null object 27 Mas Vnr Area 2907 non-null float64 28 Exter Qual 2930 non-null object 29 Exter Cond 2930 non-null object 30 Foundation 2930 non-null object 31 Bsmt Qual 2850 non-null object 32 Bsmt Cond 2850 non-null object 33 Bsmt Exposure 2847 non-null object 34 BsmtFin Type 1 2850 non-null object 35 BsmtFin SF 1 2929 non-null float64 36 BsmtFin Type 2 2849 non-null object 37 BsmtFin SF 2 2929 non-null float64 38 Bsmt Unf SF 2929 non-null float64 39 Total Bsmt SF 2929 non-null float64 40 Heating 2930 non-null object 41 Heating QC 2930 non-null object 42 Central Air 2930 non-null object 43 Electrical 2929 non-null object 44 1st Flr SF 2930 non-null int64 45 2nd Flr SF 2930 non-null int64 46 Low Qual Fin SF 2930 non-null int64 47 Gr Liv Area 2930 non-null int64 48 Bsmt Full Bath 2928 non-null float64 49 Bsmt Half Bath 2928 non-null float64 50 Full Bath 2930 non-null int64 51 Half Bath 2930 non-null int64 52 Bedroom AbvGr 2930 non-null int64 53 Kitchen AbvGr 2930 non-null int64 54 Kitchen Qual 2930 non-null object 55 TotRms AbvGrd 2930 non-null int64 56 Functional 2930 non-null object 57 Fireplaces 2930 non-null int64 58 Fireplace Qu 1508 non-null object 59 Garage Type 2773 non-null object 60 Garage Yr Blt 2771 non-null float64 61 Garage Finish 2771 non-null object 62 Garage Cars 2929 non-null float64 63 Garage Area 2929 non-null float64 64 Garage Qual 2771 non-null object 65 Garage Cond 2771 non-null object 66 Paved Drive 2930 non-null object 67 Wood Deck SF 2930 non-null int64 68 Open Porch SF 2930 non-null int64 69 Enclosed Porch 2930 non-null int64 70 3Ssn Porch 2930 non-null int64 71 Screen Porch 2930 non-null int64 72 Pool Area 2930 non-null int64 73 Pool QC 13 non-null object 74 Fence 572 non-null object 75 Misc Feature 106 non-null object 76 Misc Val 2930 non-null int64 77 Mo Sold 2930 non-null int64 78 Yr Sold 2930 non-null int64 79 Sale Type 2930 non-null object 80 Sale Condition 2930 non-null object 81 SalePrice 2930 non-null int64 dtypes: float64(11), int64(28), object(43) memory usage: 1.8+ MB . The output above shows the name of each column, its number of non-null values, and its data type. Most of the names are self-explanatory, but others are not so clear. One can visit the data documentation to learn what each column represents. . Here are the first steps I took in order to clean these columns: . Based on a suggestion in page 4 of De Cock (2011), I deleted 5 outlier observations which had above-ground living areas higher than 4000 square feet. | I selected 9 useful numeric/ordinal columns and 2 useful categorical columns based on their descriptions in the data documentation. | I transformed the Overall Qual and Overall Cond columns. Originally, these contained integers from 1 to 10, which represented ordinal ratings from Very Poor to Very Excellent. I put these ratings into 4 groups: 1 represents Very Poor to Fair | 2 represents Below Average to Above Average | 3 represents Good and Very Good | 4 represents Excellent and Very Excellent | . | I inspected the missing values in my final set of columns. | . # Remove outliers based on suggestion in journal article houses = houses.loc[houses[&quot;Gr Liv Area&quot;] &lt; 4000] # Identify important columns useful_numerics = [ &quot;Lot Frontage&quot;, &quot;Lot Area&quot;, &quot;Mas Vnr Area&quot;, &quot;Total Bsmt SF&quot;, &quot;Gr Liv Area&quot;, &quot;Fireplaces&quot;, &quot;Garage Area&quot;, # The 2 below are ordinal. &quot;Overall Qual&quot;, &quot;Overall Cond&quot;, ] useful_categoricals = [ &quot;Lot Config&quot;, &quot;Bldg Type&quot;, ] target_col = &quot;SalePrice&quot; # Keep only the important columns houses = houses.loc[:, useful_numerics + useful_categoricals + [target_col]] ratings_simplified = { 10: 4, 9: 4, 8: 3, 7: 3, 6: 2, 5: 2, 4: 2, 3: 1, 2: 1, 1: 1, } # Replace integers in these two columns with a new set of integers for col in [&quot;Overall Qual&quot;, &quot;Overall Cond&quot;]: houses[col] = houses[col].replace(ratings_simplified) houses.isnull().sum() / houses.shape[0] * 100 . . Lot Frontage 16.752137 Lot Area 0.000000 Mas Vnr Area 0.786325 Total Bsmt SF 0.034188 Gr Liv Area 0.000000 Fireplaces 0.000000 Garage Area 0.034188 Overall Qual 0.000000 Overall Cond 0.000000 Lot Config 0.000000 Bldg Type 0.000000 SalePrice 0.000000 dtype: float64 . The table above shows the percentage of missing values in each column. Most columns have less than 1% missingness, but the Lot Frontage column has almost 17% missingness. This is higher than my preferred benchmark of 5% missingness, so I will remove the Lot Frontage column. . After deleting that column, I will remove rows where there are any remaining missing values. . houses = houses.drop(&quot;Lot Frontage&quot;, axis = 1).dropna() print(f&quot;Total number of missing values: {houses.isnull().sum().sum()}&quot;) print(f&quot;New shape: {houses.shape}&quot;) . . Total number of missing values: 0 New shape: (2900, 11) . Now, the dataset has 2900 rows and 11 columns, including 10 features and 1 target. . Next, two of my features are categorical. Thus, in order to use them in regression, I have to dummy code them. According to the UCLA: Statistical Consulting Group (2021), if there are $k$ categories in one variable, I must make $k - 1$ new variables containing zeroes and ones. The reason that only $k - 1$ variables are made is that one category has to be set aside as the &quot;reference level.&quot; The other categories will be compared to the reference level when I fit the model. . In Python, I will do this using the pd.get_dummies() function. I will set the drop_first parameter to True so that one category will be excluded from the final variables. . cat_features = [&quot;Lot Config&quot;, &quot;Bldg Type&quot;] houses = houses.merge( pd.get_dummies(houses[cat_features], drop_first = True), left_index = True, right_index = True, how = &quot;outer&quot;, ) houses = houses.drop(cat_features, axis = 1) ( houses .loc[:, houses.columns.to_series().str.match(r&quot;Lot Config|Bldg Type&quot;)] .head() ) . . Lot Config_CulDSac Lot Config_FR2 Lot Config_FR3 Lot Config_Inside Bldg Type_2fmCon Bldg Type_Duplex Bldg Type_Twnhs Bldg Type_TwnhsE . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . The table above shows the first 5 rows of the new variables that were created. Since there were 5 categories in the Lot Config variable, 4 new variables were created. This is also the case for the Bldg Type variable. . Let&#39;s take the first column, Lot Config_CulDSac, as an example. It represents houses that are situated on a cul-de-sac. A house with a value of 1 in this column is a house on a cul-de-sac. Other houses, with values of 0, are not. . The data is clean now, so I can proceed to statistical inference. . Statistical Inference . In this section, I will fit an Ordinary least Squares (OLS) linear regression model. Then, I will check the assumptions of linear regression in order to make sure that this is the right model for the problem. I will also use the results of the model to select the best features. . The code below fits the OLS model and outputs the first of three tables of results. I will interpret the results based on what I learned from the article &quot;Linear Regression&quot; by Python for Data Science (n.d.). . feature_cols = [x for x in houses.columns if x != target_col] # Add a constant column in the dataset so that the model can find the y-intercept. X = sms.add_constant( houses[feature_cols] ) y = houses[target_col] # Obtain Variance Inflation Factors. vif_df = lrt.get_vif(X) model = sms.OLS(y, X) results = model.fit() summary = results.summary() tables = lrt.extract_summary(summary, vif_df) tables[0] . . C: Users migs anaconda3 envs new_streamlit_env2 lib site-packages statsmodels tsa tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument &#39;objs&#39; will be keyword-only x = pd.concat(x[::order], 1) . 0 1 2 3 . 0 Dep. Variable: | SalePrice | R-squared: | 0.838 | . 1 Model: | OLS | Adj. R-squared: | 0.837 | . 2 Method: | Least Squares | F-statistic: | 929.200 | . 3 Date: | Fri, 14 Jan 2022 | Prob (F-statistic): | 0.000 | . 4 Time: | 14:44:14 | Log-Likelihood: | -34163.000 | . 5 No. Observations: | 2900 | AIC: | 68360.000 | . 6 Df Residuals: | 2883 | BIC: | 68460.000 | . 7 Df Model: | 16 | NaN | NaN | . 8 Covariance Type: | nonrobust | NaN | NaN | . This first table shows two important things. The R-squared value is equal to 0.838, meaning that the model explained 83.8% of the variance in the data. Ideally, I would want a value close to 100% but this is good. Also, the p value of the F-statistic is equal to 0. Since $p &lt; 0.05$, the model was statistically significant overall. . Next, below is the second table of results. . tables[1] . . coef std err t P&gt;|t| [0.025 0.975] VIF . feature . const -105800.0000 | 4319.486 | -24.498 | 0.000 | -114000.000 | -97400.000 | 53.813031 | . Lot Area 0.5717 | 0.084 | 6.789 | 0.000 | 0.407 | 0.737 | 1.243636 | . Mas Vnr Area 49.6396 | 3.863 | 12.849 | 0.000 | 42.065 | 57.214 | 1.334007 | . Total Bsmt SF 40.8966 | 1.780 | 22.979 | 0.000 | 37.407 | 44.386 | 1.618427 | . Gr Liv Area 48.9365 | 1.660 | 29.481 | 0.000 | 45.682 | 52.191 | 1.883476 | . Fireplaces 7000.7868 | 1069.148 | 6.548 | 0.000 | 4904.416 | 9097.158 | 1.373267 | . Garage Area 53.8240 | 3.528 | 15.256 | 0.000 | 46.906 | 60.742 | 1.643206 | . Overall Qual 44530.0000 | 1344.831 | 33.113 | 0.000 | 41900.000 | 47200.000 | 1.924753 | . Overall Cond 9206.0505 | 1269.656 | 7.251 | 0.000 | 6716.526 | 11700.000 | 1.056642 | . Lot Config_CulDSac 14770.0000 | 2804.614 | 5.265 | 0.000 | 9266.896 | 20300.000 | 1.307018 | . Lot Config_FR2 371.8734 | 3776.296 | 0.098 | 0.922 | -7032.639 | 7776.385 | 1.156834 | . Lot Config_FR3 8290.0511 | 8929.437 | 0.928 | 0.353 | -9218.674 | 25800.000 | 1.026279 | . Lot Config_Inside 4695.2344 | 1586.888 | 2.959 | 0.003 | 1583.685 | 7806.784 | 1.425754 | . Bldg Type_2fmCon -21140.0000 | 4144.743 | -5.100 | 0.000 | -29300.000 | -13000.000 | 1.036636 | . Bldg Type_Duplex -24340.0000 | 3214.492 | -7.572 | 0.000 | -30600.000 | -18000.000 | 1.078048 | . Bldg Type_Twnhs -9050.9650 | 3363.564 | -2.691 | 0.007 | -15600.000 | -2455.731 | 1.096859 | . Bldg Type_TwnhsE 3408.7248 | 2328.503 | 1.464 | 0.143 | -1156.975 | 7974.425 | 1.146414 | . Each row other than the constant row represents a feature. . The coef column gives the increase in the target for a one-unit increase in the feature (Frost, 2017). For example, the coefficient of Gr Liv Area (above-ground living area in square feet) is around 49. Therefore, for every 1 square foot increase in living area, the sale price increases by USD 49. . According to UCLA: Statistical Consulting Group (2021), the coefficient of a dummy-coded variable gives the difference between the mean of the target for the level of interest and the mean of the target for the reference level. For example, the coefficient of Lot Config_CulDSac is 14,770. Therefore, the price of a house on a cul-de-sac is usually USD 14,770 higher than that of a house on a corner lot. . The P&gt;|t| column gives the p value of each feature. A p value less than 0.05 can be considered statistically significant, whereas a p value above that threshold is not. In the case of this model, most of the features are statistically significant. . The insignificant features are Lot Config_FR2 (houses with frontage on 2 sides), Lot Config_FR3, and Bldg Type_TwnhsE (Townhouse End Units). Though these are not significant, I will keep them in the model because they are necessary components of the dummy-coded categorical variables. . Next, the VIF column contains Variance Inflation Factors. I wrote code to calculate these values based on an article, &quot;Detecting Multicollinearity with VIF – Python,&quot; by cosine1509 (2020). . According to Frost (2017), VIFs are a measure of multicollinearity among independent variables. The lowest possible value is 1, which means no collinearity at all. Values between 1 and 5 show low to moderate multicollinearity, so these are acceptable. However, values over 5 show high multicollinearity and need to be investigated. . In the case of this model, all of the VIFs of the features are below 2. The VIF of the constant term is high, but this is not important. . In summary, based on the p values and VIF values, there is no need to change the features used in the model. Most of them are significant, and all of them have low multicollinearity. . Finally, let us look at the third table of results. . tables[2] . . 0 1 2 3 . 0 Omnibus: | 421.519 | Durbin-Watson: | 1.583 | . 1 Prob(Omnibus): | 0.000 | Jarque-Bera (JB): | 3490.274 | . 2 Skew: | 0.426 | Prob(JB): | 0.000 | . 3 Kurtosis: | 8.307 | Cond. No. | 195000.000 | . The Durbin-Watson test statistic is 1.583. According to Python for Data Science (n.d.), this statistic should ideally be close to 2. If it is below 2, there is positive autocorrelation among the residuals, so one of the assumptions of linear regression is violated. However, as a rule of thumb, values between 1.5 and 2.5 are acceptable. Thus, this statistic is not a cause of concern. . On the other hand, the Jarque-Bera p value is 0. Because $p &lt; 0.05$, the statistic is significant, so the residuals are not distributed normally. However, Mordkoff (2016) says that according to the Central Limit Theorem, &quot;as long as the sample is based on 30 or more observations, the sampling distribution of the mean can be safely assumed to be normal.&quot; Therefore, this is fine. . That&#39;s it for the statistical inference part of this project. Now that I know that my variables are statistically significant, I can make a predictive model and evaluate it. . Predictive Modeling and Evaluation . In this last part, I will use scikit-learn to fit a linear regression model for prediction. A 5-fold cross-validation will be used in order to evaluate the model. . In my recent project about the KNN model, I wrote some custom functions that help perform stratified cross-validation. This means that the folds are divided such that each fold&#39;s target distribution (in this case, price distribution) is similar to that of the full sample. I put the custom functions in this module for your reference. . I performed stratified 5-fold cross-validation on the housing data, so 5 models were fitted and their RMSEs were recorded. The results are shown below. . fold_series = skf.stratify_continuous( n_folds = 5, y = houses[&quot;SalePrice&quot;], ) lr = LinearRegression() mse_lst = skf.stratified_kfcv( X = sms.add_constant(houses[feature_cols]), y = houses[target_col], fold_series = fold_series, regression_model = lr, ) rmses = pd.Series(mse_lst).pow(1/2) print(f&quot;&quot;&quot;RMSE Values: {rmses} Mean RMSE: {rmses.mean()} SD RMSE: {rmses.std(ddof = 1)}&quot;&quot;&quot;) . . RMSE Values: 0 32046.873780 1 30927.932397 2 30637.750905 3 33020.953872 4 32832.216461 dtype: float64 Mean RMSE: 31893.145482987817 SD RMSE: 1082.253470974206 . C: Users migs anaconda3 envs new_streamlit_env2 lib site-packages statsmodels tsa tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument &#39;objs&#39; will be keyword-only x = pd.concat(x[::order], 1) . The mean RMSE is 31,893. Therefore, the predicted prices are usually USD 31,893 away from the true prices. Also, the standard deviation RMSE is around 1082, so the RMSE values were relatively consistent from test to test. . The mean RMSE may seem high, but remember the distribution of house prices in the dataset: . sns.histplot( data = houses, x = &quot;SalePrice&quot; ) plt.title(&quot;Distribution of House Sale Prices&quot;) plt.xlabel(&quot;Sale Price (USD)&quot;) plt.show() . . The prices are mostly around USD 100,000 to USD 300,000. Thus, the mean RMSE is relatively small considering the prices. Therefore, I would say that the model performs decently, but it could be improved. . Conclusion . In this project, I cleaned data about houses in Ames, Iowa, selected features that may be useful, interpreted linear regression results to determine significant features, then evaluated a predictive model. I found that the 16 independent variables explained 83.8% of the variance $(R^2 = 0.838, F(16, 2883 = 929.2), p &lt; 0.05)$. Furthermore, the predictive model had a mean RMSE of 31,880, which was decent but not ideal. . Thanks for reading! . Bibliography . Data Source . De Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education, 19(3), null. https://doi.org/10.1080/10691898.2011.11889627 . Information Sources . Coding Systems for Categorical Variables in Regression Analysis. (n.d.). UCLA: Statistical Consulting Group. Retrieved December 26, 2020, from https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis-2/ . cosine1509. (2020, August 14). Detecting Multicollinearity with VIF - Python. GeeksforGeeks. https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/ . Frost, J. (2017a, April 2). Multicollinearity in Regression Analysis: Problems, Detection, and Solutions. Statistics By Jim. http://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/ . Frost, J. (2017b, April 12). How to Interpret P-values and Coefficients in Regression Analysis. Statistics By Jim. http://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/ . Mordkoff, J. T. (2016). The Assumption(s) of Normality. http://www2.psychology.uiowa.edu/faculty/mordkoff/GradStats/part%201/I.07%20normal.pdf . Python for Data Science. (2018, March 15). Linear Regression. Python for Data Science. https://pythonfordatascienceorg.wordpress.com/linear-regression-python/ . Image Source . Assis, B. (2018, January 17). Photo by Breno Assis on Unsplash. Unsplash. https://unsplash.com/photos/r3WAWU5Fi5Q .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/seaborn/matplotlib/sklearn/statsmodels/2021/12/28/Linear-Regression-House-Prices.html",
            "relUrl": "/python/pandas/seaborn/matplotlib/sklearn/statsmodels/2021/12/28/Linear-Regression-House-Prices.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
            "content": "Wheelscene | Chris Smith Introduction . K Nearest Neighbors or KNN is an an algorithm that can make predictions based on the similarity between different observations. In this project, I used KNN to predict the price of a car based on how similar its features are to those of other cars. Towards this end, I applied various machine learning techniques, such as standardization, feature selection, train-test split, hyperparameter optimization, and k-fold cross validation. . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically the Guided Project: Predicting Car Prices. The general project flow and research questions were guided by Dataquest. Furthermore, though the mathematical explanations in this post were written in my own words, I learned the theory from Dataquest. . Below are the packages used in this project. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import re from scipy.stats import zscore from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import KFold, cross_val_score, train_test_split from sklearn.feature_selection import f_regression, SelectKBest from sklearn.metrics import mean_squared_error . Data Inspection and Cleaning . The dataset for this project is the Automobile Data Set by Schlimmer (1987), from the UCI Machine Learning Repository. The data and its description can be obtained here. . The dataset describes 26 features of hundreds of cars. A summary of the features and their data types is shown below. . # Data dictionary from documentation. data_dict = &quot;&quot;&quot;1. symboling: -3, -2, -1, 0, 1, 2, 3. 2. normalized-losses: continuous from 65 to 256. 3. make: alfa-romero, audi, bmw, chevrolet, dodge, honda, isuzu, jaguar, mazda, mercedes-benz, mercury, mitsubishi, nissan, peugot, plymouth, porsche, renault, saab, subaru, toyota, volkswagen, volvo 4. fuel-type: diesel, gas. 5. aspiration: std, turbo. 6. num-of-doors: four, two. 7. body-style: hardtop, wagon, sedan, hatchback, convertible. 8. drive-wheels: 4wd, fwd, rwd. 9. engine-location: front, rear. 10. wheel-base: continuous from 86.6 120.9. 11. length: continuous from 141.1 to 208.1. 12. width: continuous from 60.3 to 72.3. 13. height: continuous from 47.8 to 59.8. 14. curb-weight: continuous from 1488 to 4066. 15. engine-type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor. 16. num-of-cylinders: eight, five, four, six, three, twelve, two. 17. engine-size: continuous from 61 to 326. 18. fuel-system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi. 19. bore: continuous from 2.54 to 3.94. 20. stroke: continuous from 2.07 to 4.17. 21. compression-ratio: continuous from 7 to 23. 22. horsepower: continuous from 48 to 288. 23. peak-rpm: continuous from 4150 to 6600. 24. city-mpg: continuous from 13 to 49. 25. highway-mpg: continuous from 16 to 54. 26. price: continuous from 5118 to 45400.&quot;&quot;&quot; # Use regex to extract column names from data dictionary. col_names = re.findall( pattern = r&quot;^[0-9]{1,2} . ([a-z -]+):&quot;, string = data_dict, # Use multiline flag so that ^ indicates the start of a line. flags = re.MULTILINE, ) # Read data file and add column names. cars_df = pd.read_csv( &quot;./private/Car-Prices-KNN-Files/imports-85.data&quot;, names = col_names, ) cars_df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 205 entries, 0 to 204 Data columns (total 26 columns): # Column Non-Null Count Dtype -- -- 0 symboling 205 non-null int64 1 normalized-losses 205 non-null object 2 make 205 non-null object 3 fuel-type 205 non-null object 4 aspiration 205 non-null object 5 num-of-doors 205 non-null object 6 body-style 205 non-null object 7 drive-wheels 205 non-null object 8 engine-location 205 non-null object 9 wheel-base 205 non-null float64 10 length 205 non-null float64 11 width 205 non-null float64 12 height 205 non-null float64 13 curb-weight 205 non-null int64 14 engine-type 205 non-null object 15 num-of-cylinders 205 non-null object 16 engine-size 205 non-null int64 17 fuel-system 205 non-null object 18 bore 205 non-null object 19 stroke 205 non-null object 20 compression-ratio 205 non-null float64 21 horsepower 205 non-null object 22 peak-rpm 205 non-null object 23 city-mpg 205 non-null int64 24 highway-mpg 205 non-null int64 25 price 205 non-null object dtypes: float64(5), int64(5), object(16) memory usage: 41.8+ KB . There are 205 cars and 26 features. Most of the features directly describe physical characteristics of the cars. Some exceptions are &quot;symboling&quot; and &quot;normalized-losses&quot;, which are values related to car insurance and are beyond the scope of this project. Also, the &quot;price&quot; column provides the price of each car in USD. . Let us look at the first five rows. . cars_df.head() . . symboling normalized-losses make fuel-type aspiration num-of-doors body-style drive-wheels engine-location wheel-base ... engine-size fuel-system bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price . 0 3 | ? | alfa-romero | gas | std | two | convertible | rwd | front | 88.6 | ... | 130 | mpfi | 3.47 | 2.68 | 9.0 | 111 | 5000 | 21 | 27 | 13495 | . 1 3 | ? | alfa-romero | gas | std | two | convertible | rwd | front | 88.6 | ... | 130 | mpfi | 3.47 | 2.68 | 9.0 | 111 | 5000 | 21 | 27 | 16500 | . 2 1 | ? | alfa-romero | gas | std | two | hatchback | rwd | front | 94.5 | ... | 152 | mpfi | 2.68 | 3.47 | 9.0 | 154 | 5000 | 19 | 26 | 16500 | . 3 2 | 164 | audi | gas | std | four | sedan | fwd | front | 99.8 | ... | 109 | mpfi | 3.19 | 3.40 | 10.0 | 102 | 5500 | 24 | 30 | 13950 | . 4 2 | 164 | audi | gas | std | four | sedan | 4wd | front | 99.4 | ... | 136 | mpfi | 3.19 | 3.40 | 8.0 | 115 | 5500 | 18 | 22 | 17450 | . 5 rows × 26 columns . If we compare the data type of each column to its contents, several opportunities for data cleaning can be seen. For example, the &quot;normalized-losses&quot; feature is listed as an object-type column because it contains both strings and numbers. However, the strings in the column are question marks (?). Rather than being categories, these may be placeholders for missing data. This problem applies to several other columns, not just this one. . Furthermore, in some columns like &quot;num-of-doors&quot;, numbers are written as words. For example, 2 is written as &quot;two&quot;. Since the numbers are in string format, these cannot be used in the K Nearest Neighbors model. . Thus, in summary, the following cleaning steps have to be performed: . Replace question mark strings (&quot;?&quot;) with null values (NaN). These are the proper way to indicate missing values. | Convert several object columns, like &quot;normalized-losses&quot;, into numeric columns. | Replace numbers written as words with their proper numeric equivalents. For example, replace &quot;four&quot; with 4. | . These were performed in the following code cell. . # Clean the data. # Replace ? with NaN since these are placeholders. cars_df = cars_df.replace(&quot;?&quot;, np.nan) # Change this object column to float type. obj_to_numeric = [ &quot;normalized-losses&quot;, &quot;bore&quot;, &quot;stroke&quot;, &quot;horsepower&quot;, &quot;peak-rpm&quot;, &quot;price&quot;, ] for col in obj_to_numeric: cars_df[col] = pd.to_numeric(cars_df[col], errors = &quot;coerce&quot;) # Replace strings with numeric equivalents. cars_df[&quot;num-of-doors&quot;] = cars_df[&quot;num-of-doors&quot;].replace( { &quot;four&quot;: 4.0, &quot;two&quot;: 2.0, } ) cars_df[&quot;num-of-cylinders&quot;] = cars_df[&quot;num-of-cylinders&quot;].replace( { &quot;four&quot;: 4, &quot;six&quot;: 6, &quot;five&quot;: 5, &quot;eight&quot;: 8, &quot;two&quot;: 2, &quot;three&quot;: 3, &quot;twelve&quot;: 12, } ) cars_df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 205 entries, 0 to 204 Data columns (total 26 columns): # Column Non-Null Count Dtype -- -- 0 symboling 205 non-null int64 1 normalized-losses 164 non-null float64 2 make 205 non-null object 3 fuel-type 205 non-null object 4 aspiration 205 non-null object 5 num-of-doors 203 non-null float64 6 body-style 205 non-null object 7 drive-wheels 205 non-null object 8 engine-location 205 non-null object 9 wheel-base 205 non-null float64 10 length 205 non-null float64 11 width 205 non-null float64 12 height 205 non-null float64 13 curb-weight 205 non-null int64 14 engine-type 205 non-null object 15 num-of-cylinders 205 non-null int64 16 engine-size 205 non-null int64 17 fuel-system 205 non-null object 18 bore 201 non-null float64 19 stroke 201 non-null float64 20 compression-ratio 205 non-null float64 21 horsepower 203 non-null float64 22 peak-rpm 203 non-null float64 23 city-mpg 205 non-null int64 24 highway-mpg 205 non-null int64 25 price 201 non-null float64 dtypes: float64(12), int64(6), object(8) memory usage: 41.8+ KB . The new summary of columns is shown above. Several columns which were once &quot;object&quot; columns are now numeric. Also, since we replaced &quot;?&quot; placeholders with null values, we can now see that some columns have missing values. . null_percs = ( cars_df .isnull() .sum() .divide(cars_df.shape[0]) .multiply(100) ) null_percs.loc[null_percs &gt; 0] . . normalized-losses 20.00000 num-of-doors 0.97561 bore 1.95122 stroke 1.95122 horsepower 0.97561 peak-rpm 0.97561 price 1.95122 dtype: float64 . The table above shows the percentage of missing values in each column that has them. In particular, &quot;normalized-losses&quot; has missing values in 20% of the observations. Thus, we will have to drop this column from the dataset. This is better than the alternative, which is to delete all rows where &quot;normalized-losses&quot; is missing. . As for the other 6 columns, we will use listwise deletion. This means that we will drop all rows with missing values in any of those columns. . cars_df = ( cars_df .drop(&quot;normalized-losses&quot;, axis = 1) .dropna( subset = [ &quot;num-of-doors&quot;, &quot;bore&quot;, &quot;stroke&quot;, &quot;horsepower&quot;, &quot;peak-rpm&quot;, &quot;price&quot;, ] ) ) num_null = cars_df.isnull().sum().sum() print(f&quot;Total number of missing values: {num_null}&quot;) print(f&quot;New shape of dataset: {cars_df.shape}&quot;) . . Total number of missing values: 0 New shape of dataset: (193, 25) . Now, there are no more missing values in the dataset. There are 193 rows and 25 columns left. . The K Nearest Neighbors Algorithm . Next, I will discuss the theory behind the KNN algorithm, then implement it on the dataset. . First, let us discuss basic terminology. For your reference, below is a small part of the dataset: . cars_df.loc[:5, [&quot;make&quot;, &quot;fuel-type&quot;, &quot;num-of-doors&quot;, &quot;body-style&quot;, &quot;price&quot;]] . . make fuel-type num-of-doors body-style price . 0 alfa-romero | gas | 2.0 | convertible | 13495.0 | . 1 alfa-romero | gas | 2.0 | convertible | 16500.0 | . 2 alfa-romero | gas | 2.0 | hatchback | 16500.0 | . 3 audi | gas | 4.0 | sedan | 13950.0 | . 4 audi | gas | 4.0 | sedan | 17450.0 | . 5 audi | gas | 2.0 | sedan | 15250.0 | . Each row of data is called an observation; in this case, each observation is a car. . On the other hand, each column is either a feature or a target. The target is the variable that we try to predict, and the features are information used to make the prediction. In the case of this project, the features may include the size of the car, the number of doors, etc. The target is the price of the car. . The set of cars whose prices we will predict is called the testing set. On the other hand, the training set is the set of cars used to train the model to make predictions. Put more simply, in order to predict the price of a car in the testing set, we must compare it to the cars in the training set. . In order to compare cars, KNN uses the Euclidean distance as a similarity metric between two observations. A low distance close to 0 means that the observations are very similar to each other. The following formula is used: . $d = sqrt{ sum_{i=1}^n (q_i - p_i)^2}$ . $d$ is the Euclidean distance. | $n$ is the number of features. | $q$ and $p$ each refer to a different observation in the data. In this case, each is a different car. | $q_i$ is the value of feature $i$ for observation $q$. For example, if feature $1$ is the number of doors, $q_1$ is the number of doors on car $q$. | The differences between the two observations&#39; features are squared, then summed up. Finally, the square root of the sum gives the Euclidean distance. | . Given that we want to predict the price of a car $q$, KNN computes the Euclidean distance of $q$ from every single car in the training set. The cars most similar to $q$ are its &quot;nearest neighbors.&quot; . We then choose a number $k$, which will determine how many of the nearest neighbors will be selected. For example, if $k = 5$, we select the five most similar cars. Then, we take the mean price of these five cars, and we predict that this is the price of car $q$. . Since we make a prediction based on an observation&#39;s $k$ nearest neighbors, the algorithm is called K Nearest Neighbors. Note that what I have described is an example of a KNN regression model, as it predicts a numeric target. There are still several other forms of KNN. Some use a different similarity metric like Manhattan distance, and some perform classification, which means that they predict a categorical target (Miller, 2019). . Techniques for Implementation . Unlike with my previous post on the Naive Bayes Algorithm, I will not be programming this algorithm manually. Instead, I will use the scikit-learn workflow, which involves pre-packaged machine learning functions. . In this part, I will individually discuss certain important techniques used in the machine learning workflow. In the next part, I will combine these techniques in order to obtain the optimal KNN model. . Standardization . The first important technique is standardization. So that each feature will contribute equally to the Euclidean distance, we will standardize each numeric feature. In other words, each value will be converted into a z-score so that the mean of each feature is 0 and its standard deviation is 1. The following equation is used: . $z = frac{x - bar{x}}{s}$ . $z$ is the z-score. | $x$ is a value in a feature. | $ bar{x}$ is the mean of the feature. | $s$ is the sample standard deviation. | . all_feature_cols = [col for col in cars_df.columns if col != &quot;price&quot;] # Series of feature:data type fdt = cars_df[all_feature_cols].dtypes # Identify numeric features all_numeric_features = fdt.index[fdt != &quot;object&quot;] # Standardize cars_df[all_numeric_features] = cars_df[all_numeric_features].apply(zscore, axis = 0, ddof = 1) cars_df[all_numeric_features].head() . . symboling num-of-doors wheel-base length width height curb-weight num-of-cylinders engine-size bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg . 0 1.782215 | -1.172839 | -1.678015 | -0.442872 | -0.839080 | -2.117092 | -0.025646 | -0.410180 | 0.045098 | 0.511697 | -1.803495 | -0.287525 | 0.198054 | -0.212806 | -0.677292 | -0.555613 | . 1 1.782215 | -1.172839 | -1.678015 | -0.442872 | -0.839080 | -2.117092 | -0.025646 | -0.410180 | 0.045098 | 0.511697 | -1.803495 | -0.287525 | 0.198054 | -0.212806 | -0.677292 | -0.555613 | . 2 0.163544 | -1.172839 | -0.719041 | -0.250543 | -0.184200 | -0.613816 | 0.496473 | 1.544506 | 0.574066 | -2.388614 | 0.701095 | -0.287525 | 1.330822 | -0.212806 | -0.990387 | -0.702307 | . 3 0.972880 | 0.848214 | 0.142410 | 0.182198 | 0.143240 | 0.179580 | -0.426254 | -0.410180 | -0.459826 | -0.516262 | 0.479169 | -0.036110 | -0.039037 | 0.853987 | -0.207649 | -0.115531 | . 4 0.972880 | 0.848214 | 0.077395 | 0.182198 | 0.236794 | 0.179580 | 0.498371 | 0.567163 | 0.189362 | -0.516262 | 0.479169 | -0.538940 | 0.303427 | 0.853987 | -1.146935 | -1.289083 | . The table above shows the first 5 rows of all of the numeric features. Notice that each feature now contains positive and negative values close to 0 because it was standardized. . Feature Selection . The second technique is feature selection. We must choose features which we think are most relevant to a car&#39;s price. We can only select numeric features since categorical ones cannot be used to calculate Euclidean distance. Thus, we must select from the following features: . all_numeric_features.to_list() . . [&#39;symboling&#39;, &#39;num-of-doors&#39;, &#39;wheel-base&#39;, &#39;length&#39;, &#39;width&#39;, &#39;height&#39;, &#39;curb-weight&#39;, &#39;num-of-cylinders&#39;, &#39;engine-size&#39;, &#39;bore&#39;, &#39;stroke&#39;, &#39;compression-ratio&#39;, &#39;horsepower&#39;, &#39;peak-rpm&#39;, &#39;city-mpg&#39;, &#39;highway-mpg&#39;] . All of these features are physical characteristics of a car, except for &quot;symboling&quot;. According to the dataset documentation by Schlimmer (2019), this feature is an &quot;insurance risk rating.&quot; It elaborates: . Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process &quot;symboling&quot;. A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe. . Given that this feature is systematically associated with the price of a car, it may be relevant to our model. Thus, we will consider it along with the other numeric features. . In order to determine which combination of features is the best, we will use univariate feature selection. &quot;Univariate&quot; refers to the use of a single variable. We will perform a statistical test between each feature and the target. Then, we will select the features with the highest scores from the statistical test (scikit-learn developers, 2021). . In our case, we have a regression problem, since we want to predict a continuous variable, car price. Thus, we will use the F-statistic as our score function. According to Frost (2017), the F-statistic indicates the &quot;overall significance&quot; of a linear regression model. In univariate feature selection, we would do the following steps: . For each feature: Perform linear regression where the independent variable is the feature and the dependent variable is the target (in this case, price). | Obtain the F-statistic. | . | Compile a list with the F-statistic of each feature. | Identify the features with the highest F-statistics. | . This can be implemented automatically using the scikit-learn&#39;s SelectKBest class. It is called SelectKBest because we can set a parameter k which tells how many features to select. For example, if k = 3, the top three features with the highest F-statistic are selected. This is done below: . skb = SelectKBest( score_func = f_regression, k = 3, ) X = cars_df[all_numeric_features] y = cars_df[&quot;price&quot;] X_new = skb.fit_transform(X, y) best_features = list(skb.get_feature_names_out()) print(&quot;Top 3 features:&quot;, best_features) . . Top 3 features: [&#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;horsepower&#39;] . The results show that curb weight, engine size, and horsepower are the highest-scoring features. However, we will not select these yet for the final model, since other steps still must be discussed. . Train-Test Split with Stratification . Train-test split is the third important technique. . Before model training, the dataset has to be split into training and testing sets. We will use 80% of the data in the training set and 20% in the testing set. As the names suggest, the training set is used to train the model or help it learn how to predict car prices. Then, we make predictions on the cars on the testing set to see whether the predictions are accurate. . Before we split the data, though, we have to ensure that the frequency distribution of the target is similar between the training and testing sets. Below is a histogram of the frequency distribution of car price across the entire dataset: . sns.histplot(cars_df[&quot;price&quot;], bins = 100) plt.title(&quot;Frequency Distribution of Car Price&quot;) plt.xlabel(&quot;Price (USD)&quot;) plt.ylabel(&quot;Number of Cars&quot;) plt.show() . . The graph shows a right-skewed distribution, which means that most of the car prices are low and there are outliers with high prices. When we split the data into training and testing sets, we want each set to have a similar distribution to this. . De Cock (2011) provides a helpful suggestion on how to do this. The article says, &quot;Simply order the original data set by a variable of interest (such as sale price) and select every kth observation to achieve the desired sample size (k=2 for a 50/50 split or k=4 for a 75/25 split).&quot; . In our case, we want an 80/20 split. One-fifth of the data will go to the testing set, so we can use k = 5. We will thus order the observations by price, then assign every 5th observation to the testing set. All other observations will go to the training set. . In the code below, I have written a custom function stratify_continuous that uses this technique. I then performed a train-test split after stratification. X_train and y_train refer to the features and target in the training set, respectively. X_test and y_test are from the testing set. . def stratify_continuous(n_folds, y): &quot;&quot;&quot;Stratify a dataset on a continuous target.&quot;&quot;&quot; if n_folds &lt; 2 or n_folds &gt; 10: raise ValueError(&quot;Please select a number of folds from 2 to 10.&quot;) fold_nums = list(range(n_folds)) # DataFrame where &quot;index&quot; column contains the original indices df = pd.DataFrame( y # Shuffle before ranking so that cars with the same price are ordered randomly. .sample(frac = 1, random_state = 1, ignore_index = False) ) # This column gives a rank to each value in y. 0 is the rank of the lowest value. # Ties are broken according to order of appearance. df[&quot;rank&quot;] = df[y.name].rank(method = &quot;first&quot;) - 1 df[&quot;fold&quot;] = 0 for f in fold_nums[1:]: # start at f, then increment by n_folds indices = list(range(f, df.shape[0], n_folds)) df.loc[df[&quot;rank&quot;].isin(indices), &quot;fold&quot;] = f # Revert df to original order of indices df = df.reindex(index = y.index) # A series that indicates the fold number of each observation according to its original position in y fold_series = df[&quot;fold&quot;].copy() return fold_series folds = stratify_continuous( n_folds = 5, y = cars_df[&quot;price&quot;], ) def split_folds(X, y, fold_series, test_fold): &quot;&quot;&quot;Take a dataset whose observations have been grouped into folds, then perform a train-test split.&quot;&quot;&quot; if fold_series.dtype != &quot;int64&quot;: raise AttributeError(&quot;The fold list does not purely contain integers.&quot;) test_mask = (fold_series == test_fold) X_train = X.loc[~test_mask].copy() y_train = y.loc[~test_mask].copy() X_test = X.loc[test_mask].copy() y_test = y.loc[test_mask].copy() return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = split_folds( X = cars_df[all_numeric_features], y = cars_df[&quot;price&quot;], fold_series = folds, test_fold = 4, ) # Summary statistics for target columns. target_df = pd.concat( [y_train, y_test], axis = 1, join = &quot;outer&quot;, ) target_df.columns = [&quot;y_train price&quot;, &quot;y_test price&quot;] target_df.describe() . . y_train price y_test price . count 155.000000 | 38.000000 | . mean 13308.658065 | 13188.631579 | . std 8197.063090 | 7737.592975 | . min 5118.000000 | 5389.000000 | . 25% 7713.500000 | 7805.750000 | . 50% 10245.000000 | 10295.000000 | . 75% 16530.500000 | 16385.750000 | . max 45400.000000 | 37028.000000 | . This table shows summary statistics for the price columns of the two sets. The sets have similar means at around USD 13,200, and they also have similar medians at around USD 10,200. . Let us compare the price distributions using KDE plots: . sns.kdeplot(y_train, label = &quot;Training set&quot;) sns.kdeplot(y_test, label = &quot;Testing set&quot;) plt.title(&quot;Comparison of Car Prices Between Sets&quot;) plt.xlabel(&quot;Price (USD)&quot;) plt.ylabel(&quot;Probability Density&quot;) plt.legend() plt.show() . . The KDE plots both seem to follow the same shape and have the same center. This shows that the training and testing sets have roughly the same distribution of car prices. Thus, these were stratified correctly. . Hyperparameter Optimization . The fourth technique is hyperparameter optimization. This involves training the KNN model using different hyperparameter values to see which one performs the best. . A hyperparameter is a value that influences the behavior of a model and has no relation to the data. In the case of KNN, one important hyperparameter is the $k$ value, or the number of neighbors used to make a prediction. If $k = 5$, we take the mean price of the top five most similar cars and call this our prediction. However, if $k = 10$, we take the top ten cars, so the mean price may be different. . We can optimize $k$ in this way: . Decide values of $k$ to test. | For each $k$ value, fit and evaluate a KNN model. | Identify the best-performing model and use its $k$ value in the final model. | . In order to evaluate a model, we need an evaluation metric. In our case, we will use the Root Mean Squared Error or RMSE. This is calculated with the following equation: . $RMSE = sqrt{ frac{1}{n} sum_{i=1}^n ( text{actual}_i - text{predicted}_i)^2}$ . $n$ is the sample size. | $ text{actual}$ is the actual target value, or in this case, the actual price of a car. | $ text{predicted}$ is the predicted target value. | . RMSE can be interpreted as the average error of a regression model. For example, if $RMSE = 1000$, this means that the model&#39;s predicted car prices are USD 1000 away from the actual car prices, on average. . Below is an example of hyperparameter optimization using RMSE. All of the numeric features were used for this example. . k_values = [1, 3, 5] k_rmse = pd.Series(dtype = &quot;float64&quot;) for k in k_values: knn = KNeighborsRegressor( n_neighbors = k, algorithm = &quot;auto&quot;, ) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) k_rmse.loc[k] = rmse print(&quot;k value and RMSE&quot;) k_rmse . . k value and RMSE . 1 4086.029491 3 3146.025226 5 3251.392477 dtype: float64 . The table above shows that RMSE was lowest for $k = 3$. The RMSE was about USD 3146, which means that on average, the predicted prices are USD 3146 away from the actual prices. . K-Fold Cross-Validation . The last technique that will be discussed is K-Fold Cross-Validation. Earlier, we split the data into one training set and one testing set. The K-Fold Cross-Validation allows us to obtain a more holistic view of model performance by rotating the observations used in the two sets. In the words of Brownlee (2018), it estimates &quot;how the model is expected to perform in general when used to make predictions on data not used during the training of the model.&quot; . Here, $k$ has a different meaning. It determines the number of splits to make in a dataset. For example, if $k = 5$, the dataset will be split into 5 folds, each set containing 20% of the total data. . In summary, the following steps are performed: . Split the data into 5 folds: A, B, C, D, E. | Use fold A as the testing set and use the others as the training set. | Fit and evaluate a KNN model, thus obtaining RMSE. | Repeat the above process for a total of 5 times, so that each fold is used as a testing set once. | Compile a list of the five RMSE values obtained. | Compute the mean RMSE value. This is the final metric of model performance. | . K-Fold Cross-Validation can be implemented using scikit-learn&#39;s KFold and cross_val_score . An example of 5-fold cross-validation is shown below. . knn = KNeighborsRegressor( n_neighbors = 5, algorithm = &quot;auto&quot;, ) kf = KFold(5, shuffle = True, random_state = 1) mses = cross_val_score( estimator = knn, X = cars_df[all_numeric_features], y = cars_df[&quot;price&quot;], scoring = &quot;neg_mean_squared_error&quot;, cv = kf, ) mses = pd.Series(mses) rmses = mses.abs().pow(1/2) mean_rmse = rmses.mean() sd_rmse = rmses.std(ddof = 1) print(f&quot;&quot;&quot;Regular 5-fold cross-validation Mean RMSE: {mean_rmse:.2f} Standard Deviation RMSE: {sd_rmse:.2f} RMSE Values: {rmses.to_list()}&quot;&quot;&quot;) . . Regular 5-fold cross-validation Mean RMSE: 3722.28 Standard Deviation RMSE: 565.62 RMSE Values: [3407.8275635020186, 3902.1144860913682, 3009.7340988268425, 4521.314079941105, 3770.3892479494248] . The mean RMSE above presents a better picture of the model&#39;s performance because it takes into account different possible combinations of training and testing sets. . Note, however, that the standard deviation of the RMSE was around 566. This means that the RMSE values varied by several hundreds of dollars from model to model during the cross-validation. In simpler terms, the model performance was inconsistent. It performed much better when trained on some folds than when it was trained on other folds. . Thus, we can take k-fold cross-validation a step further by stratifying the folds so that they will have similar price distributions. This will ensure that each fold is representative of the full sample. Thus, I have written a custom function in the code cell below to do this. . def stratified_kfcv(X, y, fold_series, regression_model): &quot;&quot;&quot;Conduct k-fold cross-validation on a stratified dataset.&quot;&quot;&quot; fold_nums = fold_series.unique() mse_lst = [] for f in fold_nums: X_train, X_test, y_train, y_test = split_folds( X = X, y = y, test_fold = f, fold_series = fold_series, ) regression_model.fit(X_train, y_train) y_pred = regression_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) mse_lst.append(mse) return mse_lst knn = KNeighborsRegressor( n_neighbors = 5, algorithm = &quot;auto&quot;, ) mse_lst = stratified_kfcv( X = cars_df[all_numeric_features], y = cars_df[&quot;price&quot;], fold_series = folds, regression_model = knn, ) mse_series = pd.Series(mse_lst) rmse_series = mse_series.pow(1/2) mean_rmse = rmse_series.mean() sd_rmse = rmse_series.std(ddof = 1) print(f&quot;&quot;&quot;Stratified 5-fold cross-validation Mean RMSE: {mean_rmse:.2f} Standard Deviation RMSE: {sd_rmse:.2f} RMSE Values: {rmse_series.to_list()}&quot;&quot;&quot;) . . Stratified 5-fold cross-validation Mean RMSE: 3369.44 Standard Deviation RMSE: 387.33 RMSE Values: [3193.0727214096655, 2883.515369146238, 3844.6421242541865, 3674.5947449327227, 3251.39247707809] . The mean RMSE from stratified CV was USD 3369. This is about USD 400 lower than the result of the regular CV, USD 3722. . Furthermore, the SD RMSE is equal to 387, which is lower than the previous value of 566. Therefore, the five models trained during cross-validation performed more similarly to each other. . Thus, we can see that stratifying observations before k-fold cross-validation can be more effective at approximating the true performance of the model compared to regular k-fold cross-validation. . Combining Techniques . In this part, we will combine all of the discussed techniques to optimize the KNN model. . The steps are as follows: . Use the standardized features that were calculated earlier. | For each number n_features from 1 to 10: Perform univariate feature selection using the F-statistic. | Identify the best n_features features. | For each number k from 1 to 20: Evaluate the model using stratified 5-fold cross-validation. | For each fold, train a k nearest neighbors model using the best features. | Obtain the mean RMSE value. | . | . | Compile a list of all mean RMSE values obtained. | Identify the model with the lowest mean RMSE. This is the final model. | . This is implemented in the code below. . n_feature_list = list(range(1, 11)) result_lst = [] for n_features in n_feature_list: # Univariate feature selection skb = SelectKBest( score_func = f_regression, k = n_features, ) X = cars_df[all_numeric_features] y = cars_df[&quot;price&quot;] X_new = skb.fit_transform(X, y) # List of &quot;best&quot; features best_features = list(skb.get_feature_names_out()) k_values = list(range(1, 21)) for k in k_values: # stratified 5-fold cross validation knn = KNeighborsRegressor( # Use a different k value each time n_neighbors = k, algorithm = &quot;auto&quot;, ) mse_lst = stratified_kfcv( X = cars_df[best_features], y = cars_df[&quot;price&quot;], fold_series = folds, regression_model = knn, ) mse_series = pd.Series(mse_lst) rmse_series = mse_series.pow(1/2) mean_rmse = rmse_series.mean() sd_rmse = rmse_series.std(ddof = 1) new_row = (n_features, best_features, k, mean_rmse, sd_rmse) result_lst.append(new_row) result_df = pd.DataFrame(result_lst) result_df.columns = [&quot;Number of Features&quot;, &quot;Best Features&quot;, &quot;k Neighbors&quot;, &quot;Mean RMSE&quot;, &quot;SD RMSE&quot;] result_df = ( result_df .sort_values([&quot;Mean RMSE&quot;, &quot;SD RMSE&quot;], ascending = True) .reset_index(drop = True) ) . . Before we discuss the top-performing models, let us look at the general trends in the results using some graphs. . sns.lineplot( data = result_df, x = &quot;k Neighbors&quot;, y = &quot;Mean RMSE&quot;, hue = &quot;Number of Features&quot;, ) plt.title(&quot;Mean RMSE against k Neighbors&quot;) plt.show() . . The graph above shows that in general, no matter the number of features, the mean RMSE increased as the number of neighbors (k) increased. Therefore, it is best to have a low k value so that the model makes predictions only using a few cars that are most similar to the car being tested. . Next, let us look at a graph with the same variables, except that the number of features is now on the x-axis instead of k. . sns.lineplot( data = result_df, x = &quot;Number of Features&quot;, y = &quot;Mean RMSE&quot;, hue = &quot;k Neighbors&quot;, ) plt.title(&quot;Mean RMSE against Number of Features&quot;) plt.show() . . We can see that for models with a high k value (represented by the darker lines), the mean RMSE increased slightly as the number of features increased. . However, for models with a low k value (represented by the lighter pink lines), the mean RMSE stayed the same or even decreased when the number of features increased. . Therefore, the best model would be one with a low k value and a medium-to-high number of features. . In order to determine this more precisely, let us look at the top 10 models with the lowest RMSE. . result_df.head(10) . . Number of Features Best Features k Neighbors Mean RMSE SD RMSE . 0 8 | [length, width, curb-weight, num-of-cylinders,... | 1 | 2468.363493 | 354.699226 | . 1 4 | [width, curb-weight, engine-size, horsepower] | 1 | 2663.935533 | 809.240758 | . 2 4 | [width, curb-weight, engine-size, horsepower] | 2 | 2740.846793 | 541.902963 | . 3 8 | [length, width, curb-weight, num-of-cylinders,... | 2 | 2751.669830 | 693.467197 | . 4 8 | [length, width, curb-weight, num-of-cylinders,... | 3 | 2755.824129 | 383.365351 | . 5 3 | [curb-weight, engine-size, horsepower] | 2 | 2767.545024 | 543.282005 | . 6 5 | [width, curb-weight, num-of-cylinders, engine-... | 3 | 2784.838645 | 450.626647 | . 7 4 | [width, curb-weight, engine-size, horsepower] | 3 | 2798.859095 | 477.298250 | . 8 5 | [width, curb-weight, num-of-cylinders, engine-... | 2 | 2804.311055 | 473.482127 | . 9 7 | [width, curb-weight, num-of-cylinders, engine-... | 1 | 2806.166008 | 696.001235 | . The table above shows the ten models with the lowest mean RMSE. Interestingly, the best-performing model had 8 features and a k-value of 1. . Its RMSE was 2468, so on average, the predicted prices were USD 2468 off from the actual prices. This is decent considering that the car prices mostly fall between USD 5000 and USD 20000, though it could be better. . The SD RMSE is around 355. This means that the RMSE values usually varied by 355 from the mean. This is relatively low compared to the SD RMSE values of the other best-performing models, which range from 350 to over 800. Therefore, the model performance was consistent. . However, the following concerns are worth noting: . The number of features is somewhat large, and this may be a problem because it can cause overfitting. This means that the model may be too sensitive to small but meaningless variations in the training data. It may be unable to recognize general trends properly. However, reducing the number of features may increase the mean RMSE. | The k-value is concerning because only one neighbor is considered when predicting a car&#39;s price. I would prefer to have $k &gt; 1$ so that multiple neighbors are taken into consideration. | . Personally, I am fine with selecting this model as the final one to use, simply because its mean RMSE is a few hundred dollars lower than that of the other good models. In a real-world scenario, after I implement this model, I would see if it continued to perform well on new data, and then reduce its number of features or increase its k-value if needed. . Summary . In this project, we cleaned a dataset about car features and prices, discussed the logic behind the K Nearest Neighbors algorithm for regression, explained techniques used in the machine learning workflow, then applied these techniques to determine the optimal model for predicting car prices. . Thanks for reading! . Bibliography . Data Source . Schlimmer, J. C. (1987, May 19). UCI Machine Learning Repository: Automobile Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/automobile . Information Sources . Brownlee, J. (2018, May 22). A Gentle Introduction to k-fold Cross-Validation. Machine Learning Mastery. https://machinelearningmastery.com/k-fold-cross-validation/ . Dataquest. (n.d.). Predicting Car Prices: Machine Learning Project. Dataquest. Retrieved December 21, 2021, from https://www.dataquest.io/c/36/m/155/guided-project%3A-predicting-car-prices . De Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education, 19(3), null. https://doi.org/10.1080/10691898.2011.11889627 . Frost, J. (2017, April 4). How to Interpret the F-test of Overall Significance in Regression Analysis. Statistics By Jim. http://statisticsbyjim.com/regression/interpret-f-test-overall-significance-regression/ . Miller, M. (2019, October 18). The Basics: KNN for classification and regression. Medium. https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955 . scikit-learn developers. (2021). 1.13.2. Univariate Feature Selection. Scikit-Learn. https://scikit-learn/stable/modules/feature_selection.html . Image Source . Smith, C. (2018, January 20). Charger vs Challenger: All-American Muscle Car Comparison. WheelScene. https://wheelscene.com/charger-vs-challenger/ .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/scipy/sklearn/2021/12/21/Predicting-Car-Prices-K-Nearest-Neighbors.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/scipy/sklearn/2021/12/21/Predicting-Car-Prices-K-Nearest-Neighbors.html",
            "date": " • Dec 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "agriHanda: an Agricultural Disaster Risk Web App",
            "content": ". The &#39;Map of Butuan City&#39; feature of agriHanda. . In June 2021, I read that there was an upcoming local competition in data science and analytics. This was the Project SPARTA Open Data Challenge, which was organized by the Development Academy of the Philippines and DOST-PCIEERD. The Sparta Portal provided open data about Butuan City, which is the “commercial, industrial and administrative center” of the Caraga region (“Butuan City”, n.d.). Participants could then use this data to create cleaned datasets, datablogs, data journalism pieces, research papers, dashboards, predictive models, software applications, or visual storytelling pieces (“Approach to innovations”, n.d.). . Thus, I formed a team with three of my schoolmates (Fiona Jao, Lorenzo Layug, and Yuri Dolorfino) and we entered the competition as the “Datos Puti” team. We cleaned open data about agricultural disaster risk together, then I programmed agriHanda, an app which would visualize this data. . In the end, we were awarded as “Second Placer” and “Best in Ingenious Solution.” I was proud of this because this was the first data science competition that I had ever joined. Furthermore, despite being a senior high school student at the time, I was able to perform well against the other eleven competing teams, most of whom were from the college and professional levels. . In this post, I will talk about the web app, its strengths and points for improvement, and the data science skills that I honed along the way. . The Web App . First, I will give a brief introduction to agriHanda. The “agri” part of the name refers to agriculture, whereas the “Handa” part is a Tagalog or Bisaya word for “prepared.” Thus, the title encapsulates the goal of the app to help the LGU prepare for natural hazards which may pose a threat to the city’s agriculture industry. . Specifically, the app is a dashboard since it focuses on creating helpful visualizations. It uses data about the vulnerability of Butuan City’s crops, fisheries, and livestock viz-a-viz natural hazards such as drought, flooding, rain-induced landslide, sea level rise, and storm surge. . The app has three main features. The “Map of Butuan City” feature shows a choropleth map. This means that the user can select a variable about agricultural disaster risk, and the colors of the barangays change based on the value of this variable. For example, it can show which general areas have a high vulnerability score of crops against flooding. The “Barangay Data Summary” feature presents tables and histograms that summarize the disaster risk scores of a chosen barangay. Lastly, the “Graphing Tool” feature allows the user to create a custom chart using one or two disaster risk variables. . For more information, you can watch our short four-minute pitch presentation about the app.: agriHanda Video Pitch presentation . To try out the app yourself, visit this link: bit.ly/agriHanda . For more technical details about the project, visit the documentation through this link: agriHanda - Agricultural Disaster Risk App . The recognition ceremony can also be watched here: SPARTA Recognition Ceremony . Reflection . Now that the competition is over, it is necessary for me to reflect on agriHanda so that I can do even better next time. . First, I’d like to discuss the strengths of the project. One was that we thoroughly cleaned and combined the data so that it could be used in our web app and other software. Furthermore, because the variables were arranged in a hierarchy, new data can be added based on it. For example, the dataset only contains agriculture data as of now, but it could be expanded to other sectors such as infrastructure and population. It could also be expanded to include more natural hazards, such as volcanic eruption and earthquakes. . The main strength of the project, though, was that it had a variety of visualizations, which offered different perspectives on the data. For example, the choropleth map feature is useful for determining general areas or groups of adjacent barangays that share certain characteristics, such as high vulnerability or high adaptive capacity. The Barangay Data Summary is useful for focusing on a single barangay and determining which hazards pose the greatest threat against it. The Graphing Tool is versatile because it has various types of charts, so it can be used to answer specific questions about relationships between variables. . However, it would be difficult to use the choropleth map and the Graphing Tool if I did not provide the Help page for the variable selection system. This page explained the hierarchy of variables in the dataset and provided practice for how to use the variable selection system. Thus, even if this system was unusual, the app still managed to be user-friendly. . Lastly, the project also had detailed documentation. Apart from explaining how the web app works, it also gives instructions for how researchers can use the cleaned dataset in their own Python scripts. It also has a guide on how to start developing the web app locally, so that the LGU can revise the app instead of having me do it. . Despite all of these positive aspects, however, I know that there was still room for improvement. The most glaring issue with the project is that it has a wide breadth but lacks depth. The web app provides many options regarding variables and graphs. However, I did not investigate a specific problem or research question. Thus, I did not make any interpretations of the graphs, and instead left this task to the app users. I did not perform any statistical tests or machine learning models, which could have then led to concrete suggestions for how the city could prepare its agriculture against disasters. That would have had a much more direct and positive impact on Butuan City. . I must keep these things in mind the next time that I join a similar competition. Rather than a simple dashboard, I think I should try to form a research paper or a predictive model, since these have more potential to create change in a community. . Data Science Skills . In this last part, I will discuss some of the data science skills that I honed in this project. . pandas MultiIndex . Recall that during the data cleaning phase, I had to organize the variables into a hierarchy. Thus, I had to learn how to use the pandas MultiIndex. . For context, pandas is a Python package for manipulating tables of data, which are called DataFrames. A regular Index object is used to store the row labels or column labels of a DataFrame. In my case, each of the original data files included columns such as “Exposure”, “Sensitivity”, etc. Each row represented a unique barangay. Each file only gave data about a specific element and hazard (for example, fisheries and storm surge). . Below is a simplified example of what one file would look like. . Barangay Adaptive Capacity Score Vulnerability Score . A | 2 | 1.2 | . B | 5 | 0.8 | . When I combined the files, I used MultiIndex to create a hierarchy which looked like this: . Sector Agriculture       …     . Element | Crops |   |   |   |   | Fisheries |   | . Hazard | Drought |   | Flood |   |   | Drought |   | . Disaster Risk Aspect | Adaptive Capacity | Overall Risk | Adaptive Capacity | Overall Risk |   | Adaptive Capacity | Overall Risk | . Detail | Adaptive Capacity Score | Vulnerability Score | Adaptive Capacity Score | Vulnerability Score |   | Adaptive Capacity Score | Vulnerability Score | . Barangay |   |   |   |   |   |   |   | . A | 2 | 1.2 | 5 | 1 |   | 4 | 1.5 | . B | 5 | 0.8 | 4 | 1.1 |   | 1 | 0.6 | . The first 5 rows of the table above are part of the hierarchy of labels. The levels are Sector, Element, Hazard, Disaster Risk Aspect, and Detail. With this hierarchy, the variables can be navigated easily even if there are many of them. . For more information on MultiIndex, visit the pandas documentation. . Streamlit web app framework . Another thing that I practiced in this project was the Streamlit package. This provides a framework for building simple web apps in Python. One would first write a script which uses the Streamlit API to represent text, widgets, charts, and other visual elements. . import streamlit as st # App title st.title(&#39;My App&#39;) # Let the user select a variable from a drop-down list. input_var = st.selectbox( &#39;Select a variable&#39;, options = [&#39;Exposure&#39;, &#39;Sensitivity&#39;, &#39;Adaptive Capacity&#39;, &#39;Vulnerability&#39;, &#39;Risk&#39;], ) # Make a chart using this variable. # chart = ... # Display the chart. st.altair_chart(chart) . Then, when the app is run, the script is run from top to bottom. When the user changes something, the entire script runs again in order to update what is shown. In the case of the script above, the app would show a title, then a drop-down list, then a chart. If the user changes their selection in the drop-down list, the chart changes as well. . With this simple framework, one can quickly create an interface through which other people can interact with a dashboard or even a predictive model, so it will most likely be useful for future projects. For more information, visit the Streamlit website. . Altair package for data visualization . Another important skill I used for this project was making charts using Altair. This package allows me to create charts using a simple grammar which minimizes the amount of code needed. This usually involves the following steps: . Make a Chart object and pass a DataFrame to it | Specify a mark type, which determines the shapes used to represent data. The options include bar, boxplot, line, area, point, etc. | Specify encodings. This means deciding which variables are used in the chart, and how so. | Specify properties. These may include the chart title, font size, text rotation, etc. | . You can experience a simplified version of these steps by using the Graphing Tool in agriHanda. . In terms of code, an example would look like this: . import altair as alt chart = ( alt.Chart(df) .mark_boxplot() # Make a boxplot .encode(y = input_variable, type = &#39;quantitative&#39;) # Use the input variable on the y-axis .properties(title = &#39;Boxplot&#39;) # Set a title .interactive() # Make the chart interactive ) . Apart from the basic steps, I also learned how to use bindings, selections, and conditions. These are advanced techniques which make charts more interactive. In my case, I used these to add a slider to the Map of Butuan City feature so that only barangays near a certain value would be highlighted. This allows the user to, for example, only highlight places with high exposure to a hazard. . . The slider is set to only highlight barangays with the highest vulnerability score. . For more information about Altair, visit their documentation. . Git and GitHub workflow . Another very important thing I learned was how to use basic git commands to manage my repository. . Originally, I created a blank repository, then dragged the files to GitHub and pressed “commit.” I committed files in this way every time that I made a change. I later learned, however, that this was not the most efficient way to do things. . At some point, I learned that I could clone my repository to my laptop. This means that the files would be downloaded to a folder in my laptop, which would serve as my workspace. After I edit something in my workspace, I can commit the changes to my local repository. Thus, a snapshot of my files is saved, and I can go back to old versions if I need to. I can then push the changes to my remote repository, which is usually on GitHub. Thus, the latest changes to the repository are stored online. . In the case of agriHanda, since the Streamlit app is deployed from the GitHub repository, I can easily change the app just by pushing to the repository. . Admittedly, I am not familiar with using git commands in the command line. I rely on GitHub Desktop and Visual Studio Code, which provide graphical user interfaces for git commands. However, I hope to learn how to use Git Bash eventually. . The next step I can take would be to study articles like this article by Fachat (2021), so that I can better understand how git works and how to write commands. . Writing a README and Documentation . Lastly, the agriHanda project was my first time writing a serious README and documentation for my repository. I mainly followed the GitHub guide for READMEs. I made sure to put only the most essential information in the README, including a brief description, a link tot he documentation, the sources of the open data used in the project, the names and roles of our team members, and a statement on the terms of use of the project. . Then, in the documentation, I wrote more thoroughly about the project. It included the background of the project, purpose and objectives, main access links, explanations of the cleaned dataset and the web app, the procedure from data cleaning to app development, a local development guide for editing the project on one’s local device, recommendations for how to improve the project, the credits to our team members, and a bibiography. You can read the agriHanda documentation here. . . That’s all for this post. Thanks for reading! . Bibliography . About READMEs. (2021). GitHub Docs. https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-readmes . Altair: Declarative Visualization in Python—Altair 4.2.0rc1 documentation. (2020). Altair. https://altair-viz.github.io/ . Development Academy of the Philippines. (n.d.-a). Approach to innovations and innovative techniques to food sufficiency using data and technology. Sparta Portal. Retrieved December 17, 2021, from https://sparta.dap.edu.ph/opendata/lgu/butuancity/challenges/butuancity-agriculture . Development Academy of the Philippines. (n.d.-b). Butuan City. Sparta Portal. Retrieved December 17, 2021, from https://sparta.dap.edu.ph/opendata/lgu/butuancity/details . Fachat, A. (2021, July 20). Learn the workings of Git, not just the commands. IBM Developer. https://developer.ibm.com/tutorials/d-learn-workings-git/ . Streamlit • The fastest way to build and share data apps. (2021). Streamlit. https://streamlit.io/ . The pandas development team. (2021). MultiIndex / advanced indexing—Pandas 1.3.5 documentation. Pandas. https://pandas.pydata.org/docs/user_guide/advanced.html .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/altair/streamlit/git/2021/12/17/agriHanda-Agricultural-Disaster-Risk-Web-App.html",
            "relUrl": "/python/pandas/altair/streamlit/git/2021/12/17/agriHanda-Agricultural-Disaster-Risk-Web-App.html",
            "date": " • Dec 17, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Naive Bayes Algorithm for Detecting Spam Messages",
            "content": "Unsplash | Hannes Johnson Introduction . The Multinomial Naive Bayes Algorithm is a machine learning algorithm based on the Bayes Theorem. It calculates the probability that an event $B$ occurred given that event $A$ occurred. Thus, it is usually used in classification problems. (Vadapalli, 2021) . In this project, we will use the algorithm to determine the probability that a message is spam given its contents. We will then use this probability to decide whether to treat new messages as spam or not. For example, if the probability of being spam is over 50%, then we may treat the message as spam. . Identifying spam is important in the Philippines because phishing campaigns went up by 200% after the pandemic began (Devanesan, 2020), and a telecommunications provider recently had to block around 71 million spam messages (Yap, 2021). Such messages may attempt to steal personal information, steal money from an account, or install malware (FTC, 2020). Thus, machine learning can be a very helpful tool in preventing such harm from occurring. . Though the algorithm can be easily implemented using existing functions such as those in the scikit-learn package, I will manually code the algorithm step-by-step in order to explain the mathematical intuition behind it. . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically the Guided Project: Building a Spam Filter with Naive Bayes The general project flow came from Dataquest. The mathematical explanations are also based on what I learned from Dataquest. . Packages . Below are the packages necessary for this project. . import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay . The Dataset . We will use the SMS Spam Collection Dataset by Almeida and Hidalgo in 2012. It can be downloaded from the UCI Machine Learning Repository. . sms_df = pd.read_csv( &quot;./private/Naive-Bayes-Files/SMSSpamCollection&quot;, # Tab-separated sep = &quot; t&quot;, header = None, names = [&quot;label&quot;, &quot;sms&quot;] ) sms_df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5572 entries, 0 to 5571 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 label 5572 non-null object 1 sms 5572 non-null object dtypes: object(2) memory usage: 87.2+ KB . The dataset has 2 columns and 5572 rows. . The label column contains &quot;ham&quot; if the message is legitimate, or &quot;spam&quot; if it is spam. | The sms column contains individual SMS messages. | . For example, below are the first 5 rows of the dataset. . sms_df.head() . . label sms . 0 ham | Go until jurong point, crazy.. Available only ... | . 1 ham | Ok lar... Joking wif u oni... | . 2 spam | Free entry in 2 a wkly comp to win FA Cup fina... | . 3 ham | U dun say so early hor... U c already then say... | . 4 ham | Nah I don&#39;t think he goes to usf, he lives aro... | . Training and Testing Sets . The messages will be split into two sets. The training set, comprising 80% of the total data, will be used to train the Naive Bayes Algorithm. The testing set, with 20% of the total data, will be used to test the model&#39;s accuracy. . First, however, let us calculate what percentage of the messages in the dataset are spam. . spam_perc = sms_df[&quot;label&quot;].eq(&quot;spam&quot;).sum() / sms_df.shape[0] * 100 print(f&quot;Percentage of spam messages: {spam_perc:.2f}%&quot;) . . Percentage of spam messages: 13.41% . Only 13% of the messages are spam. Therefore, spam and non-spam messages are not equally represented in this dataset, and this may be problematic. However, this is all the data we have, so the best we can do is to ensure that both the training and testing sets have around 13% of their messages as spam. . This is an example of proportionate stratified sampling (Thomas, 2020). We first separate the data into two strata (spam and non-spam). We then take 80% of the messages from each strata as the training set. The remaining 20% of each strata is set aside for the testing set. Thus, each of the two sets will contain around 13% spam. . This has been done with the code below. . # Note: I could have used `train_test_split` from sklearn, but I coded this manually for the sake of grasping the logic. split_lists = { &quot;training&quot;: [], &quot;testing&quot;: [], } # Stratify the dataset for label in &quot;spam&quot;, &quot;ham&quot;: stratum = sms_df.loc[sms_df[&quot;label&quot;] == label] train_part = stratum.sample( # Sample 80% of the data points frac = 0.8, random_state = 1, ) # The other 20% that were not sampled go to the testing set. test_part = stratum.loc[~stratum.index.isin(train_part.index)] split_lists[&quot;training&quot;].append(train_part) split_lists[&quot;testing&quot;].append(test_part) split_dfs = pd.Series(dtype = &quot;object&quot;) for key in split_lists: # Concatenate spam and non-spam parts into one DataFrame. set_df = pd.concat(split_lists[key]).reset_index() split_dfs[key] = set_df perc_spam = set_df.label.eq(&#39;spam&#39;).sum() / set_df.shape[0] * 100 print(f&quot;Number of rows in {key} set: {set_df.shape[0]}&quot;) print(f&quot;Percentage of {key} messages that are spam: {perc_spam:.2f}%&quot;) . . Number of rows in training set: 4458 Percentage of training messages that are spam: 13.41% Number of rows in testing set: 1114 Percentage of testing messages that are spam: 13.38% . We can see that the percentage of spam messages is roughly the same between the two sets. This will help the accuracy of the model later on. . Now, the two sets will be further split into X and y. y refers to the target, or the variable that we are trying to predict. In this case, we are trying to predict whether a message is spam or non-spam, so the &quot;label&quot; column is the target: . sms_df.label.head() . . 0 ham 1 ham 2 spam 3 ham 4 ham Name: label, dtype: object . On the other hand, X refers to the features, which are information used to predict the target. We only have one feature column as of now, which is the &quot;sms&quot; column. . sms_df.sms.head() . . 0 Go until jurong point, crazy.. Available only ... 1 Ok lar... Joking wif u oni... 2 Free entry in 2 a wkly comp to win FA Cup fina... 3 U dun say so early hor... U c already then say... 4 Nah I don&#39;t think he goes to usf, he lives aro... Name: sms, dtype: object . Thus, we end up with four final objects: . X_train: The messages in the training data. | X_test: The messages in the testing data. | y_train: The labels in the training data. These correspond to X_train. | y_test: The labels in the testing data. These correspond to X_test. | . # The four objects listed above. X_train = split_dfs.training[[&quot;sms&quot;]].copy() X_test = split_dfs.testing[[&quot;sms&quot;]].copy() y_train = split_dfs.training[&quot;label&quot;].copy() y_test = split_dfs.testing[&quot;label&quot;].copy() . . The Algorithm . Now, let&#39;s discuss the multinomial naive bayes algorithm. Conditional probability is necessary in order to understand it. For our use case, let $Spam$ be the event that a message is spam, and $Ham$ be the event for non-spam. . Note: The mathematical explanations below are not my own ideas. I learned these from the Dataquest course on Naive Bayes. . Main Formulas . We want to compare the probability that a given message is spam to the probability that it is ham. Thus, we use the following formulas: . $P(Spam|w_1, w_2, dots , w_n) propto P(Spam) cdot Pi_{i=1}^n P(w_i|Spam)$ . $P(Ham|w_1, w_2, dots , w_n) propto P(Ham) cdot Pi_{i=1}^n P(w_i|Ham)$ . Note: These formulas are not the same as the Bayes Theorem. To understand how these were derived from the Bayes Theorem, see the Appendix of this post. . These two formulas are identical except for the $Spam$ or $Ham$ event. Let us just look at the first equation to unpack it. . The probability of event $B$ given that event $A$ has happened can be represented as $P(B|A)$ (&quot;probability of B given A&quot;). Thus, the left side of the formula, $P(Spam|w_1, w_2, dots , w_n)$, represents the probability of spam given the contents of a message. Each variable $w_i$ represents one word in the message. For example, $w_1$ is the first word in the message, and so on. . In the middle, the &quot;directly proportional to&quot; sign ($ propto$) is used instead of the equals sign. The left and right sides are not equal, but one increases as the other increases. . At the right side, $P(Spam)$ simply refers to the probability that any message is spam. It can be calculated as the number of spam messages in the dataset over the total number of messages. . Finally, the formula ends with $ Pi_{i=1}^n P(w_i|Spam)$. The $P(w_i|Spam)$ part refers to the probability of a certain word occurring given that the message is known to be spam. We must calculate this probability for each word in the message. Then, because the uppercase pi ($ Pi$) refers to a product, we must multiply the word probabilities together. . Additive Smoothing and Vocabulary . In order to calculate $P(w_i|Spam)$, we need to use the following formula: . $P(w_i | Spam) = frac{N_{w_i | Spam} + alpha}{N_{Spam} + alpha cdot N_{Vocabulary}}$ . We use an almost identical equation for $P(w_i|Ham)$ as well: . $P(w_i | Ham) = frac{N_{w_i | Ham} + alpha}{N_{Ham} + alpha cdot N_{Vocabulary}}$ . Again, let us just unpack the first formula. $N_{w_i|Spam}$ refers to the number of times that the word appears in the dataset&#39;s spam messages. . $ alpha$ is the additive smoothing parameter. We will use $ alpha = 1$. This is added to the numerator to prevent it from becoming zero. If it does become zero, the entire product in the main formula will become zero. . $N_{Spam}$ refers to the total number of words in all of the spam messages. Duplicate words are not removed when this is calculated. . Lastly, $N_{Vocabulary}$ refers to the number of words in the vocabulary. This is the set of all unique words found in any of the messages, whether spam or non-spam. Duplicates are removed. . Implementation . Based on the theory behind the algorithm, I have written a set of steps to implement it. I will use these steps as the pseudocode for this project. . Determine the model parameters. These are the variables in the formulas shown earlier. Only the training data will be used for this. Find $P(Spam), P(Ham)$. Divide the number of spam messages by the total number of messages. | Do the same for ham messages. | . | Preprocess the messages to focus on individual words. Make all words lowercase. | Remove punctuation marks. | . | Form a vocabulary. Make a set of all the words in the messages, without duplicates. | $N_{Vocabulary}$ is the number of words in this set. | . | Find $N_{Spam}, N_{Ham}$. Count the number of times each word appears in each message. | Count the total number of words in spam messages. Do the same for ham messages. | . | Find $N_{w_i|Spam}, N_{w_i|Ham}$ for each word in the vocabulary. Sum up the word counts in spam messages to get $N_{w_i|Spam}$. | Do the same for ham messages to get $N_{w_i|Spam}$. | . | . | Write a predictive function. This takes a new message and predicts whether it is spam or not. Plug the values that we calculated previously into the equation. | Return $P(Spam|w_1, w_2, dots , w_n)$, $P(Ham|w_1, w_2, dots , w_n)$, and the prediction (&quot;spam&quot; or &quot;ham&quot;). | . | Evaluate the model using the testing data. Make predictions for all messages in the testing set. | Divide the number of correct predictions by the total number of predictions. This will result in the accuracy of the model. | . | Model Parameters . In the first step, we will calculate the parameters of the model, which include the following. . $P(Spam), P(Ham)$ | $N_{Vocabulary}$ | $N_{Spam}, N_{Ham}$ | $N_{w_i|Spam}, N_{w_i|Ham}$ | . We will calculate these values first so that we can plug them into the equation later on when we predict whether new messages are spam or non-spam. . $P_{Spam}, P_{Ham}$ . The probability of spam is equal to the number of spam messages over the total number of messages. The same goes for ham messages. . p_label = {} p_label[&quot;spam&quot;] = y_train.eq(&quot;spam&quot;).sum() / y_train.shape[0] p_label[&quot;ham&quot;] = 1 - p_label[&quot;spam&quot;] print(f&quot;P(Spam) = {p_label[&#39;spam&#39;] * 100:.2f}%&quot;) print(f&quot;P(Ham) = {p_label[&#39;ham&#39;] * 100:.2f}%&quot;) . . P(Spam) = 13.41% P(Ham) = 86.59% . Message Preprocessing . Below are the messages: . X_train.head() . . sms . 0 Marvel Mobile Play the official Ultimate Spide... | . 1 Thank you, winner notified by sms. Good Luck! ... | . 2 Free msg. Sorry, a service you ordered from 81... | . 3 Thanks for your ringtone order, ref number R83... | . 4 PRIVATE! Your 2003 Account Statement for shows... | . In order to get individual words, we make all words lowercase and remove punctuation marks and other non-word characters. We then turn each message into a list of its words. . def preprocess_messages(series): result = ( series .str.lower() # Delete all non-word characters. .str.replace(r&quot;[^a-z0-9 ]&quot;, &quot;&quot;, regex = True) .str.strip() .str.split() ) return result X_train = pd.DataFrame(preprocess_messages(X_train.sms)) X_train.head() . . sms . 0 [marvel, mobile, play, the, official, ultimate... | . 1 [thank, you, winner, notified, by, sms, good, ... | . 2 [free, msg, sorry, a, service, you, ordered, f... | . 3 [thanks, for, your, ringtone, order, ref, numb... | . 4 [private, your, 2003, account, statement, for,... | . Vocabulary . Using the preprocessed messages, we can form a set of all of the unique words that they contain. . vocab = set() for lst in X_train.sms: vocab.update(lst) # Use a Series to delete items that are blank or only contain whitespace. vocab_series = pd.Series(list(vocab)) vocab_series = vocab_series.loc[~vocab_series.str.match(&quot;^ s*$&quot;)] vocab = set(vocab_series) n_vocab = len(vocab) print(f&quot;Number of words in the vocabulary: {n_vocab} nFirst few items:&quot;) list(vocab)[:10] . . Number of words in the vocabulary: 8385 First few items: . [&#39;silent&#39;, &#39;receivea&#39;, &#39;dismay&#39;, &#39;noise&#39;, &#39;platt&#39;, &#39;li&#39;, &#39;woohoo&#39;, &#39;ucall&#39;, &#39;wondarfull&#39;, &#39;80082&#39;] . Above are the first 10 items in the vocabulary. In total, $N_{Vocabulary} = 8385$. . $N_{Spam}, N_{Ham}$ . Using the vocabulary, we can transform the messages to show the number of times that each word appears in each message. . def to_word_counts(series, vocab = vocab): vocab_lst = list(sorted(vocab)) word_counts = pd.DataFrame({ w: [0] * series.shape[0] for w in vocab_lst }) for index, word_lst in series.iteritems(): for w in word_lst: if w in vocab: word_counts.loc[index, w] += 1 return word_counts word_counts = to_word_counts(X_train.sms) word_counts.head() . . 0 008704050406 0089my 0121 01223585236 01223585334 0125698789 020603 0207 02070836089 ... zebra zed zeros zhong zindgi zoe zogtorius zoom zouk zyada . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 8385 columns . In the table above, each row represents a message. Each column represents a unique word in the vocabulary. The cells show the number of times that each word appeared in each message. . Now, we can calculate $N_{Spam}, N_{Ham}$: . def count_n(label, word_counts = word_counts): n_label = ( word_counts .loc[y_train == label, :] # Sum all of the numbers in the df. .sum() .sum() ) return n_label n_label = {} for label in [&quot;spam&quot;, &quot;ham&quot;]: n_label[label] = count_n(label) print(f&quot;Number of words in spam messages: {n_label[&#39;spam&#39;]}&quot;) print(f&quot;Number of words in ham messages: {n_label[&#39;ham&#39;]}&quot;) . . Number of words in spam messages: 14037 Number of words in ham messages: 53977 . The result is that $N_{Spam} = 14037$ and $N_{Ham} = 53977$. . $N_{w_i|Spam}, N_{w_i|Ham}$ . Finally, we can use the word counts to determine these two parameters. Recall that $N_{w_i|Spam}$ is the number of times that a certain word $w_i$ appeared in the spam messages. . full_train = pd.concat( [y_train, word_counts], axis = 1, ) n_word_given_label = full_train.pivot_table( values = vocab_lst, index = &quot;label&quot;, aggfunc = np.sum, ) n_word_given_label . . 0 008704050406 0089my 0121 01223585236 01223585334 0125698789 020603 0207 02070836089 ... zebra zed zeros zhong zindgi zoe zogtorius zoom zouk zyada . label . ham 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | . spam 3 | 1 | 1 | 1 | 1 | 1 | 0 | 4 | 2 | 1 | ... | 1 | 4 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | . 2 rows × 8385 columns . The table above can be used to access these parameters. For example, if we want to access $N_{w_i | Spam}$ for the word &quot;hello&quot;, we can look at the value where the &quot;spam&quot; row and &quot;hello&quot; column intersect. The value is the number of times that &quot;hello&quot; appeared in the spam messages. . n_hello_spam = n_word_given_label.at[&quot;spam&quot;, &quot;hello&quot;] print(f&quot;Number of times that &#39;hello&#39; appeared in spam messages: {n_hello_spam}&quot;) . . Number of times that &#39;hello&#39; appeared in spam messages: 3 . Predictive Function . Now that all of the parameters have been found, we can write a function that will take a new message and classify it as spam or non-spam. This function will use the formulas explained earlier. . def predict(word_lst, out = &quot;both&quot;, alpha = 1, vocab = vocab, p_label = p_label, n_label = n_label, n_word_given_label = n_word_given_label): &quot;&quot;&quot;Given the list of words in a message, predict whether it is spam or ham. word_lst: The preprocessed list of words in the message. out: &quot;both&quot; to output both probabilities and prediction in a tuple. &quot;pred&quot; to output only the prediction as a string.&quot;&quot;&quot; # Set up a Series to store results results = pd.Series(dtype = np.float64) for label in [&quot;spam&quot;, &quot;ham&quot;]: # Use P(Spam) or P(Ham) final = p_label[label] # Iterate through words in the message. for w in word_lst: # Only include a word if it is already in the vocabulary. if w in vocab: # Calculate P(w1, w2, ..., wn | Spam) using the formula. p_word_given_label = ( (n_word_given_label.at[label, w] + alpha) / (n_label[label] + alpha * n_vocab) ) # Multiply the result into the final value. final *= p_word_given_label results[label] = final # The prediction is the label with the higher probability in the Series. # If the probabilities are equal, the prediction is &quot;uncertain&quot; if results[&quot;spam&quot;] == results[&quot;ham&quot;]: prediction = &quot;uncertain&quot; else: prediction = results.idxmax() if out == &quot;both&quot;: return results, prediction elif out == &quot;pred&quot;: return prediction . Let us try using this function to predict whether a message is spam or ham. We will use this example: &quot;you won a prize claim it now by sending credit card details&quot;. . results, prediction = predict(&quot;you won a prize claim it now by sending credit card details&quot;.split()) print(&quot;Results:&quot;) for label, value in results.iteritems(): print(f&quot;P({label} | message) is proportional to {value}&quot;) print(f&quot;This message is predicted to be {prediction}.&quot;) . . Results: P(spam | message) is proportional to 2.3208952599406518e-35 P(ham | message) is proportional to 1.8781562825001382e-41 This message is predicted to be spam. . The algorithm determined that $P(Spam|w_1, w_2, dots , w_n) propto 2.32 cdot 10^{-35}$, whereas $P(Ham|w_1, w_2, dots , w_n) propto 1.88 cdot 10^{-41}$. Since the probability for spam was higher, it predicted that the message was spam. . Model Evaluation . The final step is to evaluate the predictive function. We will use the function to predict labels for the messages in the testing set. Then, we will show the predicted labels side-by-side with the real labels. . # Preprocess testing messages X_test_preprocessed = preprocess_messages(X_test.sms) # Make predictions y_pred = X_test_preprocessed.apply(predict, out = &quot;pred&quot;) y_pred.name = &quot;prediction&quot; # Concatenate full_test = pd.concat( [y_test, y_pred, X_test], axis = 1 ) full_test.head() . . label prediction sms . 0 spam | spam | England v Macedonia - dont miss the goals/team... | . 1 spam | spam | SMS. ac Sptv: The New Jersey Devils and the De... | . 2 spam | spam | Please call our customer service representativ... | . 3 spam | spam | URGENT! Your Mobile No. was awarded £2000 Bonu... | . 4 spam | spam | Sunshine Quiz Wkly Q! Win a top Sony DVD playe... | . The table above shows the first 5 rows of the testing set. We can see that the algorithm correctly predicted that the first 5 rows were spam. . Accuracy . We will now calculate the overall accuracy of the model by dividing the number of correct predictions by the total number of predictions. . acc = y_test.eq(y_pred).sum() / y_pred.shape[0] * 100 print(f&quot;Accuracy: {acc:.2f}%&quot;) . . Accuracy: 98.74% . The model turned out to have a very high accuracy of 98.74%. This shows that it is effective at filtering spam from non-spam. . However, considering that spam and non-spam did not have equal representation in the data, with only 13% of all messages being spam, the accuracy may be misleading (Vallantin, 2018). For example, if a model is instructed to always predict that a message is ham, it would still have an accuracy of around 87%. Thus, to get a better picture of the performance of the model, we will make a confusion matrix and use other evaluation metrics such as precision, recall, and F1. . Confusion Matrix . A confusion matrix is a table that gives insight into how well a model was able to predict labels for datapoints (Brownlee, 2016). . cm = confusion_matrix(y_test, y_pred, labels = [&quot;ham&quot;, &quot;spam&quot;]) disp = ConfusionMatrixDisplay(cm, display_labels = [&quot;ham&quot;, &quot;spam&quot;]) disp.plot() plt.title(&quot;Confusion Matrix of Results&quot;) plt.show() . . In this case, we only have two labels (spam and ham), so there are two rows and two columns. The vertical axis refers to the true labels, whereas the horizontal axis refers to the predicted labels. . For example, if we want to know how many true spam messages were incorrectly predicted to be ham, we can look at the &quot;spam&quot; row and &quot;ham&quot; column. The value is 13, which means that 13 messages were incorrectly predicted in this way. . If we consider ham messages as &quot;negative&quot; and spam messages as &quot;positive,&quot; then the confusion matrix above shows the True Negatives (TN) in the top left, False Negatives (FN) in the top right, False Positives (FP) in the bottom left, and True Positives (TP) in the bottom right. . Recall, Precision, F1 . Recall, Precision, and F1 are other metrics of model performance which may be useful when the labels are not represented equally in the data. Their equations are shown below; these were taken from Brownlee (2020). . Recall is the number of true positives divided by the total number of actual positives. It measures the &quot;ability of the classifier to find all the positive samples. (scikit-learn developers, 2021)&quot; . $Recall = frac{TP}{TP + FN}$ . Precision is the number of true positives divided by the total number of predicted positives. It is the &quot;ability of the classifier not to label as positive a sample that is negative&quot; (scikit-learn developers, 2021). In other words, it is the ability of the model to avoid Type I error (Bhandari, 2021). . $Precision = frac{TP}{TP+FP}$ . Finally, $F_{ beta}$ is the &quot;weighted harmonic mean of the precision and recall,&quot; and $F_1$ is the version where both of these factors are given equal importance (scikit-learn developers, 2021). . $F1 = frac{2 TP}{2 TP + FP + FN}$ . Let us calculate these metrics for the spam filter using the confusion matrix. . tp = 136 tn = 964 fp = 1 fn = 13 metrics = {} metrics[&quot;recall&quot;] = tp / (tp + fn) metrics[&quot;precision&quot;] = tp / (tp + fp) metrics[&quot;f1&quot;] = 2 * tp / (2 * tp + fp + fn) for key in metrics: print(f&quot;{key}: {metrics[key] * 100:.2f}%&quot;) . . recall: 91.28% precision: 99.27% f1: 95.10% . All three metrics are above 90%. In particular, precision is 99.27%, which means that the model is very good at avoiding labelling ham messages as spam. . Therefore, we can say that the naive bayes algorithm performs very well with the data that it was given. . scikit-learn Classes for Naive Bayes . In practice, a data scientist would not code the entire algorithm each time they use it, since this is time-consuming. Instead, one can use prepackaged classes in scikit-learn. . For the purpose of classifying spam based on word counts, we can apply the following classes, according to the scikit-learn developers (2021): . MultinomialNB: This uses the same logic as the algorithm explained earlier. | GaussianNB: &quot;The likelihood of the features is assumed to be Gaussian,&quot; so the mean and standard deviation of each features is considered. | ComplementNB: It is an &quot;adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets.&quot; This may be useful for our dataset because the percentages of spam and non-spam messages are imbalanced. | . Other classes which will not be used in this project include BernoulliNB, which assumes each feature to be binary (only two categories), and CategoricalNB, which assumes that each feature is categorical rather than numeric. . Let us try using the first three classes and comparing them. We will also use functions from sklearn.metrics to calculate accuracy score and the other metrics. . from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score nb_classes = { &quot;Multinomial&quot;: MultinomialNB, &quot;Gaussian&quot;: GaussianNB, &quot;Complement&quot;: ComplementNB, } metric_funcs = { &quot;Accuracy&quot;: accuracy_score, &quot;Precision&quot;: precision_score, &quot;Recall&quot;: recall_score, &quot;F1&quot;: f1_score, } result_rows = [] test_word_counts = to_word_counts(X_test_preprocessed) for class_name in nb_classes: nb = nb_classes[class_name] model = nb() model.fit( word_counts, y_train, ) y_pred = model.predict(test_word_counts) metric_results = pd.Series(name = class_name, dtype = &quot;float64&quot;) for metric_name, score_func in metric_funcs.items(): if metric_name == &quot;Accuracy&quot;: score = score_func( y_true = y_test, y_pred = y_pred, ) else: score = score_func( y_true = y_test, y_pred = y_pred, pos_label = &quot;spam&quot;, ) metric_results[metric_name] = score * 100 result_rows.append(metric_results) result_df = pd.DataFrame(result_rows) result_df . . Accuracy Precision Recall F1 . Multinomial 98.743268 | 99.270073 | 91.275168 | 95.104895 | . Gaussian 90.843806 | 60.444444 | 91.275168 | 72.727273 | . Complement 97.396768 | 88.461538 | 92.617450 | 90.491803 | . MultinomialNB performed the best, as it had the highest accuracy, precision, and F1 score. Its accuracy was 98.74%, which is the same as the accuracy of the algorithm that I manually coded earlier. Thus, I know that what I coded was correct. . ComplementNB also performed well, though its precision was 88%, meaning that it was more prone to labeling ham messages as spam. However, the GaussianNB class was the worst performing one, as its precision was 60% and its F1 was 73%. This likely occurred because the assumption that the features&#39; likelihoods were Gaussian did not hold. . Conclusion . In this project, we obtained a dataset of spam and non-spam messages, used this to calculate the parameters for a multinomial naive bayes algorithm, wrote a function to predict whether a new message is spam or non-spam, evaluated the model using various performance metrics, and tried three variations of the model in scikit-learn. . Naive bayes is just one of various classification models, such as SVM, logistic regression, random forest, etc. It is useful as most of its calculations are done during the training stage, so it is fast at making predictions. However, it is not perfect, as the assumption that the features are independent usually does not hold true in real life. (Gupta, 2020) . However, models cannot represent reality perfectly. Simplifications like this are good enough if they are useful in making predictions or testing relationships among variables. . Another important insight from this project is that accuracy should not be the only metric used to evaluate model performance. We should use other metrics like recall, precision, and F1 score, which provide more nuanced perspectives. Though this was not done for this project, in other scenarios, we may have to adjust the model to favor either recall or precision depending on the specific use case. . Thanks for reading! . Bibliography . Data Source . Almeida, T. A., &amp; Hidalgo, J. M. G. (2012, June 22). SMS Spam Collection Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/sms+spam+collection# . Information Sources . Bhandari, P. (2021, January 18). Type I and Type II errors. Scribbr. https://www.scribbr.com/statistics/type-i-and-type-ii-errors/ . Brownlee, J. (2016, November 17). What is a Confusion Matrix in Machine Learning. Machine Learning Mastery. https://machinelearningmastery.com/confusion-matrix-machine-learning/ . Brownlee, J. (2020, August 27). How to Calculate Precision, Recall, F1, and More for Deep Learning Models. Machine Learning Mastery. https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ . Dataquest. (n.d.). Guided Project: Building A Spam Filter With Naive Bayes. Dataquest. Retrieved December 14, 2021, from https://www.dataquest.io/c/74/m/433/guided-project%3A-building-a-spam-filter-with-naive-bayes . Devanesan, J. (2020, August 19). Phishing scams dominate the Philippines cybercrime landscape. Tech Wire Asia. https://techwireasia.com/2020/08/phishing-scams-dominate-the-philippines-cybercrime-landscape/ . FTC CI. (2020, February 19). How To Recognize and Report Spam Text Messages. Federal Trade Commission Consumer Information. https://www.consumer.ftc.gov/articles/how-recognize-and-report-spam-text-messages . Gupta, S. (2020, February 28). Pros and cons of various Classification ML algorithms. Medium. https://towardsdatascience.com/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6 . scikit-learn developers. (2021). 1.9. Naive Bayes. Scikit-Learn. https://scikit-learn/stable/modules/naive_bayes.html . scikit-learn developers. (2021). 3.3. Metrics and scoring: Quantifying the quality of predictions. Scikit-Learn. https://scikit-learn/stable/modules/model_evaluation.html . Thomas, L. (2020, September 18). How to use stratified sampling. Scribbr. https://www.scribbr.com/methodology/stratified-sampling/ Vadapalli, P. (2021, January 5). Naive Bayes Explained: Function, Advantages &amp; Disadvantages, Applications in 2021. UpGrad Blog. https://www.upgrad.com/blog/naive-bayes-explained/ . Vallantin, L. (2018, September 6). Why you should not trust only in accuracy to measure machine learning performance. Medium. https://medium.com/@limavallantin/why-you-should-not-trust-only-in-accuracy-to-measure-machine-learning-performance-a72cf00b4516 . Yap, C. (2021, November 25). Millions of spam messages blocked in Philippines as scams surge. Yahoo News. https://ph.news.yahoo.com/millions-of-spam-messages-blocked-in-philippines-as-scams-surge-005658569.html . Image Source . Johnson, H. (2020, February 24). Photo by Hannes Johnson on Unsplash. Unsplash. https://unsplash.com/photos/mRgffV3Hc6c . Appendix . Here, I explain how the multinomial naive bayes algorithm was derived from the Bayes Theorem. This was originally placed earlier in the post, but I deemed it to be unnecessary for that part. . Note: The mathematical explanations below are not my own ideas. I learned these from the Dataquest course on Naive Bayes. . Given two events $A$ and $B$, we can use the theorem to determine the probability that $B$ happened given that $A$ happened. This probability is written as $P(B|A)$. . $P(B|A) = frac{P(B) cdot P(A|B)}{ Sigma_{i = 1}^n (P(B_i) cdot P(A|B_i))}$ . In this case, $B_1$ is the event that the message is non-spam, and $B_2$ is the event that it is spam. $B$ can refer to either $B_1$ or $B_2$, depending on which probability we want to calculate. Also, $A$ refers to the specific contents of one message. . In order to make things clearer, let us say that $Spam$ is the event that the message is spam, and $Ham$ is the event that the message is non-spam. . Then, let us expand event $A$ (the message itself) in order to consider the individual words inside it. For example, the first word in a message can be labeled $w_1$. If we have a total of $n$ words, then the words can be labeled as $w_1, w_2, dots , w_n$. . Thus, we can rewrite the equation. Here is the probability of a given message being spam: . $P(Spam|w_1, w_2, dots , w_n) = frac{P(Spam) cdot P(w_1, w_2, dots , w_n|Spam)}{ Sigma_{i = 1}^n (P(B_i) cdot P(w_1, w_2, dots , w_n|B_i))}$ . Here is the probability of a given message being non-spam: . $P(Ham|w_1, w_2, dots , w_n) = frac{P(Ham) cdot P(w_1, w_2, dots , w_n|Ham)}{ Sigma_{i = 1}^n (P(B_i) cdot P(w_1, w_2, dots , w_n|B_i))}$ . Notice that the denominators are the same. Since we only want to compare these two probabilities, we can skip calculating the denominator and just calculate the numerators. We can thus rewrite the equation as follows. Note that the $ propto$ symbol is used instead of $=$ because the two quantities are not equal but directly proportional. . $P(Spam|w_1, w_2, dots , w_n) propto P(Spam) cdot P(w_1, w_2, dots , w_n|Spam)$ . The first factor, $P(Spam)$, is easy to find, as it is simply the number of spam messages divided by the total number of messages. However, $P(w_1, w_2, dots , w_n|Spam)$ needs to be further expanded. . If we make the assumption that the probability of each word is independent of the probability of the other words, we can use the multiplication rule. The assumption of independence is what makes the algorithm &quot;naive,&quot; as it usually doesn&#39;t hold true in reality. However, the algorithm is still useful for predictions despite this. . $P(Spam) cdot P(w_1, w_2, dots , w_n|Spam) = P(Spam) cdot P(w_1 cap w_2 cap dots cap w_n | Spam) = P(Spam) cdot P(w_1|Spam) cdot P(w_2|Spam) cdot dots cdot P(w_n|Spam)$ . Note that we still have to find the probability of each word given $Spam$ because we assume that the presence of each word is dependent on $Spam$. . Thus, the final formula is: . $P(Spam|w_1, w_2, dots , w_n) propto P(Spam) cdot Pi_{i=1}^n P(w_i|Spam)$ . Likewise, the formula for $Ham$ is: . $P(Ham|w_1, w_2, dots , w_n) propto P(Ham) cdot Pi_{i=1}^n P(w_i|Ham)$ .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/scikit-learn/2021/12/14/Naive-Bayes-Algorithm-Detecting-Spam-Messages.html",
            "relUrl": "/python/pandas/numpy/matplotlib/scikit-learn/2021/12/14/Naive-Bayes-Algorithm-Detecting-Spam-Messages.html",
            "date": " • Dec 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Basic Statistics with Fandango Movie Ratings",
            "content": "Unsplash | Alex Litvin Overview . Fandango is a website that sells tickets for movies. For each movie, the site provides a synopsis as well as an aggregated rating from 0.0 to 5.0. . A data journalism piece (Hickey, 2015) investigated Fandango&#39;s rating system. By comparing the displayed ratings to those in the website&#39;s code, it found that almost half of the displayed ratings were rounded up inappropriately. Thus, most movies had a rating of at least 3 stars. Furthermore, Fandango&#39;s ratings were generally higher than those of other movie rating websites. This implied that Fandango may have altered its ratings in order to encourage people to purchase its tickets. . The present project aimed to determine whether the distribution of ratings has changed from the time of Hickey&#39;s analysis. Thus, the data used by Hickey was compared to more recent data from 2016. Furthermore, basic statistical concepts and hypothesis testing were applied in this project. The Mann-Whitney U rank test for independent samples showed that the distribution of 2015 ratings was significantly higher than that of 2016 ratings (p &lt; 0.001). . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically the Guided Project: Investigating Fandango Movie Reviews. The general project flow and research questions came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Preparations . Below are the imports used for this project. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import mannwhitneyu . Population and Samples . Given the project goal, there were two populations of interest: . All movies released in 2015 with ratings on Fandango. | All movies released in 2016 with ratings on Fandango. | . However, the two datasets available included only samples of this population. These datasets were taken from Dataquest. . Dataset A was fandango_score_comparison.csv. It included movies released in 2014-2015 that had at least 30 reviews as of mid-2015. These amounted to 146 movies. The data dictionary, which explains the meanings of the data columns, can be found here. . Dataset B was movie_ratings_16_17.csv. It included 214 of the most popular movies released in 2016-2017. The ratings provided are from 2017. The data dictionary is here. . Given the descriptions of the two samples, it was clear that the sampling was not random. Certain criteria, such as the number of reviews or the popularity of the movie, were used to select movies. Therefore, the samples did not truly represent the populations of interest, which include all of the movies released in each year. . Therefore, the goal of the project was adjusted to account for the samples. We then sought to determine whether the distribution of Fandango ratings for popular movies in 2015 was similar to the distribution of Fandango ratings for popular movies in 2016. This new goal only considers &quot;popular&quot; movies, not all available movies. . However, &quot;popularity&quot; had to be given a precise definition. Since this could not be directly measured, the benchmark used in Hickey&#39;s analysis was used in this project. That is, all movies with at least 30 ratings were considered as &quot;popular&quot; movies. . Data Cleaning . Based on the adjusted goal of the project, the datasets were cleaned to get samples representative of the populations. . First, only movies released in 2015 were taken from Dataset A, and only movies released in 2016 were taken from Dataset B. . The next concern was that Dataset B did not provide the number of reviews, so the benchmark of having at least 30 reviews could not be used to identify objectively popular movies. However, it was possible to assume that all of the movies in Dataset B were popular. The person who scraped the data, Alex Olteanu, stated that he took the &quot;most voted and reviewed&quot; movies released in 2016 and 2017 (Olteanu, 2017). . The code below cleaned the data based on the above requirements, then printed the final sizes of the samples. . folder = &quot;./private/2021-11-18-Fandango-Ratings-Files&quot; # Obtain full data, which includes ratings from various sites df_a_all = pd.read_csv(folder + &quot;/fandango_score_comparison.csv&quot;) df_b_all = pd.read_csv(folder + &quot;/movie_ratings_16_17.csv&quot;) # Dataset A: Take only the data from Fandango, not other sites df_a = df_a_all[[ &quot;FILM&quot;, &quot;Fandango_Stars&quot;, &quot;Fandango_Ratingvalue&quot;, &quot;Fandango_votes&quot;, &quot;Fandango_Difference&quot;, ]].copy() # Create a column of release years based on the FILM column df_a.loc[:, &quot;year&quot;] = ( df_a .loc[:, &quot;FILM&quot;] .str.extract(r&quot; ((201[45]) )$&quot;) .astype(int, errors = &quot;ignore&quot;) ) # Take movies from 2015 df_a = df_a.loc[df_a[&quot;year&quot;] == 2015] # Dataset B: Take only data from Fandango, not other sites df_b = df_b_all[[ &quot;movie&quot;, &quot;year&quot;, &quot;fandango&quot;, ]].copy() # Take movies from 2016 df_b = df_b.loc[df_b[&quot;year&quot;] == 2016] print(&quot;Number of 2015 movies:&quot;, df_a.shape[0]) print(&quot;Number of 2016 movies:&quot;, df_b.shape[0]) . . Number of 2015 movies: 129 Number of 2016 movies: 191 . The sample sizes decreased because movies released in 2014 and 2017 were excluded. . The first 5 entries in Dataset A, from 2015, were shown below. . df_a.head() . . FILM Fandango_Stars Fandango_Ratingvalue Fandango_votes Fandango_Difference year . 0 Avengers: Age of Ultron (2015) | 5.0 | 4.5 | 14846 | 0.5 | 2015 | . 1 Cinderella (2015) | 5.0 | 4.5 | 12640 | 0.5 | 2015 | . 2 Ant-Man (2015) | 5.0 | 4.5 | 12055 | 0.5 | 2015 | . 3 Do You Believe? (2015) | 5.0 | 4.5 | 1793 | 0.5 | 2015 | . 4 Hot Tub Time Machine 2 (2015) | 3.5 | 3.0 | 1021 | 0.5 | 2015 | . The FILM column gives the name of the film. Fandango_Ratingvalue is the hidden rating found in the website&#39;s HTML, whereas Fandango_Stars is the value that was displayed on the webpages. . On the other hand, Dataset B only contained displayed ratings. The hidden ratings had been removed from the HTML before this dataset was collected. . df_b.head() . . movie year fandango . 0 10 Cloverfield Lane | 2016 | 3.5 | . 1 13 Hours | 2016 | 4.5 | . 2 A Cure for Wellness | 2016 | 3.0 | . 4 A Hologram for the King | 2016 | 3.0 | . 5 A Monster Calls | 2016 | 4.0 | . In both datasets, it may be observed that the displayed ratings were rounded to the nearest half-star from their original values. . Analysis . The goal was to determine whether the two samples have a similar distribution. Thus, a kernel density estimation (KDE) plot was used to visualize the distributions. KDE plots are similar to histograms, but these are smooth and continuous. The y-axis represents the approximated probability density at a particular value of x. (Waskom, 2021) . plt.style.use(&#39;fivethirtyeight&#39;) plt.figure(figsize = (10, 6)) sns.kdeplot( data = df_a, x = &quot;Fandango_Stars&quot;, label = &quot;Dataset A: 2015&quot;, ) sns.kdeplot( data = df_b, x = &quot;fandango&quot;, label = &quot;Dataset B: 2016&quot; ) plt.title(&quot;Fandango Displayed Rating Distribution in 2015 and 2016&quot;) plt.xlabel(&quot;Fandango Rating&quot;) # Limit the x-axis to [0, 5] because this is the range of ratings plt.xlim(0, 5) # Show ticks in 0.5 intervals plt.xticks(np.arange(0.0, 5.5, 0.5)) plt.ylabel(&quot;Probability Density&quot;) plt.legend() plt.show() . . In the KDE plot above, the blue line represents the distribution of 2015 displayed ratings. On the other hand, the red line represents the distribution of 2016 displayed ratings. The hidden ratings from 2015 were excluded from this plot because these did not directly influence Fandango customers&#39; ticket-purchasing decisions, unlike the displayed ratings. . Both distributions were left-skewed. There were more high ratings than low ratings. However, the 2015 ratings noticeably had a higher mode, which was near 4.5. In general, the whole 2015 distribution seemed to be shifted to the right of the 2016 distribution. Therefore, it seemed that movies generally had higher Fandango ratings in 2015 than 2016. . In order to get more specific numbers, frequency tables were generated. . # Create a frequency table for Dataset B (2016) freq_b = df_b[&quot;fandango&quot;].value_counts(bins = 10).sort_index() freq_b.name = &quot;2016&quot; # Use the bins in the B table for the A table freq_a = {} for rating in df_a[&quot;Fandango_Stars&quot;]: for rng in freq_b.index: if rating in rng: freq_a.setdefault(rng, 0) freq_a[rng] += 1 freq_a = pd.Series(freq_a, name = &quot;2015&quot;) # Combine the tables into one freq_both = ( pd.DataFrame(freq_b) .merge( freq_a, how = &quot;left&quot;, left_index = True, right_index = True, ) .fillna(0) .loc[:, [&quot;2015&quot;, &quot;2016&quot;]] ) # Display the table print(&quot;Number of ratings per interval of 0.25 stars&quot;) freq_both . . Number of ratings per interval of 0.25 stars . 2015 2016 . (2.4970000000000003, 2.75] 0.0 | 6 | . (2.75, 3.0] 11.0 | 14 | . (3.0, 3.25] 0.0 | 0 | . (3.25, 3.5] 23.0 | 46 | . (3.5, 3.75] 0.0 | 0 | . (3.75, 4.0] 37.0 | 77 | . (4.0, 4.25] 0.0 | 0 | . (4.25, 4.5] 49.0 | 47 | . (4.5, 4.75] 0.0 | 0 | . (4.75, 5.0] 9.0 | 1 | . The table shows that: . There were 11 more ratings above 4.25 in 2015 than in 2016. | There were 72 fewer ratings below 4.25 in 2015 than in 2016. | . This further supported the observation that Fandango&#39;s displayed ratings were generally higher in 2015 than in 2016. . In order to determine whether this difference was statistically significant, hypothesis testing for two independent samples was used. Since the two distributions both looked left-skewed rather than normal, it was appropriate to use the Mann-Whitney U rank test. According to NCSS (n.d.), this is a nonparametric test with the following assumptions: . The variable is continuous, or at least ordinal. Both datasets contained rounded mean ratings, so these followed an ordinal distribution with intervals of 0.5. | The two populations&#39; distributions are identical except for location. One may occupy a higher range than the other. This was fulfilled because all of the ratings came from Fandango. It was expected that the ratings would be consistently left-skewed, especially since there was only a one-year gap between our two populations. | The two samples are independent. This was true because there was no movie that existed in both datasets. Dataset A only contained movies released in 2015, and Dataset B only contained movies released in 2016. | The samples are simple random samples. It was not certain that the people who collected the data chose totally randomly from a list of popular movies. However, since this is the best data available, it had to be used. | . Let the distribution of Dataset A be F(u), and that of Dataset B be G(u). The alternative hypothesis was that F(u) &gt; G(u). The test was performed below using the Scipy package. . U1, p = mannwhitneyu( df_a[&quot;Fandango_Stars&quot;], df_b[&quot;fandango&quot;], # Alternative hypothesis alternative = &quot;greater&quot;, # Adjust for ties method = &quot;asymptotic&quot;, ) print(f&quot;U1 statistic: {U1}&quot;) print(f&quot;p-value: {p}&quot;) . . U1 statistic: 14871.5 p-value: 0.000516222939793581 . The $U$ statistic is the &quot;the total number of times an observation in one group is preceded by an observation in the other group in the ordered configuration of combined samples (Gibbons, 1985)[.]&quot; (NCSS, n.d.) . In this case, $U_1$ was the $U$ statistic of Dataset A with respect to Dataset B. Therefore, for around 14871.5 times, an observation in A was preceded by an observation in B. This was rather high, considering that each sample only had a few hundred observations. . More importantly, the p-value was approximately 0.0005. Therefore, the difference between the two populations was significant at $ alpha = 0.001$. The distribution of ratings in 2015 was significantly greater than that in 2016. . Conclusion . A KDE plot, frequency table, and Mann-Whitney U rank test were used to investigate the Fandango ratings of popular movies. Based on the results, the alternative hypothesis is accepted. The ratings displayed in 2015 were significantly higher than those in 2016 (p &lt; 0.001). . The implication is that Fandango may have investigated the issues raised in Hickey&#39;s analysis and fixed their displayed ratings. They may have stopped the numbers from being rounded up excessively, so the 2016 ratings ended up being lower than the 2015 ratings. . Bibliography . Information Sources . Details for Non-Parametric Alternatives in Case C-Q. (2021). In Biostatistics Open Learning Textbook. University of Florida. https://bolt.mph.ufl.edu/6050-6052/unit-4b/module-13/details-for-non-parametric-alternatives/ . Guided Project: Investigating Fandango Movie Ratings. (n.d.). Dataquest. Retrieved November 18, 2021, from https://www.dataquest.io/c/53/m/288/guided-project%3A-investigating-fandango-movie-ratings . Hickey, W. (2015, October 15). Be Suspicious Of Online Movie Ratings, Especially Fandango’s. FiveThirtyEight. https://fivethirtyeight.com/features/fandango-movies-ratings/ . LaMorte, W. W. (2017, May 4). Mann-Whitney-Table-CriticalValues.pdf. Boston University School of Public Health. https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Nonparametric/Mann-Whitney-Table-CriticalValues.pdf . NCSS. (n.d.). Chapter 206 Two-Sample T-Test. NCSS Statistics Solutions. Retrieved November 18, 2021, from https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Two-Sample_T-Test.pdf . scipy.stats.mannwhitneyu—SciPy v1.7.1 Manual. (2021). Scipy. https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html . Waskom, M. (2021). Visualizing distributions of data—Seaborn 0.11.2 documentation. Seaborn. http://seaborn.pydata.org/tutorial/distributions.html#tutorial-kde . Image Sources . Litvin, A. (2018, August 23). Photo by Alex Litvin on Unsplash. Unsplash. https://unsplash.com/photos/MAYsdoYpGuk .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/matplotlib/seaborn/scipy/2021/11/18/Basic-Statistics-Fandango-Ratings.html",
            "relUrl": "/python/pandas/matplotlib/seaborn/scipy/2021/11/18/Basic-Statistics-Fandango-Ratings.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Answering Business Questions for an Online Music Store using SQL",
            "content": "Unsplash | Clay Banks Overview . The Chinook database by Luis Rocha and Brice Lambson is a sample database about a hypothetical digital media store called Chinook. This store sells individual music tracks online, similar to iTunes. The database contains tables covering various aspects of the company, such as the employees, customers, invoices, tracks, albums, and artists. . The schema below, which was designed by Dataquest, lists the columns under each table. Columns connected by lines contain matching information. . Dataquest Guided Project: Answering Business Questions Using SQL The matching columns allow us to perform joins on these tables. Thus, we are able to answer more complicated questions about the data. . In our hypothetical scenario, the Chinook company has requested us to answer the following business questions: . What are the best-selling music genres with regards to USA customers? Based on this, which new albums should be purchased for the Chinook store? | Which of Chinook&#39;s sales support agents has the highest total sales from their assigned customers? Can the exemplary performance of these employees be explained by any information in the database? | What are the statistics on the customers and sales for each country where Chinook offers its service? | How many purchases are full albums, and how many are selected sets of tracks? Based on this, what strategy should Chinook adopt when buying new tracks from record companies? | . SQL will be used to answer all of these questions. Matplotlib and Altair will also be used to produce helpful visualizations. . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically the Guided Project: Answering Business Questions Using SQL. The general project flow and research questions came from Dataquest. However, the text and code here are written by me unless stated otherwise. . Preparations . Install the necessary packages. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import altair as alt . Connect to the database using SQLite. . %load_ext sql %sql sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db . &#39;Connected: @private/2021-07-31-Intermediate-SQL-Files/chinook.db&#39; . Analysis . Tables and Views . First, we&#39;ll inspect the tables and views available in the chinook.db database. . Tables contain columns of data. Each column has a different name and data type. | Views do not contain data. Instead, these are pre-written SQL queries which show a transformation of existing data. Thus, it can be called a &quot;virtual table.&quot; (Sławińska 2020) | . %%sql SELECT name, type FROM sqlite_master WHERE type IN (&quot;table&quot;, &quot;view&quot;) ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. . name type . album | table | . artist | table | . customer | table | . employee | table | . genre | table | . invoice | table | . invoice_line | table | . media_type | table | . playlist | table | . playlist_track | table | . track | table | . usa_track_purchases | view | . usa_genre_sales | view | . agent_customer | view | . agent_stats | view | . country_invoices | view | . country_customers | view | . country_labels | view | . country_avg_order | view | . country_stats | view | . invoice_tracks_bought | view | . invoice_album | view | . invoice_full_album_tracks | view | . invoice_purchase_type | view | . purchase_type_proportion | view | . Originally, there were only 11 tables and 0 views in the database. The views listed above are ones which I made throughout this project. I will show how I created these views in later sections. . For now, let&#39;s inspect the customer table, as we will be using it to answer most of the company&#39;s business questions. . %%sql SELECT * FROM customer LIMIT 5 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. . customer_id first_name last_name company address city state country postal_code phone fax email support_rep_id . 1 | Luís | Gonçalves | Embraer - Empresa Brasileira de Aeronáutica S.A. | Av. Brigadeiro Faria Lima, 2170 | São José dos Campos | SP | Brazil | 12227-000 | +55 (12) 3923-5555 | +55 (12) 3923-5566 | luisg@embraer.com.br | 3 | . 2 | Leonie | Köhler | None | Theodor-Heuss-Straße 34 | Stuttgart | None | Germany | 70174 | +49 0711 2842222 | None | leonekohler@surfeu.de | 5 | . 3 | François | Tremblay | None | 1498 rue Bélanger | Montréal | QC | Canada | H2G 1A7 | +1 (514) 721-4711 | None | ftremblay@gmail.com | 3 | . 4 | Bjørn | Hansen | None | Ullevålsveien 14 | Oslo | None | Norway | 0171 | +47 22 44 22 22 | None | bjorn.hansen@yahoo.no | 4 | . 5 | František | Wichterlová | JetBrains s.r.o. | Klanova 9/506 | Prague | None | Czech Republic | 14700 | +420 2 4172 5555 | +420 2 4172 5555 | frantisekw@jetbrains.com | 4 | . Each row in this table contains data on a different customer of Chinook. Each customer has a unique customer ID and an assigned support representative from Chinook. The support rep&#39;s employee ID is stored in the support_rep_id column. The other columns contain information on the customer&#39;s name, occupation, location, and contact details. . Note: All names and personal details in the Chinook database are fictitious and randomly generated. Public use of this database is not a breach of data privacy. . Best-Selling Music Genres in the USA . In our first scenario, Chinook has signed a deal with a new record company, so its tracks can now be put up for sale on the Chinook store. The record company has 4 albums so far; below are the artist names and their genres. . Regal (Hip-Hop) | Red Tone (Punk) | Meteor and the Girls (Pop) | Slim Jim Bites (Blues) | . However, Chinook would like to spread its releases over time, so it will only add 3 albums to the store. Thus, we have to determine the best-selling genres on the store. Furthermore, since the record company would like to target a USA audience, we can narrow our analysis to Chinook&#39;s USA customers. . First, we create a view called usa_track_purchases. This will show the genre, track name, unit price, and quantity bought for each of the invoice lines of USA customers. . %%sql DROP VIEW IF EXISTS usa_track_purchases; CREATE VIEW usa_track_purchases AS SELECT il.invoice_line_id AS invoice_line_id, g.name AS genre, t.name AS track_name, il.unit_price AS unit_price, il.quantity AS quantity FROM customer AS c INNER JOIN invoice AS iv ON iv.customer_id = c.customer_id INNER JOIN invoice_line AS il ON il.invoice_id = iv.invoice_id INNER JOIN track AS t ON t.track_id = il.track_id INNER JOIN genre AS g ON g.genre_id = t.genre_id WHERE c.country = &quot;USA&quot; ; SELECT * FROM usa_track_purchases LIMIT 7 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . invoice_line_id genre track_name unit_price quantity . 1 | Rock | Right Next Door to Hell | 0.99 | 1 | . 2 | Rock | Dust N&amp;#x27; Bones | 0.99 | 1 | . 3 | Rock | Live and Let Die | 0.99 | 1 | . 4 | Rock | Don&amp;#x27;t Cry (Original) | 0.99 | 1 | . 5 | Rock | Perfect Crime | 0.99 | 1 | . 6 | Rock | You Ain&amp;#x27;t the First | 0.99 | 1 | . 7 | Rock | Bad Obsession | 0.99 | 1 | . By querying the view above, we will create another view, usa_genre_sales. This will contain the following specific information about each genre: . Number of tracks sold | Percentage of tracks sold | Total sales in US dollars | . %%sql DROP VIEW IF EXISTS usa_genre_sales; CREATE VIEW usa_genre_sales AS SELECT genre, SUM(quantity) AS number_sold, --Get the quantity per genre and divide it by the total quantity of USA purchases. ROUND( CAST(SUM(quantity) AS Float) / CAST( (SELECT COUNT(*) FROM usa_track_purchases) AS Float ) * 100.0, 2 ) AS percentage_sold, ROUND( SUM(unit_price * CAST(quantity AS Float)), 2 ) AS total_sales FROM usa_track_purchases GROUP BY genre ORDER BY number_sold DESC, total_sales DESC ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. . [] . %%sql result &lt;&lt; SELECT * FROM usa_genre_sales ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Returning data to local variable result . usa_genre_df = result.DataFrame() usa_genre_df . genre number_sold percentage_sold total_sales . 0 Rock | 561 | 53.38 | 555.39 | . 1 Alternative &amp; Punk | 130 | 12.37 | 128.70 | . 2 Metal | 124 | 11.80 | 122.76 | . 3 R&amp;B/Soul | 53 | 5.04 | 52.47 | . 4 Blues | 36 | 3.43 | 35.64 | . 5 Alternative | 35 | 3.33 | 34.65 | . 6 Latin | 22 | 2.09 | 21.78 | . 7 Pop | 22 | 2.09 | 21.78 | . 8 Hip Hop/Rap | 20 | 1.90 | 19.80 | . 9 Jazz | 14 | 1.33 | 13.86 | . 10 Easy Listening | 13 | 1.24 | 12.87 | . 11 Reggae | 6 | 0.57 | 5.94 | . 12 Electronica/Dance | 5 | 0.48 | 4.95 | . 13 Classical | 4 | 0.38 | 3.96 | . 14 Heavy Metal | 3 | 0.29 | 2.97 | . 15 Soundtrack | 2 | 0.19 | 1.98 | . 16 TV Shows | 1 | 0.10 | 0.99 | . We can make a bar graph from this result in order to communicate findings better. . ( alt.Chart(usa_genre_df) .mark_bar() .encode( x = alt.X(&quot;genre:N&quot;, title = &quot;Music Genre&quot;, sort = &quot;-y&quot;), y = alt.Y(&quot;percentage_sold:Q&quot;, title = &quot;Percentage of All Purchases in the USA&quot;), color = alt.Color(&quot;total_sales:Q&quot;, title = &quot;Total Sales (USD)&quot;), tooltip = usa_genre_df.columns.tolist(), ) .properties( title = &quot;Popularity of Music Genres with Chinook&#39;s USA Customers&quot;, height = 300, width = 600, ) .configure_axis( labelAngle = 30, ) .interactive() ) . . One can hover over each bar in the chart above for a tooltip with more specific information. . Results show that Rock is the best-selling music genre as it makes up 53% of total purchases in the USA. Rock is followed by the Alternative &amp; Punk and Metal genres, which each make up over 10% of purchases. . On a side note, the total_sales column&#39;s values are very close to that of the number_sold column. This can be explained by the fact that track prices range from USD 0.99 to USD 1.99. . %%sql SELECT MIN(unit_price) AS min_price, MAX(unit_price) AS max_price FROM track ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. . min_price max_price . 0.99 | 1.99 | . Since there is little variation among track prices, the genre with the most units sold is usually also the genre with the highest sales. . Going back to the scenario at hand, we need to compare the statistics on the Hip-Hop, Punk, Pop, and Blues genres. We will run a query for this below. . %%sql SELECT * FROM usa_genre_sales WHERE genre IN (&quot;Hip Hop/Rap&quot;, &quot;Alternative &amp; Punk&quot;, &quot;Pop&quot;, &quot;Blues&quot;) ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. . genre number_sold percentage_sold total_sales . Alternative &amp; Punk | 130 | 12.37 | 128.7 | . Blues | 36 | 3.43 | 35.64 | . Pop | 22 | 2.09 | 21.78 | . Hip Hop/Rap | 20 | 1.9 | 19.8 | . The result above shows that Alternative &amp; Punk, Blues, and Pop are the three best-selling genres out of the four. Notably, Alternative &amp; Punk makes up 12.37% of all purchases in the USA. . Therefore, we would recommend the Chinook administration to add the albums of Red Tone (Punk), Slim Jim Bites (Blues), and Meteor and the Girls (Pop) to the digital store. The album of Regal (Hip-Hop) has lower priority and can be added at a later date. . Sales Support Agent Performance . Next, Chinook is requesting us to evaluate the performance of its sales support agents. Each customer is assigned to an agent after their first purchase. Since there are only 3 agents, each agent provides support to many customers. Details about the agents are shown in the query below. . %%sql SELECT * FROM employee WHERE title = &quot;Sales Support Agent&quot; ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. . employee_id last_name first_name title reports_to birthdate hire_date address city state country postal_code phone fax email . 3 | Peacock | Jane | Sales Support Agent | 2 | 1973-08-29 00:00:00 | 2017-04-01 00:00:00 | 1111 6 Ave SW | Calgary | AB | Canada | T2P 5M5 | +1 (403) 262-3443 | +1 (403) 262-6712 | jane@chinookcorp.com | . 4 | Park | Margaret | Sales Support Agent | 2 | 1947-09-19 00:00:00 | 2017-05-03 00:00:00 | 683 10 Street SW | Calgary | AB | Canada | T2P 5G3 | +1 (403) 263-4423 | +1 (403) 263-4289 | margaret@chinookcorp.com | . 5 | Johnson | Steve | Sales Support Agent | 2 | 1965-03-03 00:00:00 | 2017-10-17 00:00:00 | 7727B 41 Ave | Calgary | AB | Canada | T3B 1Y7 | 1 (780) 836-9987 | 1 (780) 836-9543 | steve@chinookcorp.com | . Since we have data on each customer&#39;s purchases, we can calculate the total purchases associated with each support agent. We can then use this to compare the performance of the agents. . First, we create a view called agent_customer by joining the employee table with the customer table based on the support representative ID. We will include some extra details about each agent, such as their birthdate and hire date. We won&#39;t include their location since we know that they are all in Calgary, AB, Canada. As for the customers, we will include their customer ID, name, and total purchases as the sum of their invoices. . %%sql DROP VIEW IF EXISTS agent_customer; CREATE VIEW agent_customer AS SELECT e.employee_id AS agent_id, e.first_name || &quot; &quot; || e.last_name AS agent_name, e.birthdate AS agent_bd, e.hire_date AS agent_hire_date, c.customer_id AS customer_id, c.first_name || &quot; &quot; || c.last_name AS customer_name, SUM(iv.total) AS customer_total_purchases FROM employee AS e LEFT JOIN customer AS c ON c.support_rep_id = e.employee_id LEFT JOIN invoice AS iv ON iv.customer_id = c.customer_id WHERE e.title = &quot;Sales Support Agent&quot; GROUP BY c.customer_id ORDER BY agent_id, customer_id ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. . [] . %%sql result &lt;&lt; SELECT * FROM agent_customer ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Returning data to local variable result . agent_customer_df = result.DataFrame() agent_customer_df.head() . agent_id agent_name agent_bd agent_hire_date customer_id customer_name customer_total_purchases . 0 3 | Jane Peacock | 1973-08-29 00:00:00 | 2017-04-01 00:00:00 | 1 | Luís Gonçalves | 108.90 | . 1 3 | Jane Peacock | 1973-08-29 00:00:00 | 2017-04-01 00:00:00 | 3 | François Tremblay | 99.99 | . 2 3 | Jane Peacock | 1973-08-29 00:00:00 | 2017-04-01 00:00:00 | 12 | Roberto Almeida | 82.17 | . 3 3 | Jane Peacock | 1973-08-29 00:00:00 | 2017-04-01 00:00:00 | 15 | Jennifer Peterson | 66.33 | . 4 3 | Jane Peacock | 1973-08-29 00:00:00 | 2017-04-01 00:00:00 | 18 | Michelle Brooks | 79.20 | . Next, we will query this view in order to determine the following information about each agent: . number of customers | total sales from the agent&#39;s customers | percentage of all agents&#39; sales | average sales per customer | . %%sql DROP VIEW IF EXISTS agent_stats; CREATE VIEW agent_stats AS SELECT agent_id, agent_name, agent_bd, agent_hire_date, COUNT(customer_id) AS number_customers, ROUND( SUM(customer_total_purchases), 2 ) AS sales_number, ROUND( SUM(customer_total_purchases) / (SELECT SUM(customer_total_purchases) FROM agent_customer) * 100, 2 )AS sales_percentage, ROUND( AVG(customer_total_purchases), 2 ) AS average_per_customer FROM agent_customer GROUP BY agent_id ; SELECT * FROM agent_stats ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . agent_id agent_name agent_bd agent_hire_date number_customers sales_number sales_percentage average_per_customer . 3 | Jane Peacock | 1973-08-29 00:00:00 | 2017-04-01 00:00:00 | 21 | 1731.51 | 36.77 | 82.45 | . 4 | Margaret Park | 1947-09-19 00:00:00 | 2017-05-03 00:00:00 | 20 | 1584.0 | 33.63 | 79.2 | . 5 | Steve Johnson | 1965-03-03 00:00:00 | 2017-10-17 00:00:00 | 18 | 1393.92 | 29.6 | 77.44 | . At first glance, Jane Peacock seems like the best-performing agent since Chinook got the highest total sales from her customers (USD 1731.51). This idea is cast into doubt when it is mentioned that she had the highest number of customers (21). . Also, the differences among the average sales per customer is quite small. Each of Jane&#39;s customers spends 3 more dollars on Chinook than each of Margaret&#39;s. Each of Margaret&#39;s customers spends 2 more dollars on Chinook than each of Steve Johnson&#39;s. . The average sales per customer may also be influenced by outliers, as shown by the boxplot below. . ( alt.Chart(agent_customer_df) .mark_boxplot() .encode( y = alt.Y(&quot;customer_total_purchases:Q&quot;, title = &quot;Customer Total Purchases&quot;), x = alt.X(&quot;agent_name:N&quot;, title = &quot;Agent Name&quot;), ) .properties( title = &quot;Distribution of Customer Purchases by Sales Support Agent&quot;, height = 300, width = 500, ) .interactive() ) . . If we hover over each of the boxes above, we can get more information such as minimum, 1st quartile, median, etc. The median is less influenced by outliers, so we can look at that. The median values are 79.20 (Jane), 77.72 (Margaret), and 75.74 (Steve). These values are still very close to each other. . Therefore, we cannot conclusively state that any agent performs better than the others. . Sales Data by Country . Next, Chinook is requesting us to analyze sales in each country where it offers its service. Specifically, they would like to know the: . total number of customers | total value of sales | average sales per customer | average order value Every order is a batch purchase of multiple tracks. | . | . Furthermore, since there are some countries in the database with only one customer, we shall group these customers together under a category called &quot;Other&quot;, which must appear at the very bottom of our final result. . First, we will create a view, country_invoices, which shows all invoices and the country of the customer. . %%sql DROP VIEW IF EXISTS country_invoices; CREATE VIEW country_invoices AS SELECT c.country AS country, c.customer_id AS customer_id, c.first_name || &quot; &quot; || c.last_name AS customer_name, iv.invoice_id AS invoice_id, iv.total AS order_value FROM customer AS c LEFT JOIN invoice AS iv ON iv.customer_id = c.customer_id ; SELECT * FROM country_invoices LIMIT 5 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . country customer_id customer_name invoice_id order_value . Brazil | 1 | Luís Gonçalves | 16 | 8.91 | . Brazil | 1 | Luís Gonçalves | 77 | 5.9399999999999995 | . Brazil | 1 | Luís Gonçalves | 149 | 8.91 | . Brazil | 1 | Luís Gonçalves | 153 | 13.86 | . Brazil | 1 | Luís Gonçalves | 182 | 5.9399999999999995 | . Then, we will create a view, country_customers, which shows the total purchases per customer. . %%sql DROP VIEW IF EXISTS country_customers; CREATE VIEW country_customers AS SELECT country, customer_id, customer_name, SUM(order_value) AS customer_total_purchase FROM country_invoices GROUP BY customer_id ; SELECT * FROM country_customers LIMIT 5 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . country customer_id customer_name customer_total_purchase . Brazil | 1 | Luís Gonçalves | 108.89999999999998 | . Germany | 2 | Leonie Köhler | 82.17 | . Canada | 3 | François Tremblay | 99.99 | . Norway | 4 | Bjørn Hansen | 72.27000000000001 | . Czech Republic | 5 | František Wichterlová | 144.54000000000002 | . We will then create a view called country_labels. It will show the number of customers per country. Countries with only 1 customer will be given a label of &quot;Other&quot;. . %%sql DROP VIEW IF EXISTS country_labels; CREATE VIEW country_labels AS SELECT country, COUNT(customer_id) AS number_customers, CASE WHEN COUNT(customer_id) &gt; 1 THEN country ELSE &quot;Other&quot; END AS country_label, CASE WHEN COUNT(customer_id) &gt; 1 THEN 0 ELSE 1 END AS is_other FROM country_customers GROUP BY country ; SELECT * FROM country_labels LIMIT 10 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . country number_customers country_label is_other . Argentina | 1 | Other | 1 | . Australia | 1 | Other | 1 | . Austria | 1 | Other | 1 | . Belgium | 1 | Other | 1 | . Brazil | 5 | Brazil | 0 | . Canada | 8 | Canada | 0 | . Chile | 1 | Other | 1 | . Czech Republic | 2 | Czech Republic | 0 | . Denmark | 1 | Other | 1 | . Finland | 1 | Other | 1 | . We will also create a view called country_avg_order which simply shows the average order value per country. This will be done by querying the country_invoices view. . %%sql DROP VIEW IF EXISTS country_avg_order; CREATE VIEW country_avg_order AS SELECT cl.country_label, AVG(civ.order_value) AS avg_order_value FROM country_invoices AS civ INNER JOIN country_labels AS cl ON cl.country = civ.country GROUP BY cl.country_label ; SELECT * FROM country_avg_order ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . country_label avg_order_value . Brazil | 7.011147540983605 | . Canada | 7.047236842105264 | . Czech Republic | 9.107999999999999 | . France | 7.781400000000001 | . Germany | 8.161463414634145 | . India | 8.72142857142857 | . Other | 7.44857142857143 | . Portugal | 6.3837931034482756 | . USA | 7.942671755725194 | . United Kingdom | 8.768571428571429 | . Finally, we will create a view called country_stats that shows all of the 4 statistics that were requested by the Chinook management. The &quot;Other&quot; entry will be forced to the bottom of the query result using the is_other column we created in the country_labels view. . %%sql DROP VIEW IF EXISTS country_stats; CREATE VIEW country_stats AS SELECT l.country_label, COUNT(c.customer_id) AS number_customers, ROUND(SUM(c.customer_total_purchase), 2) AS total_sales, ROUND(AVG(c.customer_total_purchase), 2) AS avg_sales_customer, ROUND(a.avg_order_value, 2) AS avg_order_value FROM country_customers AS c INNER JOIN country_labels AS l ON l.country = c.country INNER JOIN country_avg_order AS a ON a.country_label = l.country_label GROUP BY l.country_label ORDER BY l.is_other ASC, total_sales DESC ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. . [] . %%sql result &lt;&lt; SELECT * FROM country_stats ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Returning data to local variable result . country_stats_df = result.DataFrame() country_stats_df . country_label number_customers total_sales avg_sales_customer avg_order_value . 0 USA | 13 | 1040.49 | 80.04 | 7.94 | . 1 Canada | 8 | 535.59 | 66.95 | 7.05 | . 2 Brazil | 5 | 427.68 | 85.54 | 7.01 | . 3 France | 5 | 389.07 | 77.81 | 7.78 | . 4 Germany | 4 | 334.62 | 83.66 | 8.16 | . 5 Czech Republic | 2 | 273.24 | 136.62 | 9.11 | . 6 United Kingdom | 3 | 245.52 | 81.84 | 8.77 | . 7 Portugal | 2 | 185.13 | 92.57 | 6.38 | . 8 India | 2 | 183.15 | 91.58 | 8.72 | . 9 Other | 15 | 1094.94 | 73.00 | 7.45 | . We can make a scatter plot with the data above in order to make results more clear. . # Exclude the &quot;Other&quot; entry. main_df = country_stats_df.loc[country_stats_df[&quot;country_label&quot;] != &quot;Other&quot;] # Base layer. base = ( alt.Chart(main_df) .mark_point(size = 300) # Set default size of points to 300 pixels. .encode( x = alt.X(&quot;number_customers:Q&quot;, title = &quot;Number of Customers&quot;), y = alt.Y(&quot;total_sales:Q&quot;, title = &quot;Total Sales (USD)&quot;), ) ) # Scatter plot layer. points = base.encode( color = alt.Color(&quot;avg_sales_customer:Q&quot;, title = &quot;Average Sales per Customer (USD)&quot;), tooltip = country_stats_df.columns.tolist(), ) # Text layer. text = ( base .mark_text( # Move text to the top left of each point. align = &quot;right&quot;, dy = -5, dx = -20, ) .encode( text = &quot;country_label:N&quot; ) ) # Combine layers. chart = ( (points + text) .properties( title = &quot;Chinook Sales by Country&quot;, height = 300, width = 700, ) .interactive() ) # Display chart. chart . . Looking at the top right of the chart, we can see that the USA has the highest number of Chinook customers (13), as well as the highest total sales (USD 1040.49). All of the other countries have only 2 to 8 customers. . On the other hand, the Czech Republic has the highest average sales per customer at USD 136.62; this is indicated by its dark color in the chart. This country also had the highest average order value at USD 9.11. This means that though the country has few customers, these people are avid buyers of music. . Chinook may benefit from marketing its service more aggressively in countries other than the USA where it has gained a good foothold, such as Canada, Brazil, and France. Chinook may also target the Czech Republic since the customers there seem to buy a lot of music on a per-person basis. . Comparing Purchase Types: Full Album vs Selected Sets . In our last scenario, we have been requested to compare the popularities of Chinook&#39;s two purchase types: . Full album The customer buys a full album. | Albums are pre-defined in Chinook&#39;s library. The customer may not add other tracks on top of an album. | . | Selected set of tracks The customer manually selects any number of individual tracks. | . | . In both cases, each track is bought at its unit price; there are no discounts. . Currently, Chinook&#39;s purchasing strategy is to buy full albums from record companies. However, Chinook doesn&#39;t know whether full album purchases are popular among its customers. If selected sets are more popular, then Chinook may switch to a new strategy in which it will only buy the most popular individual tracks from record companies. . Our analysis will help Chinook make the final decision. For each purchase type, we will show the number of invoices and percentage of all invoices with that type. The type with the higher number will be the more popular one. . First, we create a view, invoice_tracks, that shows all tracks under each invoice. The album associated with each track is also shown. . %%sql DROP VIEW IF EXISTS invoice_tracks_bought; CREATE VIEW invoice_tracks_bought AS SELECT iv.invoice_id AS invoice_id, il.track_id AS track_id, t.name AS track_name, t.album_id AS album_id, a.title AS album_name FROM invoice AS iv INNER JOIN invoice_line AS il ON il.invoice_id = iv.invoice_id INNER JOIN track AS t ON t.track_id = il.track_id INNER JOIN album AS a ON a.album_id = t.album_id ORDER BY invoice_id, track_id, album_id ; SELECT * FROM invoice_tracks_bought LIMIT 7 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . invoice_id track_id track_name album_id album_name . 1 | 1158 | Right Next Door to Hell | 91 | Use Your Illusion I | . 1 | 1159 | Dust N&amp;#x27; Bones | 91 | Use Your Illusion I | . 1 | 1160 | Live and Let Die | 91 | Use Your Illusion I | . 1 | 1161 | Don&amp;#x27;t Cry (Original) | 91 | Use Your Illusion I | . 1 | 1162 | Perfect Crime | 91 | Use Your Illusion I | . 1 | 1163 | You Ain&amp;#x27;t the First | 91 | Use Your Illusion I | . 1 | 1164 | Bad Obsession | 91 | Use Your Illusion I | . Then, we create a view called invoice_album, which shows only the album associated with the first track of each invoice. This way, there is only one row per invoice. We will also include the total purchase amount of each invoice so that we can use it later. . %%sql DROP VIEW IF EXISTS invoice_album; CREATE VIEW invoice_album AS SELECT itb.invoice_id AS invoice_id, MIN(itb.album_id) AS album_id, itb.album_name AS album_name, iv.total AS total_purchase FROM invoice_tracks_bought AS itb INNER JOIN invoice AS iv ON iv.invoice_id = itb.invoice_id GROUP BY itb.invoice_id ORDER BY itb.invoice_id ; SELECT * FROM invoice_album LIMIT 5 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . invoice_id album_id album_name total_purchase . 1 | 91 | Use Your Illusion I | 15.84 | . 2 | 20 | The Best Of Buddy Guy - The Millenium Collection | 9.9 | . 3 | 203 | A-Sides | 1.98 | . 4 | 58 | Come Taste The Band | 7.92 | . 5 | 163 | From The Muddy Banks Of The Wishkah [live] | 16.83 | . Then, we join invoice_album with the track table in order to list all of the tracks under the album associated with each invoice. The result will be a view called invoice_full_album_tracks. . %%sql DROP VIEW IF EXISTS invoice_full_album_tracks; CREATE VIEW invoice_full_album_tracks AS SELECT ia.*, t.track_id AS track_id, t.name AS track_name FROM invoice_album AS ia INNER JOIN track AS t ON t.album_id = ia.album_id ORDER BY invoice_id, album_id, track_id ; SELECT * FROM invoice_full_album_tracks LIMIT 5 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . invoice_id album_id album_name total_purchase track_id track_name . 1 | 91 | Use Your Illusion I | 15.84 | 1158 | Right Next Door to Hell | . 1 | 91 | Use Your Illusion I | 15.84 | 1159 | Dust N&amp;#x27; Bones | . 1 | 91 | Use Your Illusion I | 15.84 | 1160 | Live and Let Die | . 1 | 91 | Use Your Illusion I | 15.84 | 1161 | Don&amp;#x27;t Cry (Original) | . 1 | 91 | Use Your Illusion I | 15.84 | 1162 | Perfect Crime | . The invoice_full_album_tracks view looks very similar to the invoice_tracks_bought view at first glance. However, there is a difference: . invoice_tracks_bought It contains the tracks bought in each invoice. | Each set of tracks may or may not be a full album. | . | invoice_full_album_tracks It contains all of the tracks under one album associated with an invoice. | Some of these tracks may not have been bought by the customer. | . | . Next, we will create a new view called invoice_purchase_type which indicates whether each invoice is a &quot;Full Album&quot; or &quot;Selected Set&quot; purchase. . In order to determine this, we will have a CASE statement which can be explained as follows: . WHEN clause: If the set of tracks bought and the full album are exactly the same, mark the purchase type as &quot;Full Album&quot;. | ELSE clause: Otherwise, mark the purchase type as &quot;Selected Set&quot;. | . Inside the WHEN clause, we have a rather complicated-looking set of operations. Let&#39;s look at one part: . ( SELECT itb.track_id FROM invoice_tracks_bought AS itb WHERE itb.invoice_id = ia.invoice_id EXCEPT SELECT ifa.track_id FROM invoice_full_album_tracks AS ifa WHERE ifa.invoice_id = ia.invoice_id ) IS NULL . In order to make the explanation more simple, we can call the subqueries above &quot;set 1&quot; and &quot;set 2&quot;. . ( {set 1} EXCEPT {set 2} ) IS NULL . Set 1 represents all tracks bought in one invoice. | Set 2 represents the full set of tracks in an album associated with the invoice. | Via EXCEPT and IS NULL, we check whether Set 1 is a subset of Set 2. If it is, the result is True. | Otherwise, False. | . | . We then repeat the same process but in reverse, to check if Set 2 is a subset of Set 1. We thus end up with two boolean values, and we use the AND operator on these. . ( {set 1} EXCEPT {set 2} ) IS NULL AND ( {set 2} EXCEPT {set 1} ) IS NULL . The purpose of AND is to determine the following. . If both conditions are True: The two sets of tracks match exactly. | The invoice is a &quot;Full Album&quot; purchase. | . | If any condition is False: The two sets of tracks do not match exactly. | The invoice is a &quot;Selected Set&quot; purchase. | . | . The full query is shown below. . %%sql DROP VIEW IF EXISTS invoice_purchase_type; CREATE VIEW invoice_purchase_type AS SELECT ia.invoice_id, CASE WHEN ( ( SELECT itb.track_id FROM invoice_tracks_bought AS itb WHERE itb.invoice_id = ia.invoice_id EXCEPT SELECT ifa.track_id FROM invoice_full_album_tracks AS ifa WHERE ifa.invoice_id = ia.invoice_id ) IS NULL AND ( SELECT ifa.track_id FROM invoice_full_album_tracks AS ifa WHERE ifa.invoice_id = ia.invoice_id EXCEPT SELECT itb.track_id FROM invoice_tracks_bought AS itb WHERE itb.invoice_id = ia.invoice_id ) IS NULL ) THEN &quot;Full Album&quot; ELSE &quot;Selected Set&quot; END AS purchase_type, ia.total_purchase AS total_purchase FROM invoice_album AS ia ; SELECT * FROM invoice_purchase_type LIMIT 10 ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. Done. . invoice_id purchase_type total_purchase . 1 | Full Album | 15.84 | . 2 | Selected Set | 9.9 | . 3 | Selected Set | 1.98 | . 4 | Selected Set | 7.92 | . 5 | Full Album | 16.83 | . 6 | Selected Set | 1.98 | . 7 | Selected Set | 10.89 | . 8 | Selected Set | 9.9 | . 9 | Selected Set | 8.91 | . 10 | Selected Set | 1.98 | . With the view above, we can finally answer the question being asked. A view called purchase_type_proportion will be created which shows the following information per purchase type: . number of invoices | percentage of invoices | average sales per invoice | sales in USD | percentage of total sales | . %%sql DROP VIEW IF EXISTS purchase_type_proportion; CREATE VIEW purchase_type_proportion AS SELECT purchase_type, COUNT(purchase_type) AS type_count, ROUND( CAST(COUNT(purchase_type) AS Float) / CAST( (SELECT COUNT(purchase_type) FROM invoice_purchase_type) AS Float ) * 100, 2 ) AS type_percentage, ROUND( AVG(total_purchase), 2 ) AS avg_sales_per_invoice, ROUND( SUM(total_purchase), 2 ) AS sales_number, ROUND( SUM(total_purchase) / (SELECT SUM(total_purchase) FROM invoice_purchase_type) * 100, 2 ) AS sales_percentage FROM invoice_purchase_type GROUP BY purchase_type ORDER BY type_count DESC ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Done. . [] . %%sql result &lt;&lt; SELECT * FROM purchase_type_proportion ; . * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db Done. Returning data to local variable result . purchase_type_df = result.DataFrame() purchase_type_df . purchase_type type_count type_percentage avg_sales_per_invoice sales_number sales_percentage . 0 Selected Set | 500 | 81.43 | 6.50 | 3248.19 | 68.97 | . 1 Full Album | 114 | 18.57 | 12.82 | 1461.24 | 31.03 | . The type_percentage column shows that the majority (81%) of all Chinook invoices are Selected Sets. Full Album purchases only make up 19% of all invoices. . This is shown in the pie chart below. . colors = sns.color_palette(&#39;pastel&#39;)[:2] plt.pie( x = purchase_type_df[&quot;type_percentage&quot;], labels = purchase_type_df[&quot;purchase_type&quot;], colors = colors, autopct = &quot;%0.2f%%&quot;, ) plt.title(&quot;Proportion of Chinook Store Invoices by Purchase Type&quot;) plt.show() . . However, the average sales per Full Album purchase is almost twice that of a Selected Set purchase. This is shown in the bar chart below. . # Base layer with bar chart base = ( alt.Chart(purchase_type_df) .mark_bar() .encode( x = alt.X(&quot;purchase_type:N&quot;, title = &quot;Purchase Type&quot;), y = alt.Y(&quot;avg_sales_per_invoice:Q&quot;, title = &quot;Average Sales per Invoice (USD)&quot;), tooltip = purchase_type_df.columns.tolist(), ) ) # Text layer text = ( base .mark_text( align = &quot;center&quot;, dy = 10, color = &quot;white&quot;, ) .encode( text = &quot;avg_sales_per_invoice:Q&quot;, ) ) # Combine layers into one chart chart = ( (base + text) .properties( title = &quot;Average Sales per Invoice by Purchase Type&quot;, height = 300, width = 200, ) .configure_axis(labelAngle = 30) .interactive() ) # Display chart. chart . . Full Albums cost more than Selected Sets because the former tend to have more tracks. Thus, even if Full Albums only represent 19% of all invoices, these also represent 31% of all dollar sales. This is shown in the pie chart below. . plt.pie( x = purchase_type_df[&quot;sales_percentage&quot;], labels = purchase_type_df[&quot;purchase_type&quot;], colors = colors, autopct = &quot;%0.2f%%&quot;, ) plt.title(&quot;Proportion of Chinook Store Sales by Purchase Type&quot;) plt.show() . . Overall, though, Selected Sets still represent the majority of Chinook&#39;s invoices (81%) and sales (69%). Therefore, we recommend that Chinook shift to a new purchasing strategy in which it only buys the most popular tracks from record companies. Chinook should not buy full albums since it is less likely for customers to purchase these. . Conclusion . In this project, we used intermediate SQL techniques to answer business questions for a hypothetical digital media store called Chinook. We solved a total of 4 scenarios by creating views and gradually working towards a final query. We also ended each scenario with a chart that communicates our findings more engagingly. . Below is a summary of all of the questions and our findings. . . What are the best-selling music genres with regards to USA customers? Based on this, which new albums should be purchased for the Chinook store? . Rock is the best-selling music genre as it makes up 53% of total purchases in the USA. Rock is followed by the Alternative &amp; Punk genre and Metal genre, which each make up over 10% of purchases. . Among the 4 new albums which may be added to the digital store, we recommend the following 3 artists&#39; albums: Red Tone (Punk), Slim Jim Bites (Blues), and Meteor and the Girls (Pop). . . Which of Chinook&#39;s sales support agents has the highest total sales from their assigned customers? Can the exemplary performance of these employees be explained by any information in the database? . Sales support agent Jane Peacock has the highest total sales (37% of all sales) from her customers. She also has the highest average sales per customer (USD 82). . However, her statistics are only slightly higher than that of her colleagues. Therefore, we cannot conclusively say that she is the best-performing agent. . . What are the statistics on the customers and sales for each country where Chinook offers its service? . The USA has the highest number of Chinook customers (13), as well as the highest total sales (USD 1040.49). All of the other countries have only 2 to 8 customers. . We recommend marketing Chinook more aggressively in Canada, Brazil, and France since these countries have the highest number of customers after the USA. We also recommend expanding into the Czech Republic since it has a high average value of sales per customer. . . How many purchases are full albums, and how many are manually selected sets of tracks? Based on this, what strategy should Chinook adopt when buying new tracks from record companies? . Selected Sets account for 81% of all invoices and 69% of total sales. Therefore, these are more popular than Full Albums. . Thus, we recommend that Chinook buy only the most popular tracks from record companies. This would be more cost-effective than buying full albums. . . That concludes this project. Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/sql/sqlite/python/pandas/numpy/matplotlib/seaborn/altair/2021/07/31/Answering-Business-Questions-Online-Music-Store-SQL.html",
            "relUrl": "/sql/sqlite/python/pandas/numpy/matplotlib/seaborn/altair/2021/07/31/Answering-Business-Questions-Online-Music-Store-SQL.html",
            "date": " • Jul 31, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Using SQL to Query and Analyze a CIA Factbook Database",
            "content": "Unsplash | NASA Overview . The CIA World Factbook website is a collection of basic information on hundreds of countries and geopolitical entities. Each country is discussed with regards to a wide range of aspects, such as politics, security, socio-economics, and environment. . For this project, we have downloaded the factbook.db database file, which contains CIA World Factbook data compiled by Dataquest. The goal of the project is to use sqlite3 in Jupyter Notebook to query this database and perform basic exploratory data analysis. Specifically, SQL is used to produce summary statistics and data transformations, Pandas is used to perform further transformations, and Altair is used to create interactive charts. . . Note: I wrote this notebook by following a guided project on the Dataquest platform, specifically, the Analyzing CIA Factbook Data Using SQL Guided Project. The general project flow and research questions came from Dataquest. However, the text and code in this notebook are written by me unless stated otherwise. . Preparations . Install necessary packages. . import pandas as pd import numpy as np import altair as alt . Connect Jupyter Notebook to the database file using the SQLite flavor of SQL. . %load_ext sql %sql sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db . &#39;Connected: @private/2021-07-24-SQL-Fundamentals-Files/factbook.db&#39; . Exploratory Data Analysis . Database Tables . First, we inspect the tables in the database. SQL is used in a code cell via the %%sql magic. . %%sql SELECT * FROM sqlite_master WHERE type = &quot;table&quot;; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . type name tbl_name rootpage sql . table | sqlite_sequence | sqlite_sequence | 3 | CREATE TABLE sqlite_sequence(name,seq) | . table | facts | facts | 47 | CREATE TABLE &quot;facts&quot; (&quot;id&quot; INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, &quot;code&quot; varchar(255) NOT NULL, &quot;name&quot; varchar(255) NOT NULL, &quot;area&quot; integer, &quot;area_land&quot; integer, &quot;area_water&quot; integer, &quot;population&quot; integer, &quot;population_growth&quot; float, &quot;birth_rate&quot; float, &quot;death_rate&quot; float, &quot;migration_rate&quot; float) | . There are two tables in the database: sqlite_sequence and facts. The latter contains the data that we will analyze. The sql column above lists the columns in the facts table and their data types. The column names include &quot;area&quot;, &quot;population&quot;, &quot;population_growth&quot;, etc. which are the kind of information that we would expect to know about a country. . The facts Table . Let us inspect the first 5 rows of the facts table. . %%sql SELECT * FROM facts LIMIT 5 --#Limit the result to the first 5 rows. ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . id code name area area_land area_water population population_growth birth_rate death_rate migration_rate . 1 | af | Afghanistan | 652230 | 652230 | 0 | 32564342 | 2.32 | 38.57 | 13.89 | 1.51 | . 2 | al | Albania | 28748 | 27398 | 1350 | 3029278 | 0.3 | 12.92 | 6.58 | 3.3 | . 3 | ag | Algeria | 2381741 | 2381741 | 0 | 39542166 | 1.84 | 23.67 | 4.31 | 0.92 | . 4 | an | Andorra | 468 | 468 | 0 | 85580 | 0.12 | 8.13 | 6.96 | 0.0 | . 5 | ao | Angola | 1246700 | 1246700 | 0 | 19625353 | 2.78 | 38.78 | 11.49 | 0.46 | . Most of the column names are self-explanatory. The following are some additional information about the data according to Dataquest. . The areas are given in square kilometers ($ text{km}^2$) | population_growth is given as the percentage of increase of the total population per year. | birth_rate and death_rate are given as the number of people who are born/pass away per year per 1000 people. | . Summary Statistics on Population Size and Growth . Below, we calculate some basic summary statistics about the countries&#39; populations and population growths. We want to see which countries have the highest and lowest value of each variable. . %%sql SELECT MIN(population), MAX(population), MIN(population_growth), MAX(population_growth) FROM facts ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . MIN(population) MAX(population) MIN(population_growth) MAX(population_growth) . 0 | 7256490011 | 0.0 | 4.02 | . Interestingly, the minimum population size among all of the countries in the table is 0, meaning no people at all. This is surprising since one would expect a country to have many people. This may indicate an error in the data, so let&#39;s check which countries have 0 people. . %%sql SELECT * FROM facts WHERE population == 0 ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . id code name area area_land area_water population population_growth birth_rate death_rate migration_rate . 250 | ay | Antarctica | None | 280000 | None | 0 | None | None | None | None | . It turns out that it is not a country, but rather the continent of Antarctica, which is at the South Pole. It is too cold there for people to live comfortably, so of course its population size would be 0. . The maximum population size, on the other hand, is over 7 billion, which seems too large. Let&#39;s check which country this is. . %%sql SELECT * FROM facts WHERE population == ( SELECT MAX(population) --# Use a subquery to get the exact value of the maximum population size. FROM facts ) ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . id code name area area_land area_water population population_growth birth_rate death_rate migration_rate . 261 | xx | World | None | None | None | 7256490011 | 1.08 | 18.6 | 7.8 | None | . It turns out that 7 billion refers to the total population in the entire world, not one country, so there is no error in the data. The data seems to have been collected in the mid-2010s, since the world population reached 7.2 billion in 2013. . Revised Summary Statistics . We&#39;ve come to the realization that this database doesn&#39;t just include entries on individual countries but also on entire continents and the entire world. Therefore, if we want to find useful information, we should exclude the entries for Antarctica and the World. We can do this using SQL&#39;s NOT IN operator. . Let&#39;s run our initial query again with this in mind. . %%sql SELECT MIN(population), MAX(population), MIN(population_growth), MAX(population_growth) FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) --# Exclude World and Antarctica from the entries being used in computation. ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . MIN(population) MAX(population) MIN(population_growth) MAX(population_growth) . 48 | 1367485388 | 0.0 | 4.02 | . The population growth statistics are the same as before, but now, the population size statistics are different. . Interestingly, the minimum population size is still very small—only 48 people. Which country is this? . %%sql SELECT * FROM facts WHERE population == ( SELECT MIN(population) FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ) ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . id code name area area_land area_water population population_growth birth_rate death_rate migration_rate . 238 | pc | Pitcairn Islands | 47 | 47 | 0 | 48 | 0.0 | None | None | None | . The country with a population of 48 is the Pitcairn Islands, which, according to the World Factbook, is a colony of the UK. The islands are small (the query result above shows only 47 square kilometers of land), so there are few people. . On the other hand, the maximum population size was over 1.3 billion. Let&#39;s check which country this is. . %%sql SELECT * FROM facts WHERE population == ( SELECT MAX(population) FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ) ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . id code name area area_land area_water population population_growth birth_rate death_rate migration_rate . 37 | ch | China | 9596960 | 9326410 | 270550 | 1367485388 | 0.45 | 12.49 | 7.53 | 0.44 | . Of course, the country with the largest population would be China. I expected this before I ran the query; China is well-known for the one-child policy that it enacted from 1980 to 2015 in order to curb the growth of its huge population. . Now, let&#39;s investigate the minimum population growth, which was 0%. This means that the population wasn&#39;t increasing at all at that time. . %%sql SELECT * FROM facts WHERE population_growth == 0 AND name NOT IN (&quot;Antarctica&quot;, &quot;World&quot;) ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . id code name area area_land area_water population population_growth birth_rate death_rate migration_rate . 190 | vt | Holy See (Vatican City) | 0 | 0 | 0 | 842 | 0.0 | None | None | None | . 200 | ck | Cocos (Keeling) Islands | 14 | 14 | 0 | 596 | 0.0 | None | None | None | . 207 | gl | Greenland | 2166086 | 2166086 | None | 57733 | 0.0 | 14.48 | 8.49 | 5.98 | . 238 | pc | Pitcairn Islands | 47 | 47 | 0 | 48 | 0.0 | None | None | None | . Four countries have shown up in the results. One of these is the Pitcairn Islands which were mentioned earlier. This makes sense as they have few people to begin with, and the Factbook states that it is rare for outsiders to migrate there. . Interestingly, Vatican City is among the countries without population growth, but this makes sense too. It is a small independent state within Italy that serves as the authority of Roman Catholicism. . Finally, let&#39;s check the country with the highest population growth rate. . %%sql SELECT * FROM facts WHERE population_growth == ( SELECT MAX(population_growth) FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ) ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . id code name area area_land area_water population population_growth birth_rate death_rate migration_rate . 162 | od | South Sudan | 644329 | None | None | 12042910 | 4.02 | 36.91 | 8.18 | 11.47 | . The country with the highest population growth rate is South Sudan. Its population was increasing by 4.02% annually at the time that the data was collected. According to Hicks (2017), there was widespread famine in South Sudan, and its high population growth rendered the country&#39;s level of food production insufficient. . Population Raw Increase . Out of curiosity, we can investigate which countries may have the highest population raw increase. This means the increase in terms of the number of people as opposed to the percentage of the population. . We can calculate this in an SQL query by multiplying population size by population growth rate. However, the query result needs to be stored as a DataFrame so that it can be used in a chart. . First, we can store the result of an SQL query as a ResultSet object by using the &lt;&lt; operator, according to the ipython-sql README. . %%sql result &lt;&lt; SELECT name, population, population_growth, (population * population_growth / 100.0) AS &quot;population_raw_increase&quot; FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ORDER BY population_raw_increase DESC ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. Returning data to local variable result . Then, the ResultSet can be converted into a DataFrame via its .DataFrame() method. . pop_increase = result.DataFrame() print(pop_increase.shape) pop_increase.head(10) . (259, 4) . name population population_growth population_raw_increase . 0 India | 1.251696e+09 | 1.22 | 1.527069e+07 | . 1 China | 1.367485e+09 | 0.45 | 6.153684e+06 | . 2 Nigeria | 1.815621e+08 | 2.45 | 4.448270e+06 | . 3 Pakistan | 1.990858e+08 | 1.46 | 2.906653e+06 | . 4 Ethiopia | 9.946582e+07 | 2.89 | 2.874562e+06 | . 5 Bangladesh | 1.689577e+08 | 1.60 | 2.703324e+06 | . 6 United States | 3.213689e+08 | 0.78 | 2.506677e+06 | . 7 Indonesia | 2.559937e+08 | 0.92 | 2.355142e+06 | . 8 Congo, Democratic Republic of the | 7.937514e+07 | 2.45 | 1.944691e+06 | . 9 Philippines | 1.009984e+08 | 1.61 | 1.626074e+06 | . The DataFrame above shows the top 10 countries in terms of population raw increase, which is written in scientific notation. Notably, while most of the values have an exponent of 6, India&#39;s has an exponent of 7. This means that its population raw increase is a whole order of magnitude higher than that of the other countries. . We can communicate this result better by graphing population growth against population size, with population raw increase as a continuous grouping variable. . ( alt.Chart(pop_increase) .mark_point() .encode( x = alt.X(&quot;population:Q&quot;, title = &quot;Population Size&quot;), y = alt.Y(&quot;population_growth:Q&quot;, title = &quot;Population Growth Rate (% of Population)&quot;), color = alt.Color(&quot;population_raw_increase:Q&quot;, title = &quot;Population Raw Increase&quot;), tooltip = [ &quot;name&quot;, &quot;population&quot;, &quot;population_growth&quot;, &quot;population_raw_increase&quot;, ] ) .properties(title = &quot;Population Growth Rate against Population Size&quot;) .interactive() ) . . One can click, drag, and scroll on the chart above in order to explore its data points. Hover over a point in order to view extra information like the country&#39;s name. . In the case of the chart above, the population raw increase variable ranges from 0 people to 15 million people; this range is expressed with a color gradient. Looking at the data points, we can see that most of them are pale blue, indicating a low increase. India&#39;s point has the darkest blue color. . Hovering over India&#39;s point, we can see its tooltip, which states that the population growth rate of India is 1.22%. This is not the highest global population growth rate; we saw earlier that this is around 4%. However, India&#39;s extremely large population size (over 1 billion) leads it to have the highest raw population increase. . Population Density . Next, in this section, we will find the average population size and average land area among all countries. . %%sql SELECT AVG(population), AVG(area_land) FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . AVG(population) AVG(area_land) . 32377011.0125 | 523693.2 | . The average population size is over 32 million people, and the average land area is over 500 thousand square kilometers. . We can now investigate which countries have above-average population size and below-average land area. . %%sql SELECT name, population, population_growth, area_land FROM facts WHERE population &gt; ( SELECT AVG(population) FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ) AND area_land &lt; ( SELECT AVG(area_land) FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ) AND name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ORDER BY population_growth DESC ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. . name population population_growth area_land . Uganda | 37101745 | 3.24 | 197100 | . Iraq | 37056169 | 2.93 | 437367 | . Philippines | 100998376 | 1.61 | 298170 | . Bangladesh | 168957745 | 1.6 | 130170 | . Morocco | 33322699 | 1.0 | 446300 | . Vietnam | 94348835 | 0.97 | 310070 | . Spain | 48146134 | 0.89 | 498980 | . United Kingdom | 64088222 | 0.54 | 241930 | . Thailand | 67976405 | 0.34 | 510890 | . Italy | 61855120 | 0.27 | 294140 | . Germany | 80854408 | 0.17 | 348672 | . Japan | 126919659 | 0.16 | 364485 | . Korea, South | 49115196 | 0.14 | 96920 | . Poland | 38562189 | 0.09 | 304255 | . Since the countries in this table have above-average population size and below-average land area, these may be at risk of facing high population density in the future. This may be especially true for those with the highest population growth rates, such as Uganda, Iraq, and the Philippines. . If we want to know which countries already have a high population density, though, we can run the query below. It calculates the population density as population size over land area, and then it displays the countries with the top 10 highest population densities. . %%sql result &lt;&lt; SELECT name, population, population_growth, area_land, population / area_land AS &quot;population_density&quot; FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ORDER BY population_density DESC ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. Returning data to local variable result . The resulting DataFrame is shown below. We have added 2 additional columns which will help us in making a chart: . place: Whether the country is in the top 10 countries in terms of population density. | text: The text to display for each country. | . pop_density = result.DataFrame() # Place variable pop_density[&quot;place&quot;] = [&quot;Top 10&quot;] * 10 + [&quot;Below Top 10&quot;] * (len(pop_density) - 10) # Text variable pop_density[&quot;text&quot;] = ( pop_density[&quot;name&quot;].copy() # Append text that indicates the numeric place of the country. .str.cat([&quot; (Top {})&quot;.format(i) for i in range(1, len(pop_density) + 1)]) ) # Make the text blank for the countries below the top 10. pop_density.loc[ pop_density[&quot;place&quot;] == &quot;Below Top 10&quot;, &quot;text&quot; ] = &quot;&quot; pop_density.head(11) . name population population_growth area_land population_density place text . 0 Macau | 592731.0 | 0.80 | 28.0 | 21168.0 | Top 10 | Macau (Top 1) | . 1 Monaco | 30535.0 | 0.12 | 2.0 | 15267.0 | Top 10 | Monaco (Top 2) | . 2 Singapore | 5674472.0 | 1.89 | 687.0 | 8259.0 | Top 10 | Singapore (Top 3) | . 3 Hong Kong | 7141106.0 | 0.38 | 1073.0 | 6655.0 | Top 10 | Hong Kong (Top 4) | . 4 Gaza Strip | 1869055.0 | 2.81 | 360.0 | 5191.0 | Top 10 | Gaza Strip (Top 5) | . 5 Gibraltar | 29258.0 | 0.24 | 6.0 | 4876.0 | Top 10 | Gibraltar (Top 6) | . 6 Bahrain | 1346613.0 | 2.41 | 760.0 | 1771.0 | Top 10 | Bahrain (Top 7) | . 7 Maldives | 393253.0 | 0.08 | 298.0 | 1319.0 | Top 10 | Maldives (Top 8) | . 8 Malta | 413965.0 | 0.31 | 316.0 | 1310.0 | Top 10 | Malta (Top 9) | . 9 Bermuda | 70196.0 | 0.50 | 54.0 | 1299.0 | Top 10 | Bermuda (Top 10) | . 10 Bangladesh | 168957745.0 | 1.60 | 130170.0 | 1297.0 | Below Top 10 | | . The results above show the top 10 countries in terms of population density in people per square kilometer of land. Notably, Monaco and Gibraltar are cases of countries with under 10 square kilometers of land but also tens of thousands of people. On the other hand, Singapore and Hong Kong are cases of countries with a somewhat larger land area but huge population size due to urbanization. Hong Kong has the highest number of over 150 m tall buildings in the world, and Singapore has the 12th highest. . Jackson (2017) states that high population density in a place can lead to noise pollution, territorial behavior, and lack of personal space for the people there. Thus, these may be significant challenges for the countries listed above. . Continuing on, we can take our analysis a step further by investigating countries with both high population density and high population growth rate. Such countries may be at risk of further aggravation of population density issues. . # Base layer with x and y axes base = ( alt.Chart(pop_density) .mark_point() .encode( x = alt.X( &quot;area_land:Q&quot;, title = &quot;Land Area (Square Kilometers)&quot;, scale = alt.Scale(domain = (0, 1100)), # Limit the x and y axis domains. # We will zoom in on the part of the chart that shows the important points. ), y = alt.Y( &quot;population:Q&quot;, title = &quot;Population Size&quot;, scale = alt.Scale(domain = (0, 6000000)), ), ) ) # Scatter plot layer points = ( base .encode( # Express groupings using color and size. color = alt.Color(&quot;place:N&quot;, title = &quot;Category of Population Density&quot;), size = alt.Size(&quot;population_growth:Q&quot;, title = &quot;Population Growth Rate (% of Population)&quot;), # Provide a tooltip with extra information. tooltip = [ &quot;name&quot;, &quot;area_land&quot;, &quot;population&quot;, &quot;population_density&quot;, &quot;population_growth&quot;, ], ) ) # Text layer text = ( base .mark_text( align = &quot;center&quot;, baseline = &quot;middle&quot;, dx = 60, # Nudge text to the right. ) .encode( # Display the text variable. text = &quot;text&quot;, ) ) # Combine layers. chart = ( (points + text) .properties(title = &quot;Top 10 Countries in Population Density&quot;) .interactive() ) # Display chart. chart . . In the chart above, the orange points represent the top 10 countries in terms of population density, while the blue points represent the other countries. Larger points have a higher population growth rate. . Macau has the highest population density. However, Gaza Strip, Bahrain, and Singapore have higher population growth rates than Macau does (as indicated by the larger circles). This indicates that these countries&#39; population sizes may increase quickly, thus leading to increased population density and aggravating the issues that come with it. . Death Rate and Birth Rate . Our last query in SQL shall investigate which countries have a higher death rate than birth rate. Such countries may face a decrease in their population size. . Note that we will calculate a new variable, death_offset_birth, by subtracting birth rate from death rate. This variable is indicative of the net decrease in population (if only births and deaths are taken into account). . %%sql result &lt;&lt; SELECT name, population, population_growth, birth_rate, death_rate, death_rate - birth_rate AS &quot;death_offset_birth&quot; FROM facts WHERE name NOT IN (&quot;World&quot;, &quot;Antarctica&quot;) ORDER BY death_offset_birth DESC ; . * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db Done. Returning data to local variable result . The result of the query is shown below. Additionally, we have created a new column, higher_category. This indicates whether the death rate or the birth rate is higher for a particular country. . db_rates = result.DataFrame() db_rates[&quot;higher_category&quot;] = ( db_rates[&quot;death_offset_birth&quot;] .apply(lambda x: &quot;Death Rate Higher&quot; if x &gt; 0 else &quot;Birth Rate Higher&quot;) ) db_rates.head(10) . name population population_growth birth_rate death_rate death_offset_birth higher_category . 0 Bulgaria | 7186893.0 | 0.58 | 8.92 | 14.44 | 5.52 | Death Rate Higher | . 1 Serbia | 7176794.0 | 0.46 | 9.08 | 13.66 | 4.58 | Death Rate Higher | . 2 Latvia | 1986705.0 | 1.06 | 10.00 | 14.31 | 4.31 | Death Rate Higher | . 3 Lithuania | 2884433.0 | 1.04 | 10.10 | 14.27 | 4.17 | Death Rate Higher | . 4 Ukraine | 44429471.0 | 0.60 | 10.72 | 14.46 | 3.74 | Death Rate Higher | . 5 Hungary | 9897541.0 | 0.22 | 9.16 | 12.73 | 3.57 | Death Rate Higher | . 6 Germany | 80854408.0 | 0.17 | 8.47 | 11.42 | 2.95 | Death Rate Higher | . 7 Slovenia | 1983412.0 | 0.26 | 8.42 | 11.37 | 2.95 | Death Rate Higher | . 8 Romania | 21666350.0 | 0.30 | 9.14 | 11.90 | 2.76 | Death Rate Higher | . 9 Croatia | 4464844.0 | 0.13 | 9.45 | 12.18 | 2.73 | Death Rate Higher | . The results above show the top 10 countries in terms of the difference between death rate and birth rate. At the top is Bulgaria; for every 1000 people each year, 14.44 die and 8.92 are born, meaning that deaths outnumber births by 5.52. . We can make an area chart in order to get an idea of the death rate situation for all countries. . Before that, though, how many countries have a death rate that is higher than the birth rate? . (db_rates[&quot;higher_category&quot;] == &quot;Death Rate Higher&quot;).sum() . 24 . There are 24 such countries. Thus, we can mention this fact in the chart title. . The chart is shown below. . ( alt.Chart(db_rates) .mark_area(point = True) # Show a point for each observation. .encode( x = alt.X(&quot;birth_rate:Q&quot;, title = &quot;Birth Rate (Births per 1000 People per Year)&quot;), y = alt.Y(&quot;death_offset_birth:Q&quot;, title = &quot;Death Rate offset by Birth Rate&quot;), color = alt.Color(&quot;higher_category:N&quot;, title = &quot;Higher Rate&quot;), tooltip = [ &quot;name&quot;, &quot;birth_rate&quot;, &quot;death_rate&quot;, &quot;death_offset_birth&quot;, ], ) .properties(title = &quot;Death Rate is Higher than Birth Rate for 24 Countries&quot;) .interactive() ) . . In the chart above, each point represents a country. Click, drag, and scroll in order to explore the countries and read their tooltips. . The horizontal line marks the spot where the birth rate and death rate are equal—they cancel each other out. The blue part represents the majority of countries, which have a birth rate higher than the death rate. On the other hand, the orange part represents the 24 countries where deaths outnumber births. . Interestingly, there is a downward trend in the chart. Death rate offset by birth rate appears to have a strong negative correlation with birth rate. . More importantly, one would expect the 24 highlighted countries to have a negative population growth. However, if we look at the table, the population growth rates are still positive. This means that other factors such as migration may be involved in keeping the population growing rather than shrinking. . Summary . In this project, we used a local database file of CIA World Factbook data about 259 countries. We explored the topics of population size, growth rate, raw increase, and density, as well as death rate and birth rate. We queried the database with SQL in order to calculate summary statistics and generate tables showing the top countries with certain characteristics. After identifying countries of interest, we did a little research on each one in order to understand why it had such characteristics. Lastly, we created interesting interactive charts that highlighted the countries of interest. These charts allowed the viewer to read extra information about each country via tooltips. . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/sql/sqlite/python/pandas/numpy/altair/2021/07/24/Using-SQL-Query-Analyze-CIA-Factbook-Database.html",
            "relUrl": "/sql/sqlite/python/pandas/numpy/altair/2021/07/24/Using-SQL-Query-Analyze-CIA-Factbook-Database.html",
            "date": " • Jul 24, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "How Student Demographic Affects SAT Performance in NYC",
            "content": "Unsplash | Nguyen Dang Hoang Nhu Overview . Updated on July 21, 2021. . The SAT, or Scholastic Assessment Test, is taken by many high school students in the USA and around the world before college applications. SAT scores are one of the kinds of information used by colleges to determine a student&#39;s level of college readiness. . However, the SAT has faced controversy due to analyses which have shown that students&#39; demographics - such as sex, race, socio-economic status - may have influenced their SAT performance. . Thus, the goal of this project is to determine the ways in which New York City high school students&#39; demographics may affect SAT performance. Towards this end, we have: . Downloaded various datasets on NYC high schools from NYC Open Data | Cleaned, transformed, and combined these datasets into one DataFrame | Conducted exploratory data analysis and visualizations | Performed multiple linear regression | . The final regression model explained 78.6% of the variance (R2= 0.786, F(6, 394) = 96.44, p &lt; 0.01). The percentage of each race in a school significantly predicted each school&#39;s average SAT score. . . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Analyzing NYC High School Data. The general project flow and research questions came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm import statsmodels.stats.api as sms import re . Data Transformation and Cleaning . In this section, we transform some columns in each dataset in order to get useful values. Then, we combine all of the datasets into one DataFrame. Some data cleaning will also be performed throughout the process. This is necessary in order to perform analysis using all of the columns available. . List of Datasets . The following are the datasets that were downloaded from NYC Open Data for this project. The files were renamed to have shorter names. . sat_results.csv: 2012 SAT results (link) | hs_directory: 2010 - 2011 School Attendance and Enrollment Statistics by District (link) | class_size.csv: 2010-2011 Class Size - School-level detail (link) | ap_2010.csv: 2010 AP (College Board) School Level Results (link) | graduation.csv: 2005-2010 Graduation Outcomes - School Level (link) | demographics.csv: 2006 - 2012 School Demographics and Accountability Snapshot (link) | survey_all.txt, survey_d75.txt: 2011 NYC School Survey (link) | . The code below reads the CSVs into the program. . data_files = [ &quot;ap_2010.csv&quot;, &quot;class_size.csv&quot;, &quot;demographics.csv&quot;, &quot;graduation.csv&quot;, &quot;hs_directory.csv&quot;, &quot;sat_results.csv&quot; ] data = {} for filename in data_files: key = filename[:-4] # Remove &quot;.csv&quot; df = pd.read_csv(&quot;./private/2021-06-13-HSD-Files/&quot; + filename) # Rename &quot;dbn&quot; to &quot;DBN&quot;. df = df.rename( {&quot;dbn&quot;: &quot;DBN&quot;}, axis = 1, ) # Assign the DataFrame to a key in the dictionary. data[key] = df . As for the TXT files, these are tab-delimited and encoded using Windows-1252 encoding, so we have to specify that when we read them in. We will also concatenate these two datasets vertically because these have the same column names. . all_survey = pd.read_csv( &quot;./private/2021-06-13-HSD-Files/survey_all.txt&quot;, delimiter = &quot; t&quot;, encoding = &quot;windows-1252&quot;, ) # Survey data on all District 75 schools. d75_survey = pd.read_csv( &quot;./private/2021-06-13-HSD-Files/survey_d75.txt&quot;, delimiter = &quot; t&quot;, encoding = &quot;windows-1252&quot;, ) # Concatenate the two datasets vertically into one dataset. survey = pd.concat( [all_survey, d75_survey], axis = 0, ) # Rename &quot;dbn&quot; column to &quot;DBN&quot; for consistency. survey = survey.rename( {&quot;dbn&quot;: &quot;DBN&quot;}, axis = 1, ) # Put the DataFrame into the main dictionary. data[&quot;survey&quot;] = survey . Now, all of the datasets can be accessed via the data Series. . data = pd.Series(data) list(data.index) . [&#39;ap_2010&#39;, &#39;class_size&#39;, &#39;demographics&#39;, &#39;graduation&#39;, &#39;hs_directory&#39;, &#39;sat_results&#39;, &#39;survey&#39;] . To give an idea of what these datasets look like, we show the first few rows of the SAT results dataset below. . Each row contains data on one school. | Each column provides a different detail about the schools, such as school name, number of test takers in each school, etc. | . data.sat_results.head() . DBN SCHOOL NAME Num of SAT Test Takers SAT Critical Reading Avg. Score SAT Math Avg. Score SAT Writing Avg. Score . 0 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES | 29 | 355 | 404 | 363 | . 1 01M448 | UNIVERSITY NEIGHBORHOOD HIGH SCHOOL | 91 | 383 | 423 | 366 | . 2 01M450 | EAST SIDE COMMUNITY SCHOOL | 70 | 377 | 402 | 370 | . 3 01M458 | FORSYTH SATELLITE ACADEMY | 7 | 414 | 401 | 359 | . 4 01M509 | MARTA VALLE HIGH SCHOOL | 44 | 390 | 433 | 384 | . SAT Total Scores . The SAT results dataset contains columns showing the average score in each section of the test. . However, there is no column representing the average SAT total score. This is important to know because it gauges a student&#39;s overall mastery of high school knowledge and college preparedness. . Thus, we will generate a total score column by taking the sum of the three section score columns. . score_cols = [ &quot;SAT Math Avg. Score&quot;, &quot;SAT Critical Reading Avg. Score&quot;, &quot;SAT Writing Avg. Score&quot;, ] # Convert each column from string to numeric. for col in score_cols: data.sat_results[col] = pd.to_numeric( data.sat_results[col], errors = &quot;coerce&quot;, # Set invalid parsing as NaN. ) # Get the total scores by summing the 3 section scores. data.sat_results[&quot;sat_score&quot;] = ( data.sat_results[score_cols] .sum( axis = 1, skipna = False, # Return NaN if the row has a NaN. ) ) # Drop rows with missing values. data.sat_results.dropna(inplace = True) data.sat_results[score_cols + [&quot;sat_score&quot;]].head() . SAT Math Avg. Score SAT Critical Reading Avg. Score SAT Writing Avg. Score sat_score . 0 404.0 | 355.0 | 363.0 | 1122.0 | . 1 423.0 | 383.0 | 366.0 | 1172.0 | . 2 402.0 | 377.0 | 370.0 | 1149.0 | . 3 401.0 | 414.0 | 359.0 | 1174.0 | . 4 433.0 | 390.0 | 384.0 | 1207.0 | . The sat_score column now shows each school&#39;s average SAT total score. . School Location . Another piece of information we want to find is the set of geographical coordinates of each NYC school. We can find this using the hs_directory dataset, which has a Location 1 column. . data.hs_directory[[&quot;DBN&quot;, &quot;school_name&quot;, &quot;Location 1&quot;]].head() . DBN school_name Location 1 . 0 17K548 | Brooklyn School for Music &amp; Theatre | 883 Classon Avenue nBrooklyn, NY 11225 n(40.67... | . 1 09X543 | High School for Violin and Dance | 1110 Boston Road nBronx, NY 10456 n(40.8276026... | . 2 09X327 | Comprehensive Model School Project M.S. 327 | 1501 Jerome Avenue nBronx, NY 10452 n(40.84241... | . 3 02M280 | Manhattan Early College School for Advertising | 411 Pearl Street nNew York, NY 10038 n(40.7106... | . 4 28Q680 | Queens Gateway to Health Sciences Secondary Sc... | 160-20 Goethals Avenue nJamaica, NY 11432 n(40... | . Let&#39;s inspect the first value in this column to see its format. . print(data.hs_directory.loc[0, &quot;Location 1&quot;]) . 883 Classon Avenue Brooklyn, NY 11225 (40.67029890700047, -73.96164787599963) . Each value is a string with three lines. The third line contains the school&#39;s latitude and longitude in parentheses. . We can extract the coordinate data using the regular expression ((.+) ). This matches a pair of parentheses containing 1 or more characters between them. . def get_coords(text, which = 0): &quot;&quot;&quot;Take a string, extract coordinates, and return one of the values.&quot;&quot;&quot; pattern = r&quot; ((.+) )&quot; # Regex extracted = re.findall(pattern, text) # Returns a list of extracted strings coords = extracted[0].split(&quot;, &quot;) # Split string into list of strings # Return one of the coordinates as a float64. result = np.float64(coords[which]) return result # Make latitude and longitude columns. data.hs_directory[&quot;lat&quot;] = ( data.hs_directory[&quot;Location 1&quot;] .apply(get_coords, which = 0) ) data.hs_directory[&quot;lon&quot;] = ( data.hs_directory[&quot;Location 1&quot;] .apply(get_coords, which = 1) ) data.hs_directory[[&quot;lat&quot;, &quot;lon&quot;]].head() . lat lon . 0 40.670299 | -73.961648 | . 1 40.827603 | -73.904475 | . 2 40.842414 | -73.916162 | . 3 40.710679 | -74.000807 | . 4 40.718810 | -73.806500 | . We have successfully extracted the latitude and longitude from each string. . Survey Dataset Columns . Another issue is that there are too many columns in the survey dataset: . data.survey.shape . (1702, 2773) . There are 2773 columns. The reason is that each column represents a different survey item or option. It appears that the survey had many items. . However, if we look at the data dictionary, we can see that there are only a few very important columns: . important_fields = [ # Unique DBN of each school &quot;DBN&quot;, # Response rates &quot;rr_s&quot;, &quot;rr_t&quot;, &quot;rr_p&quot;, &quot;N_s&quot;, &quot;N_t&quot;, &quot;N_p&quot;, # Scores about various aspects of the school experience &quot;saf_p_11&quot;, &quot;com_p_11&quot;, &quot;eng_p_11&quot;, &quot;aca_p_11&quot;, &quot;saf_t_11&quot;, &quot;com_t_11&quot;, &quot;eng_t_11&quot;, &quot;aca_t_11&quot;, &quot;saf_s_11&quot;, &quot;com_s_11&quot;, &quot;eng_s_11&quot;, &quot;aca_s_11&quot;, &quot;saf_tot_11&quot;, &quot;com_tot_11&quot;, &quot;eng_tot_11&quot;, &quot;aca_tot_11&quot;, ] . Thus, we will only keep these columns in survey and remove the rest. . data.survey = data.survey[important_fields] . DBN Column . Earlier, we saw that the first column in the sat_results dataset was DBN. . data.sat_results.head() . DBN SCHOOL NAME Num of SAT Test Takers SAT Critical Reading Avg. Score SAT Math Avg. Score SAT Writing Avg. Score sat_score . 0 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES | 29 | 355.0 | 404.0 | 363.0 | 1122.0 | . 1 01M448 | UNIVERSITY NEIGHBORHOOD HIGH SCHOOL | 91 | 383.0 | 423.0 | 366.0 | 1172.0 | . 2 01M450 | EAST SIDE COMMUNITY SCHOOL | 70 | 377.0 | 402.0 | 370.0 | 1149.0 | . 3 01M458 | FORSYTH SATELLITE ACADEMY | 7 | 414.0 | 401.0 | 359.0 | 1174.0 | . 4 01M509 | MARTA VALLE HIGH SCHOOL | 44 | 390.0 | 433.0 | 384.0 | 1207.0 | . The DBN, or District Borough Number, is a unique code that identifies each school in NYC. . Since it is unique to each school, we can use it to match rows across our datasets and combine them into one. However, the class_size dataset doesn&#39;t have a DBN column. . data.class_size.head() . CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO . 0 1 | M | M015 | P.S. 015 Roberto Clemente | 0K | GEN ED | - | - | - | 19.0 | 1.0 | 19.0 | 19.0 | 19.0 | ATS | NaN | . 1 1 | M | M015 | P.S. 015 Roberto Clemente | 0K | CTT | - | - | - | 21.0 | 1.0 | 21.0 | 21.0 | 21.0 | ATS | NaN | . 2 1 | M | M015 | P.S. 015 Roberto Clemente | 01 | GEN ED | - | - | - | 17.0 | 1.0 | 17.0 | 17.0 | 17.0 | ATS | NaN | . 3 1 | M | M015 | P.S. 015 Roberto Clemente | 01 | CTT | - | - | - | 17.0 | 1.0 | 17.0 | 17.0 | 17.0 | ATS | NaN | . 4 1 | M | M015 | P.S. 015 Roberto Clemente | 02 | GEN ED | - | - | - | 15.0 | 1.0 | 15.0 | 15.0 | 15.0 | ATS | NaN | . By comparing the sat_results dataset to the class_size dataset, we can see that the DBN is actually a combination of: . the CSD (Community School District), like &quot;01&quot;. | the School Code, like &quot;M015&quot;. | . The CSD must have two digits, so single-digit CSDs need to be padded with a zero in front. This is done below. . def zero_pad(num): &quot;&quot;&quot;Zero-pad a number if it only has 1 digit.&quot;&quot;&quot; digits = len(str(num)) if digits == 1: zero_padded = str(num).zfill(2) return zero_padded elif digits == 2: return str(num) else: raise ValueError(&quot;Invalid number of digits.&quot;) data.class_size[&quot;padded_csd&quot;] = data.class_size[&quot;CSD&quot;].apply(zero_pad) data.class_size[[&quot;CSD&quot;, &quot;padded_csd&quot;, &quot;SCHOOL CODE&quot;]].head() . CSD padded_csd SCHOOL CODE . 0 1 | 01 | M015 | . 1 1 | 01 | M015 | . 2 1 | 01 | M015 | . 3 1 | 01 | M015 | . 4 1 | 01 | M015 | . Now, we can combine padded_csd and SCHOOL CODE to make the DBN. . data.class_size[&quot;DBN&quot;] = ( data.class_size[[&quot;padded_csd&quot;, &quot;SCHOOL CODE&quot;]] .sum(axis = 1) # Add values along rows. ) # Reorder the df so that DBN is in front. def reorder_columns(df, first_cols): &quot;&quot;&quot;Take a DataFrame and a list of columns. Move those columns to the left side. Return the new DataFrame.&quot;&quot;&quot; result = df[ first_cols + [col for col in df if col not in first_cols] ] return result data.class_size = reorder_columns(data.class_size, [&quot;DBN&quot;]) # Remove padded_csd column data.class_size.drop( &quot;padded_csd&quot;, axis = 1, inplace = True, ) data.class_size.head() . DBN CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO . 0 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 0K | GEN ED | - | - | - | 19.0 | 1.0 | 19.0 | 19.0 | 19.0 | ATS | NaN | . 1 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 0K | CTT | - | - | - | 21.0 | 1.0 | 21.0 | 21.0 | 21.0 | ATS | NaN | . 2 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 01 | GEN ED | - | - | - | 17.0 | 1.0 | 17.0 | 17.0 | 17.0 | ATS | NaN | . 3 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 01 | CTT | - | - | - | 17.0 | 1.0 | 17.0 | 17.0 | 17.0 | ATS | NaN | . 4 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 02 | GEN ED | - | - | - | 15.0 | 1.0 | 15.0 | 15.0 | 15.0 | ATS | NaN | . We were successful in creating a DBN column for the class size dataset. . Condensing Datasets . Another issue that needs to be addressed is that some datasets have multiple rows with the same DBN. For example, look at the frequency table of DBN values in class_size. . data.class_size[&quot;DBN&quot;].value_counts() . 15K429 57 09X505 56 09X517 56 21K690 52 15K448 52 .. 27Q273 3 03M452 3 09X090 2 27Q465 2 02M267 2 Name: DBN, Length: 1487, dtype: int64 . Each DBN appears at least twice in the dataset. This would be problematic when merging it with other datasets, since each row must have a unique DBN value. . Therefore, we need to condense class_size and other such datasets so that the DBN value is unique to each row. . The code below identifies which columns need to be condensed based on the frequency of its DBN values. . [ df_name for df_name in data.index if data[df_name][&quot;DBN&quot;].value_counts().max() &gt; 1 # Check if any DBN values appear more than once. ] . [&#39;ap_2010&#39;, &#39;class_size&#39;, &#39;demographics&#39;, &#39;graduation&#39;] . The 4 datasets listed above will be condensed one by one. . Advanced Placement Dataset . First, the ap_2010 dataset will be condensed. It contains data on AP (Advanced Placement) test results in 2010. . print(data.ap_2010.shape) data.ap_2010.head() . (258, 5) . DBN SchoolName AP Test Takers Total Exams Taken Number of Exams with scores 3 4 or 5 . 0 01M448 | UNIVERSITY NEIGHBORHOOD H.S. | 39 | 49 | 10 | . 1 01M450 | EAST SIDE COMMUNITY HS | 19 | 21 | s | . 2 01M515 | LOWER EASTSIDE PREP | 24 | 26 | 24 | . 3 01M539 | NEW EXPLORATIONS SCI,TECH,MATH | 255 | 377 | 191 | . 4 02M296 | High School of Hospitality Management | s | s | s | . From the first five rows, it is not immediately clear why DBN values repeat. The DBNs are unique in this sample. . Let&#39;s check the frequency table of DBN values. . data.ap_2010[&quot;DBN&quot;].value_counts() . 04M610 2 01M448 1 19K507 1 17K528 1 17K537 1 .. 09X329 1 09X365 1 09X412 1 09X414 1 32K556 1 Name: DBN, Length: 257, dtype: int64 . This frequency table is ordered by frequency, descending. Thus, we can tell that 04M610 is the only DBN that repeats in this dataset. Let&#39;s inspect the 2 rows with this DBN. . data.ap_2010.loc[ data.ap_2010[&quot;DBN&quot;] == &quot;04M610&quot; ] . DBN SchoolName AP Test Takers Total Exams Taken Number of Exams with scores 3 4 or 5 . 51 04M610 | THE YOUNG WOMEN&#39;S LEADERSHIP SCHOOL OF EAST HA... | 41 | 55 | 29 | . 52 04M610 | YOUNG WOMEN&#39;S LEADERSHIP SCH | s | s | s | . It looks like the duplication was caused by a simple error in data entry. Row 51 is valid, whereas row 52 has an incomplete school name. It also has a string &quot;s&quot; in numerical columns; this is invalid data. . Thus, we will drop row 52 from the dataset. . data.ap_2010.drop(52, inplace = True) data.ap_2010[&quot;DBN&quot;].value_counts().max() . 1 . Now that we&#39;ve dropped the inaccurate row, none of the DBN values repeat. . As a side note, since there were &quot;s&quot; strings in the numerical columns, we realize that these are object-type columns. Thus, we have to convert these into numerical columns in order to perform analysis on them later on. . cols = [&#39;AP Test Takers &#39;, &#39;Total Exams Taken&#39;, &#39;Number of Exams with scores 3 4 or 5&#39;] for col in cols: # Convert each column to numerical format. data.ap_2010[col] = pd.to_numeric( data.ap_2010[col], errors = &quot;coerce&quot;, # In case of errors, use NaN values. ) data.ap_2010.dtypes . DBN object SchoolName object AP Test Takers float64 Total Exams Taken float64 Number of Exams with scores 3 4 or 5 float64 dtype: object . The numerical columns were successfully converted into float64 (decimal) format. . Class Size Dataset . Next, we will condense the class_size dataset. It contains aggregated data on the number of students in classes in NYC schools. . Why are there rows with duplicate DBNs? Let&#39;s look at the first few rows. . print(data.class_size.shape) data.class_size.head() . (27611, 17) . DBN CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO . 0 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 0K | GEN ED | - | - | - | 19.0 | 1.0 | 19.0 | 19.0 | 19.0 | ATS | NaN | . 1 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 0K | CTT | - | - | - | 21.0 | 1.0 | 21.0 | 21.0 | 21.0 | ATS | NaN | . 2 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 01 | GEN ED | - | - | - | 17.0 | 1.0 | 17.0 | 17.0 | 17.0 | ATS | NaN | . 3 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 01 | CTT | - | - | - | 17.0 | 1.0 | 17.0 | 17.0 | 17.0 | ATS | NaN | . 4 01M015 | 1 | M | M015 | P.S. 015 Roberto Clemente | 02 | GEN ED | - | - | - | 15.0 | 1.0 | 15.0 | 15.0 | 15.0 | ATS | NaN | . Look at the GRADE and PROGRAM TYPE columns. Each school&#39;s data is split by grade level, from K to 12. The data are further split by the program type (GEN ED, CTT, etc.). That&#39;s why each DBN has multiple rows. . In our case, we are only concerned with high school students&#39; data (grades 9 to 12) since the SAT is taken at this level of education. Let&#39;s see which GRADE value/s correspond to high school. . data.class_size[&quot;GRADE &quot;].value_counts().sort_index() . 01 1185 02 1167 03 1143 04 1140 05 1086 06 846 07 778 08 735 09 20 09-12 10644 0K 1237 0K-09 1384 MS Core 4762 Name: GRADE , dtype: int64 . Conveniently, there is a 09-12 value. Rows with this value would contain aggregated data on all high school grade levels. Thus, we will filter the class_size dataset to keep only these rows. . There is still the issue of the PROGRAM TYPE column, however. Let&#39;s look at its frequency table. . data.class_size[&quot;PROGRAM TYPE&quot;].value_counts() . GEN ED 14545 CTT 7460 SPEC ED 3653 G&amp;T 469 Name: PROGRAM TYPE, dtype: int64 . There are hundreds or thousands of rows under each program type. According to the data dictionary, the following are the meanings of these values: . GEN ED: General Education | CTT: Collaborative Team Teaching | SPEC ED: Special Education | G&amp;T: Gifted &amp; Talented | . The General Education program best represents the majority of high school students, and this is the most frequent program in the data. Thus, we will keep the GEN ED rows and drop the rest. . To recap, we chose to filter the dataset to keep rows where the grade level is from 9 to 12 and the program type is General Education. We will do this using the code below. . data.class_size = data.class_size.loc[ (data.class_size[&quot;GRADE &quot;] == &quot;09-12&quot;) &amp; (data.class_size[&quot;PROGRAM TYPE&quot;] == &quot;GEN ED&quot;) ] print(data.class_size.shape) data.class_size.head() . (6513, 17) . DBN CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO . 225 01M292 | 1 | M | M292 | Henry Street School for International Studies | 09-12 | GEN ED | ENGLISH | English 9 | - | 63.0 | 3.0 | 21.0 | 19.0 | 25.0 | STARS | NaN | . 226 01M292 | 1 | M | M292 | Henry Street School for International Studies | 09-12 | GEN ED | ENGLISH | English 10 | - | 79.0 | 3.0 | 26.3 | 24.0 | 31.0 | STARS | NaN | . 227 01M292 | 1 | M | M292 | Henry Street School for International Studies | 09-12 | GEN ED | ENGLISH | English 11 | - | 38.0 | 2.0 | 19.0 | 16.0 | 22.0 | STARS | NaN | . 228 01M292 | 1 | M | M292 | Henry Street School for International Studies | 09-12 | GEN ED | ENGLISH | English 12 | - | 69.0 | 3.0 | 23.0 | 13.0 | 30.0 | STARS | NaN | . 229 01M292 | 1 | M | M292 | Henry Street School for International Studies | 09-12 | GEN ED | MATH | Integrated Algebra | - | 53.0 | 3.0 | 17.7 | 16.0 | 21.0 | STARS | NaN | . The first few rows of the dataset show us that we were successful in filtering rows by GRADE and PROGRAM TYPE. From 27611 rows, we have narrowed the dataset down to 6513 rows. . However, there are still duplicate DBN values. The new problem is that the data are divided by the following columns: . CORE SUBJECT (MS CORE and 9-12 ONLY) | CORE COURSE (MS CORE and 9-12 ONLY) | . These columns contain the specific subject or course of the class. We want to get class size data that accounts for all subjects, so we cannot simply filter the dataset by 1 subject. . Instead, we will: . Group the data by DBN value. | Aggregate the numerical data by taking the mean of each column in each group. | . data.class_size = ( data.class_size .groupby(&quot;DBN&quot;) # Group by the school&#39;s unique DBN .agg(np.mean) # Aggregate numerical columns using mean .reset_index() # Put the DBN column back in the df ) print(data.class_size.shape) data.class_size.head() . (583, 8) . DBN CSD NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS SCHOOLWIDE PUPIL-TEACHER RATIO . 0 01M292 | 1.0 | 88.0000 | 4.000000 | 22.564286 | 18.50 | 26.571429 | NaN | . 1 01M332 | 1.0 | 46.0000 | 2.000000 | 22.000000 | 21.00 | 23.500000 | NaN | . 2 01M378 | 1.0 | 33.0000 | 1.000000 | 33.000000 | 33.00 | 33.000000 | NaN | . 3 01M448 | 1.0 | 105.6875 | 4.750000 | 22.231250 | 18.25 | 27.062500 | NaN | . 4 01M450 | 1.0 | 57.6000 | 2.733333 | 21.200000 | 19.40 | 22.866667 | NaN | . The following changes have occurred: . The dataset now has only 583 rows. | Only the DBN, CSD, and numerical columns remain. | The numerical columns contain decimals since the data were aggregated. | . Let&#39;s check what the maximum frequency of DBN values is. . data.class_size[&quot;DBN&quot;].value_counts().max() . 1 . Each DBN only appears once in the data. We have condensed the dataset successfully. . Demographics Dataset . Next, we condense the Demographics dataset. It contains data on NYC high school students&#39; grade level, gender, ethnicity, etc. from 2006 to 2012. . Let&#39;s try to identify why each DBN has multiple rows. . print(data.demographics.shape) data.demographics.head() . (10075, 38) . DBN Name schoolyear fl_percent frl_percent total_enrollment prek k grade1 grade2 ... black_num black_per hispanic_num hispanic_per white_num white_per male_num male_per female_num female_per . 0 01M015 | P.S. 015 ROBERTO CLEMENTE | 20052006 | 89.4 | NaN | 281 | 15 | 36 | 40 | 33 | ... | 74 | 26.3 | 189 | 67.3 | 5 | 1.8 | 158.0 | 56.2 | 123.0 | 43.8 | . 1 01M015 | P.S. 015 ROBERTO CLEMENTE | 20062007 | 89.4 | NaN | 243 | 15 | 29 | 39 | 38 | ... | 68 | 28.0 | 153 | 63.0 | 4 | 1.6 | 140.0 | 57.6 | 103.0 | 42.4 | . 2 01M015 | P.S. 015 ROBERTO CLEMENTE | 20072008 | 89.4 | NaN | 261 | 18 | 43 | 39 | 36 | ... | 77 | 29.5 | 157 | 60.2 | 7 | 2.7 | 143.0 | 54.8 | 118.0 | 45.2 | . 3 01M015 | P.S. 015 ROBERTO CLEMENTE | 20082009 | 89.4 | NaN | 252 | 17 | 37 | 44 | 32 | ... | 75 | 29.8 | 149 | 59.1 | 7 | 2.8 | 149.0 | 59.1 | 103.0 | 40.9 | . 4 01M015 | P.S. 015 ROBERTO CLEMENTE | 20092010 | | 96.5 | 208 | 16 | 40 | 28 | 32 | ... | 67 | 32.2 | 118 | 56.7 | 6 | 2.9 | 124.0 | 59.6 | 84.0 | 40.4 | . 5 rows × 38 columns . Based on the schoolyear column, each school has a separate row for each academic year. Let&#39;s inspect the years available. . data.demographics[&quot;schoolyear&quot;].value_counts().sort_index() . 20052006 1356 20062007 1386 20072008 1410 20082009 1441 20092010 1475 20102011 1498 20112012 1509 Name: schoolyear, dtype: int64 . One of the values represents school year 2011-2012. This is relevant to our analysis because the SAT results dataset is from 2012. Thus, we will filter demographics to include only this year. . data.demographics = data.demographics.loc[ data.demographics[&quot;schoolyear&quot;] == 20112012 ] print(data.demographics.shape) data.demographics.head() . (1509, 38) . DBN Name schoolyear fl_percent frl_percent total_enrollment prek k grade1 grade2 ... black_num black_per hispanic_num hispanic_per white_num white_per male_num male_per female_num female_per . 6 01M015 | P.S. 015 ROBERTO CLEMENTE | 20112012 | NaN | 89.4 | 189 | 13 | 31 | 35 | 28 | ... | 63 | 33.3 | 109 | 57.7 | 4 | 2.1 | 97.0 | 51.3 | 92.0 | 48.7 | . 13 01M019 | P.S. 019 ASHER LEVY | 20112012 | NaN | 61.5 | 328 | 32 | 46 | 52 | 54 | ... | 81 | 24.7 | 158 | 48.2 | 28 | 8.5 | 147.0 | 44.8 | 181.0 | 55.2 | . 20 01M020 | PS 020 ANNA SILVER | 20112012 | NaN | 92.5 | 626 | 52 | 102 | 121 | 87 | ... | 55 | 8.8 | 357 | 57.0 | 16 | 2.6 | 330.0 | 52.7 | 296.0 | 47.3 | . 27 01M034 | PS 034 FRANKLIN D ROOSEVELT | 20112012 | NaN | 99.7 | 401 | 14 | 34 | 38 | 36 | ... | 90 | 22.4 | 275 | 68.6 | 8 | 2.0 | 204.0 | 50.9 | 197.0 | 49.1 | . 35 01M063 | PS 063 WILLIAM MCKINLEY | 20112012 | NaN | 78.9 | 176 | 18 | 20 | 30 | 21 | ... | 41 | 23.3 | 110 | 62.5 | 15 | 8.5 | 97.0 | 55.1 | 79.0 | 44.9 | . 5 rows × 38 columns . Filtering the dataset has reduced it to 1509 rows. Also, if we look at the DBN column, the values appear to be distinct now. Let&#39;s check the maximum frequency of its values: . data.demographics[&quot;DBN&quot;].value_counts().max() . 1 . Each DBN value appears only once. . Graduation Dataset . Lastly, we will condense the graduation dataset. It contains data on the graduation rate in the Cohorts of 2001 to 2006. The cohort of a year is the set of students that entered Grade 9 in that year. Thus, the cohorts in this dataset graduated between 2005 and 2010. . Below are the first five rows of the dataset. . print(data.graduation.shape) data.graduation.head(5) . (25096, 23) . Demographic DBN School Name Cohort Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n Total Regents - % of cohort Total Regents - % of grads ... Regents w/o Advanced - n Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads Local - n Local - % of cohort Local - % of grads Still Enrolled - n Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort . 0 Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2003 | 5 | s | s | s | s | s | ... | s | s | s | s | s | s | s | s | s | s | . 1 Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2004 | 55 | 37 | 67.3% | 17 | 30.9% | 45.9% | ... | 17 | 30.9% | 45.9% | 20 | 36.4% | 54.1% | 15 | 27.3% | 3 | 5.5% | . 2 Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2005 | 64 | 43 | 67.2% | 27 | 42.2% | 62.8% | ... | 27 | 42.2% | 62.8% | 16 | 25% | 37.200000000000003% | 9 | 14.1% | 9 | 14.1% | . 3 Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006 | 78 | 43 | 55.1% | 36 | 46.2% | 83.7% | ... | 36 | 46.2% | 83.7% | 7 | 9% | 16.3% | 16 | 20.5% | 11 | 14.1% | . 4 Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006 Aug | 78 | 44 | 56.4% | 37 | 47.4% | 84.1% | ... | 37 | 47.4% | 84.1% | 7 | 9% | 15.9% | 15 | 19.2% | 11 | 14.1% | . 5 rows × 23 columns . At first glance, the Cohort column appears to be responsible for the duplication of DBNs. Each school has a separate row for each cohort. Below are the cohort values: . data.graduation[&quot;Cohort&quot;].value_counts().sort_index() . 2001 2637 2002 3095 2003 3432 2004 3708 2005 3963 2006 4130 2006 Aug 4131 Name: Cohort, dtype: int64 . As expected, the cohorts range from 2001 to 2006. There also appears to be an extra value representing August of 2006. . Since we can only keep 1 row from each school, it is best to keep the 2006 rows since these are the most recent. . data.graduation = data.graduation.loc[ data.graduation[&quot;Cohort&quot;] == &quot;2006&quot; ] print(data.graduation.shape) data.graduation.head() . (4130, 23) . Demographic DBN School Name Cohort Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n Total Regents - % of cohort Total Regents - % of grads ... Regents w/o Advanced - n Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads Local - n Local - % of cohort Local - % of grads Still Enrolled - n Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort . 3 Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006 | 78 | 43 | 55.1% | 36 | 46.2% | 83.7% | ... | 36 | 46.2% | 83.7% | 7 | 9% | 16.3% | 16 | 20.5% | 11 | 14.1% | . 10 Total Cohort | 01M448 | UNIVERSITY NEIGHBORHOOD HIGH SCHOOL | 2006 | 124 | 53 | 42.7% | 42 | 33.9% | 79.2% | ... | 34 | 27.4% | 64.2% | 11 | 8.9% | 20.8% | 46 | 37.1% | 20 | 16.100000000000001% | . 17 Total Cohort | 01M450 | EAST SIDE COMMUNITY SCHOOL | 2006 | 90 | 70 | 77.8% | 67 | 74.400000000000006% | 95.7% | ... | 67 | 74.400000000000006% | 95.7% | 3 | 3.3% | 4.3% | 15 | 16.7% | 5 | 5.6% | . 24 Total Cohort | 01M509 | MARTA VALLE HIGH SCHOOL | 2006 | 84 | 47 | 56% | 40 | 47.6% | 85.1% | ... | 23 | 27.4% | 48.9% | 7 | 8.300000000000001% | 14.9% | 25 | 29.8% | 5 | 6% | . 31 Total Cohort | 01M515 | LOWER EAST SIDE PREPARATORY HIGH SCHO | 2006 | 193 | 105 | 54.4% | 91 | 47.2% | 86.7% | ... | 22 | 11.4% | 21% | 14 | 7.3% | 13.3% | 53 | 27.5% | 35 | 18.100000000000001% | . 5 rows × 23 columns . The dataset is left with 4130 rows. Let&#39;s check if the DBNs are unique. . data.graduation[&quot;DBN&quot;].value_counts().max() . 13 . It seems that the DBNs still have up to 13 duplicates each. . This may be due to the Demographic column. What if it contains values other than &quot;Total Cohort&quot;? . data.graduation[&quot;Demographic&quot;].value_counts() . Special Education Students 411 Total Cohort 405 General Education Students 405 English Proficient Students 401 Female 396 Hispanic 395 Male 395 Black 394 English Language Learners 340 Asian 296 White 292 Name: Demographic, dtype: int64 . Indeed, each school has a separate row for each demographic. Since we want to include all or most of the students, we will take the &quot;Total Cohort&quot; rows. . data.graduation = data.graduation.loc[ data.graduation[&quot;Demographic&quot;] == &quot;Total Cohort&quot; ] print(data.graduation.shape) data.graduation.head() . (405, 23) . Demographic DBN School Name Cohort Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n Total Regents - % of cohort Total Regents - % of grads ... Regents w/o Advanced - n Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads Local - n Local - % of cohort Local - % of grads Still Enrolled - n Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort . 3 Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006 | 78 | 43 | 55.1% | 36 | 46.2% | 83.7% | ... | 36 | 46.2% | 83.7% | 7 | 9% | 16.3% | 16 | 20.5% | 11 | 14.1% | . 10 Total Cohort | 01M448 | UNIVERSITY NEIGHBORHOOD HIGH SCHOOL | 2006 | 124 | 53 | 42.7% | 42 | 33.9% | 79.2% | ... | 34 | 27.4% | 64.2% | 11 | 8.9% | 20.8% | 46 | 37.1% | 20 | 16.100000000000001% | . 17 Total Cohort | 01M450 | EAST SIDE COMMUNITY SCHOOL | 2006 | 90 | 70 | 77.8% | 67 | 74.400000000000006% | 95.7% | ... | 67 | 74.400000000000006% | 95.7% | 3 | 3.3% | 4.3% | 15 | 16.7% | 5 | 5.6% | . 24 Total Cohort | 01M509 | MARTA VALLE HIGH SCHOOL | 2006 | 84 | 47 | 56% | 40 | 47.6% | 85.1% | ... | 23 | 27.4% | 48.9% | 7 | 8.300000000000001% | 14.9% | 25 | 29.8% | 5 | 6% | . 31 Total Cohort | 01M515 | LOWER EAST SIDE PREPARATORY HIGH SCHO | 2006 | 193 | 105 | 54.4% | 91 | 47.2% | 86.7% | ... | 22 | 11.4% | 21% | 14 | 7.3% | 13.3% | 53 | 27.5% | 35 | 18.100000000000001% | . 5 rows × 23 columns . After the dataset was filtered for the second time, it was left with 405 rows. . Let&#39;s check the maximum frequency of DBNs again. . data.graduation[&quot;DBN&quot;].value_counts().max() . 1 . The DBNs are now unique to each row in the dataset. . Column Names . We&#39;re almost at the point where we can combine the datasets. However, the final dataset would have many columns. It would be good to be able to know which dataset each column came from. Thus, we will use the following prefix system: . A_: The ap_2010 dataset | C_: The class_size dataset | D_: The demographics dataset | G_: The graduation dataset | H_: The hs_directory dataset | R_: The sat_results dataset | S_: The survey dataset | . I add these prefixes using the code below. . for df_name in data.index: # Take the first letter, capitalize it, and append an underscore. if df_name == &quot;sat_results&quot;: prefix = &quot;R_&quot; else: prefix = df_name[0].upper() + &quot;_&quot; # Add the prefix to each column label. df = data[df_name] non_dbn = [col for col in df if col != &quot;DBN&quot;] dbn_col = df[[&quot;DBN&quot;]] other_cols = df[non_dbn].add_prefix(prefix) # Put the df back into the data Series. data[df_name] = pd.concat( [dbn_col, other_cols], axis = 1, ) . For example, let&#39;s see what ap_2010 looks like now: . data.ap_2010.head() . DBN A_SchoolName A_AP Test Takers A_Total Exams Taken A_Number of Exams with scores 3 4 or 5 . 0 01M448 | UNIVERSITY NEIGHBORHOOD H.S. | 39.0 | 49.0 | 10.0 | . 1 01M450 | EAST SIDE COMMUNITY HS | 19.0 | 21.0 | NaN | . 2 01M515 | LOWER EASTSIDE PREP | 24.0 | 26.0 | 24.0 | . 3 01M539 | NEW EXPLORATIONS SCI,TECH,MATH | 255.0 | 377.0 | 191.0 | . 4 02M296 | High School of Hospitality Management | NaN | NaN | NaN | . The prefixes were added successfully. . Combining Datasets . At last, we are ready to combine all of the datasets into one. Datasets be merged on the DBN column since this is unique to each row. . We will start with the sat_results dataset as the first &quot;left&quot; dataset in the merging process. . We will consider two ways to merge datasets: . Left join: Keep all rows in the &quot;left&quot; dataset, but remove rows in the &quot;right&quot; dataset if their DBNs had no match. | Inner join: Only keep rows where the DBN exists in both datasets. | . Before we choose, though, we will have to find out whether the DBNs in sat_results match the ones present in other datasets. . Heatmap of DBNs . In previous Dataquest guided projects, heatmaps were sometimes used to visualize correlation tables and missing values. I realized that this could be extended to the situation of the DBNs. . What if we created a DataFrame where: . The columns represent the 7 datasets. | The indices represent the unique DBNs. | Each value is True if the DBN appears in the specific dataset, else False. | . Then, we would be able to create a heatmap to visualize which DBNs were present and absent in each dataset. . I wrote the code below in order to make the DataFrame. . col_list = [] for df_name in data.index: dbn = data[df_name][[&quot;DBN&quot;]] # Get a 1-column DataFrame of DBN values. new_col = dbn.set_index(&quot;DBN&quot;) # Set the DBNs as the indices. There are no columns now. new_col[df_name] = 1 # Create a new column of ones. col_list.append(new_col) # Append the column to the list. # Combine the columns into one DataFrame. dbn_grid = pd.concat( col_list, axis = 1, ).notnull() # Replace values with False if they&#39;re null, else True. dbn_grid.head() . ap_2010 class_size demographics graduation hs_directory sat_results survey . DBN . 01M448 True | True | True | True | True | True | True | . 01M450 True | True | True | True | True | True | True | . 01M515 True | True | True | True | False | True | True | . 01M539 True | True | True | True | True | True | True | . 02M296 True | True | True | True | True | True | True | . Let&#39;s take the top left cell as an example. Its value is True. This means that the DBN 01M448 appears in the ap_2010 dataset. . Since SAT performance is our focus, we can filter the rows in the DataFrame to only include the ones where the DBN is present in sat_results. . dbn_grid = dbn_grid.loc[ dbn_grid[&quot;sat_results&quot;] # Keep rows where this column contains True values. ] . Now, we can plot our heatmap. Above it, we will also print a table of the exact number of relevant DBNs present in each dataset. . print(&quot;Counts of relevant DBNs in each dataset&quot;) print(dbn_grid.sum()) # Initiate the figure plt.figure(figsize = (15, 5)) # Create a heatmap of the boolean dataframe sns.heatmap( dbn_grid, cbar = True, # Include a color bar. yticklabels = False, # Do not show y-tick labels. ) plt.title(&quot;Heatmap of Relevant DBN Values in each Dataset&quot;) plt.xlabel(&quot;Dataset&quot;) plt.xticks(rotation = 0) plt.show() . Counts of relevant DBNs in each dataset ap_2010 251 class_size 401 demographics 412 graduation 381 hs_directory 339 sat_results 421 survey 418 dtype: int64 . Each column in the heatmap corresponds to one of our 7 datasets. The y-axis represents the unique DBNs. . The DBNs present in a dataset are represented by a light brown color. The missing DBNs are represented by black. . The following can be observed: . The sat_results dataset is complete since we are using it as our basis for which DBNs are &quot;relevant.&quot; There are 421 relevant DBNs in total. | The ap_2010 dataset has the least DBNs, only 251. | hs_directory and graduation have missing DBNs scattered around. They have less than 400 DBNs each. | class_size, demographics, and survey are the most complete datasets, with over 400 DBNs. | . This information will help us in choosing how to merge the datasets. . Left join . The advantage of performing a left join is apparent when the &quot;right&quot; dataset has many missing DBN values. . With a left join, all of the sat_results rows would be kept. | With an inner join, the number of rows would be significantly reduced. | . We saw that ap_2010, hs_directory, and graduation have the most missing DBN values, so these are good candidates for a left join. These 3 datasets provide information that is supplementary but not very crucial to our analysis, so it is fine for some rows to contain missing values. . combined = data.sat_results for df_name in [&quot;ap_2010&quot;, &quot;hs_directory&quot;, &quot;graduation&quot;]: # Merge horizontally. combined = combined.merge( data[df_name], on = &quot;DBN&quot;, # Merge on DBN column how = &quot;left&quot;, # Left join ) print(combined.shape) combined.head() . (421, 92) . DBN R_SCHOOL NAME R_Num of SAT Test Takers R_SAT Critical Reading Avg. Score R_SAT Math Avg. Score R_SAT Writing Avg. Score R_sat_score A_SchoolName A_AP Test Takers A_Total Exams Taken ... G_Regents w/o Advanced - n G_Regents w/o Advanced - % of cohort G_Regents w/o Advanced - % of grads G_Local - n G_Local - % of cohort G_Local - % of grads G_Still Enrolled - n G_Still Enrolled - % of cohort G_Dropped Out - n G_Dropped Out - % of cohort . 0 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES | 29 | 355.0 | 404.0 | 363.0 | 1122.0 | NaN | NaN | NaN | ... | 36 | 46.2% | 83.7% | 7 | 9% | 16.3% | 16 | 20.5% | 11 | 14.1% | . 1 01M448 | UNIVERSITY NEIGHBORHOOD HIGH SCHOOL | 91 | 383.0 | 423.0 | 366.0 | 1172.0 | UNIVERSITY NEIGHBORHOOD H.S. | 39.0 | 49.0 | ... | 34 | 27.4% | 64.2% | 11 | 8.9% | 20.8% | 46 | 37.1% | 20 | 16.100000000000001% | . 2 01M450 | EAST SIDE COMMUNITY SCHOOL | 70 | 377.0 | 402.0 | 370.0 | 1149.0 | EAST SIDE COMMUNITY HS | 19.0 | 21.0 | ... | 67 | 74.400000000000006% | 95.7% | 3 | 3.3% | 4.3% | 15 | 16.7% | 5 | 5.6% | . 3 01M458 | FORSYTH SATELLITE ACADEMY | 7 | 414.0 | 401.0 | 359.0 | 1174.0 | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 01M509 | MARTA VALLE HIGH SCHOOL | 44 | 390.0 | 433.0 | 384.0 | 1207.0 | NaN | NaN | NaN | ... | 23 | 27.4% | 48.9% | 7 | 8.300000000000001% | 14.9% | 25 | 29.8% | 5 | 6% | . 5 rows × 92 columns . The dataset still has all of the data from sat_results due to the left join. There are some missing values where the other datasets didn&#39;t find a matching DBN. . Inner join . Inner joins are advantageous when the &quot;right&quot; dataset is important to analysis. We would want each row to have complete data from this dataset. . In our case, the remaining datasets to merge are class_size, demographics, and survey. We need the latter two datasets to be complete, since we aim to explore the relationships between student demographics and SAT performance. Thus, we will use an inner join. . for df_name in [&quot;class_size&quot;, &quot;demographics&quot;, &quot;survey&quot;]: # Merge horizontally. combined = combined.merge( data[df_name], on = &quot;DBN&quot;, # Merge on DBN column how = &quot;inner&quot;, # Inner join ) print(combined.shape) combined.head() . (401, 158) . DBN R_SCHOOL NAME R_Num of SAT Test Takers R_SAT Critical Reading Avg. Score R_SAT Math Avg. Score R_SAT Writing Avg. Score R_sat_score A_SchoolName A_AP Test Takers A_Total Exams Taken ... S_eng_t_11 S_aca_t_11 S_saf_s_11 S_com_s_11 S_eng_s_11 S_aca_s_11 S_saf_tot_11 S_com_tot_11 S_eng_tot_11 S_aca_tot_11 . 0 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES | 29 | 355.0 | 404.0 | 363.0 | 1122.0 | NaN | NaN | NaN | ... | 6.1 | 6.5 | 6.0 | 5.6 | 6.1 | 6.7 | 6.7 | 6.2 | 6.6 | 7.0 | . 1 01M448 | UNIVERSITY NEIGHBORHOOD HIGH SCHOOL | 91 | 383.0 | 423.0 | 366.0 | 1172.0 | UNIVERSITY NEIGHBORHOOD H.S. | 39.0 | 49.0 | ... | 6.6 | 7.3 | 6.0 | 5.7 | 6.3 | 7.0 | 6.8 | 6.3 | 6.7 | 7.2 | . 2 01M450 | EAST SIDE COMMUNITY SCHOOL | 70 | 377.0 | 402.0 | 370.0 | 1149.0 | EAST SIDE COMMUNITY HS | 19.0 | 21.0 | ... | 8.0 | 8.8 | NaN | NaN | NaN | NaN | 7.9 | 7.9 | 7.9 | 8.4 | . 3 01M458 | FORSYTH SATELLITE ACADEMY | 7 | 414.0 | 401.0 | 359.0 | 1174.0 | NaN | NaN | NaN | ... | 8.9 | 8.9 | 6.8 | 6.1 | 6.1 | 6.8 | 7.8 | 7.1 | 7.2 | 7.8 | . 4 01M509 | MARTA VALLE HIGH SCHOOL | 44 | 390.0 | 433.0 | 384.0 | 1207.0 | NaN | NaN | NaN | ... | 6.1 | 6.8 | 6.4 | 5.9 | 6.4 | 7.0 | 6.9 | 6.2 | 6.6 | 7.0 | . 5 rows × 158 columns . The final combined DataFrame has 401 rows (schools) and 158 columns. That&#39;s only 20 rows less than what we started with. . City School Districts . Additionally, we will create a new column containing the city school district (CSD) of each school. The CSD is made up of the first 2 digits of a school&#39;s DBN. . combined[&quot;school_dist&quot;] = combined[&quot;DBN&quot;].apply( lambda text: text[:2] # Get first two characters. ) # Reorder the columns. combined = reorder_columns(combined, [&quot;school_dist&quot;]) combined.head() . school_dist DBN R_SCHOOL NAME R_Num of SAT Test Takers R_SAT Critical Reading Avg. Score R_SAT Math Avg. Score R_SAT Writing Avg. Score R_sat_score A_SchoolName A_AP Test Takers ... S_eng_t_11 S_aca_t_11 S_saf_s_11 S_com_s_11 S_eng_s_11 S_aca_s_11 S_saf_tot_11 S_com_tot_11 S_eng_tot_11 S_aca_tot_11 . 0 01 | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES | 29 | 355.0 | 404.0 | 363.0 | 1122.0 | NaN | NaN | ... | 6.1 | 6.5 | 6.0 | 5.6 | 6.1 | 6.7 | 6.7 | 6.2 | 6.6 | 7.0 | . 1 01 | 01M448 | UNIVERSITY NEIGHBORHOOD HIGH SCHOOL | 91 | 383.0 | 423.0 | 366.0 | 1172.0 | UNIVERSITY NEIGHBORHOOD H.S. | 39.0 | ... | 6.6 | 7.3 | 6.0 | 5.7 | 6.3 | 7.0 | 6.8 | 6.3 | 6.7 | 7.2 | . 2 01 | 01M450 | EAST SIDE COMMUNITY SCHOOL | 70 | 377.0 | 402.0 | 370.0 | 1149.0 | EAST SIDE COMMUNITY HS | 19.0 | ... | 8.0 | 8.8 | NaN | NaN | NaN | NaN | 7.9 | 7.9 | 7.9 | 8.4 | . 3 01 | 01M458 | FORSYTH SATELLITE ACADEMY | 7 | 414.0 | 401.0 | 359.0 | 1174.0 | NaN | NaN | ... | 8.9 | 8.9 | 6.8 | 6.1 | 6.1 | 6.8 | 7.8 | 7.1 | 7.2 | 7.8 | . 4 01 | 01M509 | MARTA VALLE HIGH SCHOOL | 44 | 390.0 | 433.0 | 384.0 | 1207.0 | NaN | NaN | ... | 6.1 | 6.8 | 6.4 | 5.9 | 6.4 | 7.0 | 6.9 | 6.2 | 6.6 | 7.0 | . 5 rows × 159 columns . The data is now cleaned and ready for analysis. . Exploratory Data Analysis . Correlation Coefficients . The goal of this project is to determine how demographic factors affect SAT scores. Thus, we can start our analysis by investigating the Pearson&#39;s correlation coefficient between SAT score and each of the other columns. . sat_corr = ( combined .corr()[&quot;R_sat_score&quot;] # Take sat_score correlations .drop([&quot;R_sat_score&quot;]) # Drop unnecessary variables .dropna() # Drop missing values ) . The correlation table has one value for each of the numerical columns in combined. . sat_corr.head() . R_SAT Critical Reading Avg. Score 0.976819 R_SAT Math Avg. Score 0.956406 R_SAT Writing Avg. Score 0.981751 A_AP Test Takers 0.625184 A_Total Exams Taken 0.614112 Name: R_sat_score, dtype: float64 . We&#39;ll analyze the data by going through each set of variables based on their original dataset. We&#39;ll only use the most important datasets. . Before we do this, we&#39;ll define a function that will take a Series and return values whose labels have certain prefixes. This will help us select parts of the correlation table more easily. . def filter_prefix(series, regex): &quot;&quot;&quot;Takes a Series and a regex. Returns items in the Series whose labels match the regex at the start.&quot;&quot;&quot; result = series[[ label for label in series.index if re.match(regex, label) ]] return result . For example, we could select all correlations for ap_2010 and class_size variables by filtering prefixes A and C. . filter_prefix(sat_corr, r&quot;[AC]&quot;) . A_AP Test Takers 0.625184 A_Total Exams Taken 0.614112 A_Number of Exams with scores 3 4 or 5 0.614752 C_CSD 0.054011 C_NUMBER OF STUDENTS / SEATS FILLED 0.400095 C_NUMBER OF SECTIONS 0.364537 C_AVERAGE CLASS SIZE 0.395964 C_SIZE OF SMALLEST CLASS 0.282388 C_SIZE OF LARGEST CLASS 0.327099 Name: R_sat_score, dtype: float64 . Let&#39;s proceed to the first set of variables. . AP Exam Results . First, we&#39;ll look at the correlations for ap_2010 variables. . filter_prefix(sat_corr, r&quot;[A]&quot;) . A_AP Test Takers 0.625184 A_Total Exams Taken 0.614112 A_Number of Exams with scores 3 4 or 5 0.614752 Name: R_sat_score, dtype: float64 . The number of AP test takers has a positive correlation of 0.63 with SAT score. . Remember that the AP exams are &quot;Advanced Placement&quot; exams taken by USA high schoolers in order to get college credit. Thus, the students who take AP exams tend to have higher academic performance. . Let&#39;s make a scatter plot of SAT score against percentage of AP test takers in each school. . combined[&quot;A_ap_perc&quot;] = combined[&quot;A_AP Test Takers &quot;] / combined[&quot;D_total_enrollment&quot;] # Replace inaccurate data (above 100%) with nulls. combined[&quot;A_ap_perc&quot;] = combined[&quot;A_ap_perc&quot;].mask( combined[&quot;A_ap_perc&quot;] &gt; 1.0, np.nan, ) # Scatter plot combined.plot.scatter( x = &quot;A_ap_perc&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs AP Test Taker Percentage&quot;) plt.xlabel(&quot;AP Test Taker Percentage&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . The relationship is not linear. We can only observe that: . Most schools have less than 40% AP takers and less than 1400 SAT points. | The schools with high SAT scores (above 1600) have less than 60% AP test takers. | The schools with low SAT scores (under 1400) can have any percentage of AP test takers. | . Therefore, it doesn&#39;t seem that the percentage of AP test takers in a school reliably influences its average SAT score. . Survey Data . Let&#39;s move on to the survey variables. . filter_prefix(sat_corr, r&quot;[S]&quot;) . S_rr_s 0.286470 S_rr_t 0.022365 S_rr_p 0.087970 S_N_s 0.438427 S_N_t 0.306520 S_N_p 0.439547 S_saf_p_11 0.111892 S_com_p_11 -0.106899 S_eng_p_11 0.016640 S_aca_p_11 0.026691 S_saf_t_11 0.315796 S_com_t_11 0.097797 S_eng_t_11 0.045746 S_aca_t_11 0.135490 S_saf_s_11 0.286153 S_com_s_11 0.157417 S_eng_s_11 0.174425 S_aca_s_11 0.292592 S_saf_tot_11 0.292468 S_com_tot_11 0.083068 S_eng_tot_11 0.093489 S_aca_tot_11 0.178388 Name: R_sat_score, dtype: float64 . The variable names are abbreviated, so it is difficult to understand them. However, the data dictionary explains their meanings. . The most important ones are from S_saf_p_11 downwards. These variables represent scores of 4 aspects of each school: . Safety and Respect | Communication | Engagement | Academic Expectations | . Each aspect has 4 scores based on the respondent group: parents, teachers, students, and total (all groups). . For example, S_saf_tot_11 refers to the total Safety and Respect score of a school. It had a coefficient of 0.29. Let&#39;s see a scatterplot of this. . combined.plot.scatter( x = &quot;S_saf_tot_11&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs Safety and Respect Score (Total)&quot;) plt.xlabel(&quot;Safety and Respect Score (Total)&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . The correlation is not strong, as many schools have SAT scores under 1400 regardless of Safety and Respect score. . The schools which did have high scores (above 1500) had safety scores ranging from 7.25 to 8.75. This suggests that having sufficient safety and respect in a school may support the learning experience, though it does not always increase SAT scores. . Another interesting variable would be S_aca_s_11, the Academic Expectations score based on student responses. It had a coefficient of 0.29. . combined.plot.scatter( x = &quot;S_aca_s_11&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs Academic Expectations Score (Students)&quot;) plt.xlabel(&quot;Academic Expectations Score (Students)&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . Again, the correlation is not strong. . It&#39;s interesting to note, however, that none of the schools with an Academic Expectations score under 7.5 had SAT scores above 1600. We could say that a school must first have proper academic expectations in order to open the opportunity to excel. . In terms of strong conclusions, though, we can&#39;t make any yet. . Demographics . Lastly, we go to the demographics dataset variables. These are the most important variables to investigate for this project. . Here are the correlation coefficients: . filter_prefix(sat_corr, r&quot;[D]&quot;) . D_frl_percent -0.718163 D_total_enrollment 0.385741 D_ell_num -0.130355 D_ell_percent -0.379510 D_sped_num 0.058782 D_sped_percent -0.420646 D_asian_num 0.483939 D_asian_per 0.552204 D_black_num 0.043408 D_black_per -0.302794 D_hispanic_num 0.052672 D_hispanic_per -0.363696 D_white_num 0.460505 D_white_per 0.646568 D_male_num 0.345808 D_male_per -0.107624 D_female_num 0.403581 D_female_per 0.107676 Name: R_sat_score, dtype: float64 . The results can be divided into general topics: . School size: The total_enrollment has a coefficient of 0.39. This means that bigger schools may have higher SAT scores. | English proficiency: The coefficient for ELL percentage is -0.38. ELL means English Language Learner. It refers to students who are learning as a second or third language because they did not grow up with it. | . | SpEd percentage: The coefficient for SpEd percentage is -0.42. SpEd means Special Education. It refers to educational plans with special accommodations for students who are disabled or have developmental disorders. | . | Sex: The following are the coefficients of the percentage of each sex: Male: -0.11 | Female: 0.11 | . | Race: The following are the coefficients related to the percentage of students belonging to specific races. Asian: 0.55 | Black: -0.30 | Hispanic: -0.36 | White: 0.65 | . | Socio-economic status: The coefficient for FRL percentage -0.72. FRL means Free or Reduced-Price Lunch. This is a program that supports students below a certain income threshold who do not have the means to bring their own lunch to school. | . | . We will go through each topic one by one. . School Size . Below, we plot a scatter plot of SAT scores against the total number of enrollees in each school. . combined.plot.scatter( x = &quot;D_total_enrollment&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs Total Enrollment&quot;) plt.xlabel(&quot;Total Enrollment&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . The scatter plot doesn&#39;t have a clear linear trend. Instead, the majority of points are clustered in one area, and the rest of the points are scattered. . Let&#39;s inspect the names of the schools at the very bottom left, which have SAT scores under 1000 and total enrollment under 1000. . combined.loc[ (combined[&quot;R_sat_score&quot;] &lt; 1000) &amp; (combined[&quot;D_total_enrollment&quot;] &lt; 1000), &quot;R_SCHOOL NAME&quot; ] . 102 INTERNATIONAL COMMUNITY HIGH SCHOOL 138 ACADEMY FOR LANGUAGE AND TECHNOLOGY 139 BRONX INTERNATIONAL HIGH SCHOOL 153 KINGSBRIDGE INTERNATIONAL HIGH SCHOOL 155 INTERNATIONAL SCHOOL FOR LIBERAL ARTS 197 PAN AMERICAN INTERNATIONAL HIGH SCHOOL AT MONROE 199 HIGH SCHOOL OF WORLD CULTURES 210 BROOKLYN INTERNATIONAL HIGH SCHOOL 241 PACIFIC HIGH SCHOOL 251 INTERNATIONAL HIGH SCHOOL AT PROSPECT HEIGHTS 266 IT TAKES A VILLAGE ACADEMY 285 MULTICULTURAL HIGH SCHOOL 314 ASPIRATIONS DIPLOMA PLUS HIGH SCHOOL 320 PAN AMERICAN INTERNATIONAL HIGH SCHOOL Name: R_SCHOOL NAME, dtype: object . Interestingly, the schools with the lowest average SAT scores are mostly small international schools. These schools tend to have more students from different racial and ethnic backgrounds. These students may speak English as a second or third language. . Therefore, the factor that affected SAT performance may have been the percentage of English language learners (ELL) in each school. This makes sense since the SAT is offered only in English and includes sections on Reading and Writing. . English Proficiency . Let&#39;s plot SAT scores against the percentage of ELLs (English Language Learners) in each school. . combined.plot.scatter( x = &quot;D_ell_percent&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs ELL Percentage&quot;) plt.xlabel(&quot;ELL Percentage&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . Once again, there isn&#39;t a linear trend even though the correlation coefficient between the variables was -0.38. However, we can note that: . Most schools have less than 50% ELLs and less than 1400 SAT points. | The schools with the highest SAT scores (over 1500) have almost 0% ELLs. | All of the schools with over 50% ELLs have very low scores (under 1300). | . Therefore, it appears that ELL students are more challenged in reaching high SAT scores. . SpEd Percentage . The coefficient for SpEd percentage was -0.42, which indicates that schools with more SpEd students may have lower SAT scores. Let&#39;s view a scatter plot: . combined.plot.scatter( x = &quot;D_sped_percent&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs SpEd Percentage&quot;) plt.xlabel(&quot;SpEd Percentage&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . The correlation is somewhat linear, but it is not very steep. The points are very scattered, especially on the left side. Interestingly, the schools with an SAT score over 1600 had 15% or less SpEd students. . However, the variable doesn&#39;t predict SAT score well. . Sex . Another factor that&#39;s important to investigate is sex. Do schools with more male students or more female students have higher SAT scores? . Earlier, we saw that the male and female percentages had weak correlations with SAT score. . Male: -0.11; lower SAT scores. | Female: 0.11; higher SAT scores. | . However, these correlations are weak, so the difference may not be that great. . Let&#39;s use the female_per variable in a scatter plot. . combined.plot.scatter( x = &quot;D_female_per&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs Female Percentage&quot;) plt.xlabel(&quot;Female Percentage&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . As expected, the correlation is weak. We can see that: . In most schools, 40% to 60% of students are female. | SAT score is typically below 1400 regardless of female percentage. | The schools with the highest SAT scores (above 1500) have a 30% to 75% female population. | . This variable doesn&#39;t seem to be very important to SAT score. . Race . Earlier, we saw that the following were the coefficients for race percentage variables. Positive values imply that a higher percentage of the given race may lead to higher average SAT score. Negative values: lower score. . Asian: 0.55 | Black: -0.30 | Hispanic: -0.36 | White: 0.65 | . Out of all 4, the white percentage value has the strongest correlation, 0.65. This means that having more white students in a school can lead to a higher average SAT score. Let&#39;s see a scatterplot of this. . combined.plot.scatter( x = &quot;D_white_per&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs White Percentage&quot;) plt.xlabel(&quot;White Percentage&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . Though the correlation coefficient was stronger than the others, the trend is loose; the points are scattered far away from each other. We can see that: . Most of the schools in the dataset have SAT scores under 1400 and white percentage under 20%. | The schools with the highest scores have white percentages ranging from under 10% to over 60%. | . Thus, we can&#39;t say that a higher white percentage definitely raises the average SAT score. . Since the other race-related variables are weaker than this one, we won&#39;t investigate them further. Let&#39;s move on to the topic of socio-economic status. . Socio-economic status . As mentioned earlier, there was a variable representing the percentage of students availing of FRL (Free or Reduced-Price Lunch). Since FRL is for students below an income threshold, we can consider it similar to a poverty rate. . The correlation coefficient for this variable is the strongest at -0.72. The FRL students may have lower SAT scores due to external difficulties associated with low socio-economic status which hinder their education. . Below is the scatterplot. . combined.plot.scatter( x = &quot;D_frl_percent&quot;, y = &quot;R_sat_score&quot;, ) plt.title(&quot;SAT Score vs FRL Percentage&quot;) plt.xlabel(&quot;FRL Percentage&quot;) plt.ylabel(&quot;Average SAT Score&quot;) plt.grid(True) plt.show() . This graph shows a tighter, more linear relationship than was seen in the other graphs. . At the top left of the graph are schools with low FRL percentage and high average SAT score. There are still a few schools with an SAT score over 1600 despite an FRL percentage of over 50%. . However, for the majority of the schools, the FRL percentage is over 60%. All of these schools have an SAT score less than 1500. . Generally, the results suggest that socio-economic status is the strongest predictor of average SAT score among all variables in the dataset. High poverty rate is associated with a lower average SAT score. . Inferential Statistics: Linear Regression . As an additional step to this project, we will be performing Ordinary Least Squares linear regression. Some predictors may have a low Pearson&#39;s correlation coefficient with the response variable but turn out to be statistically significant when other variables are taken into account. . I used Linear Regression by Python for Data Science LLC, along with (Simple) Linear Regression and OLS: Introduction to the Theory (Sluijmers 2020), as my main references. . The dataset has 401 observations. The dependent variable (or response variable) will be each school&#39;s average SAT total score, represented by R_sat_score. . As for independent variables (or predictor variables), we will start by considering the following. . English proficiency D_ell_percent | . | SpEd percentage D_sped_percent | . | Sex D_female_per | D_male_per | . | Race D_asian_per | D_black_per | D_hispanic_per | D_white_per | . | Socio-economic status D_frl_percent. | . | . The variable names are placed in the list below. . iv_list = [ &quot;D_ell_percent&quot;, &quot;D_sped_percent&quot;, &quot;D_female_per&quot;, &quot;D_male_per&quot;, &quot;D_asian_per&quot;, &quot;D_black_per&quot;, &quot;D_hispanic_per&quot;, &quot;D_white_per&quot;, &quot;D_frl_percent&quot;, ] . I&#39;d like to note that none of these columns have null values, so we don&#39;t have to drop any rows or perform imputation. . print(&quot;Null values in each column&quot;) combined[iv_list].isnull().sum() . Null values in each column . D_ell_percent 0 D_sped_percent 0 D_female_per 0 D_male_per 0 D_asian_per 0 D_black_per 0 D_hispanic_per 0 D_white_per 0 D_frl_percent 0 dtype: int64 . Moving on, the code below creates a matrix X for IVs and vector y for the DV. . regression_data = ( combined[[&quot;DBN&quot;, &quot;R_sat_score&quot;] + iv_list] .set_index(&quot;DBN&quot;) ) X = regression_data[iv_list].copy() y = regression_data[&quot;R_sat_score&quot;].copy() regression_data . R_sat_score D_ell_percent D_sped_percent D_female_per D_male_per D_asian_per D_black_per D_hispanic_per D_white_per D_frl_percent . DBN . 01M292 1122.0 | 22.3 | 24.9 | 38.6 | 61.4 | 14.0 | 29.1 | 53.8 | 1.7 | 88.6 | . 01M448 1172.0 | 21.1 | 21.8 | 42.6 | 57.4 | 29.2 | 22.6 | 45.9 | 2.3 | 71.8 | . 01M450 1149.0 | 5.0 | 26.4 | 45.3 | 54.7 | 9.7 | 23.9 | 55.4 | 10.4 | 71.8 | . 01M458 1174.0 | 4.0 | 8.9 | 56.7 | 43.3 | 2.2 | 34.4 | 59.4 | 3.6 | 72.8 | . 01M509 1207.0 | 11.2 | 25.9 | 53.7 | 46.3 | 9.3 | 31.6 | 56.9 | 1.6 | 80.7 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 32K549 1035.0 | 18.2 | 18.4 | 51.6 | 48.4 | 0.5 | 28.3 | 68.7 | 1.8 | 75.1 | . 32K552 1060.0 | 23.9 | 22.1 | 45.5 | 54.5 | 1.5 | 26.4 | 70.4 | 1.0 | 77.1 | . 32K554 1315.0 | 2.7 | 4.9 | 47.9 | 52.1 | 5.3 | 12.9 | 79.5 | 1.5 | 81.4 | . 32K556 1055.0 | 18.2 | 18.6 | 46.8 | 53.2 | 0.9 | 21.2 | 77.3 | 0.0 | 88.0 | . 32K564 1034.0 | 5.2 | 9.4 | 55.8 | 44.2 | 0.3 | 35.9 | 61.5 | 1.8 | 81.8 | . 401 rows × 10 columns . Before we fit and interpret the model with these DataFrames, we must first test the assumptions of linear regression. . Multicollinearity . Multicollinearity occurs when multiple predictor variables are highly correlated. Variables with high multicollinearity have unreliable significance values. Thus, this is a problem that must be avoided. . The first way to check for multicollinearity is with a Pearson&#39;s correlation coefficient table. The code below plots this table using a heatmap for easier interpretation. . corr = X.corr() # Create a mask that selects the bottom left triangle of the table. corr = corr.iloc[1:, :-1] mask = np.triu( # Replace the lower triangle with 0&#39;s. np.ones_like(corr), # Make a DataFrame of 1&#39;s with the same shape as corr. k = 1, # Raise the diagonal by 1. ) # Plot a heatmap of the values. plt.figure(figsize = (12, 7)) ax = sns.heatmap( corr, vmin = -1, # Anchor colors on -1 and 1. vmax = 1, cbar = True, # Draw a color bar. cmap = &quot;RdBu&quot;, # Use Red for negatives and Blue for positives. mask = mask, # Use a mask to remove the upper right triangle of the matrix. annot = True, # Write the data value in each cell. ) # Format the text in the plot to make it easier to read. for text in ax.texts: coeff = float(text.get_text()) # Remove data values with low correlation. if np.abs(coeff) &lt; 0.3: text.set_text(&#39;&#39;) # Round all visible data values to the hundredths place. else: text.set_text(round(coeff, 2)) text.set_fontsize(&quot;large&quot;) plt.title(&quot;Correlation Heatmap of Independent Variables&quot;) # Rotate x-ticks to read from top to bottom. plt.xticks(rotation = -30, ha = &quot;left&quot;, size = &quot;large&quot;) plt.yticks(rotation = 0, size = &quot;large&quot;) plt.show() . In the heatmap above, blue colors represent positive correlation; red represents negative correlation. Colors with more saturation indicate that the correlation is stronger. . I removed most of the coefficient labels except for the ones greater than 0.3, which show somewhat strong correlations. Interestingly, we can see that: . Male percentage has a perfect negative correlation with female percentage. This is to be expected since both must sum up to 100. | Hispanic percentage is correlated with black percentage and FRL percentage. | White percentage is strongly negatively correlated with FRL percentage, indicating that schools with more white students also have fewer poor students. | . Another way to assess multicollinearity is to use variance inflation factors or VIFs. These are numbers greater than or equal to 1. . A VIF less than 5 suggests low to moderate collinearity. If a predictor&#39;s VIF is at least 5, this indicates high multicollinearity, so the significance value is not reliable. . The code below produces a VIF for each predictor. . from statsmodels.stats.outliers_influence import variance_inflation_factor as sm_vif def get_vif(X): &quot;&quot;&quot;Get a VIF value for each column in a DataFrame.&quot;&quot;&quot; vif_df = pd.DataFrame() vif_df[&quot;label&quot;] = X.columns vif_df[&quot;VIF&quot;] = [ sm_vif(X.values, i) for i in range(len(X.columns)) ] vif_df.set_index(&quot;label&quot;, inplace = True) return vif_df get_vif(X) . VIF . label . D_ell_percent 2.874752 | . D_sped_percent 8.509117 | . D_female_per 2578.305578 | . D_male_per 2569.217298 | . D_asian_per 304.520086 | . D_black_per 2196.055978 | . D_hispanic_per 2413.721821 | . D_white_per 231.186584 | . D_frl_percent 60.692762 | . These values are problematic. Almost all of the VIFs indicate strong multicollinearity except for that of D_ell_percent. This is likely due to the strong correlations we saw earlier in the heatmap. . Therefore, we must remove a few variables with high multicollinearity. . I choose to remove male_per since it has -1.0 correlation with female_per, which makes it redundant. | I choose to remove hispanic_per because it has over 0.5 correlation with black_per and frl_percent. | I choose to remove frl_percent since it has over 0.3 correlation with 4 other variables, including 0.7 correlation with white_per. | . Thus, below is the new list of predictors. . iv_list = [ &quot;D_ell_percent&quot;, &quot;D_sped_percent&quot;, &quot;D_female_per&quot;, &quot;D_asian_per&quot;, &quot;D_black_per&quot;, &quot;D_white_per&quot;, ] regression_data = ( combined[[&quot;DBN&quot;, &quot;R_sat_score&quot;] + iv_list] .set_index(&quot;DBN&quot;) ) X = regression_data[iv_list].copy() y = regression_data[&quot;R_sat_score&quot;].copy() vif_df = get_vif(X) vif_df . VIF . label . D_ell_percent 1.600755 | . D_sped_percent 3.824473 | . D_female_per 7.289758 | . D_asian_per 1.788134 | . D_black_per 3.926733 | . D_white_per 1.777499 | . We can see that most of the VIFs are less than 5, so the p-values in the model will be reliable. Only the female_per variable&#39;s p-value will be unreliable. However, I will still include this in the model since the percentages of sexes in a school may have an effect on SAT performance. . IID Errors and Normality of Residuals . Next, we will be testing two more assumptions: . IID (identically and independently distributed) residuals . This is tested using the Durbin-Watson test statistic, which ranges from 0 to 4. | Ideally, the statistic should be close to 2. | As a rule of thumb, values from 1.5 to 2.5 are considered acceptable. | . | Normality of residuals . This is tested using the Jarque-Bera test. | The null hypothesis is that the residuals are normally distributed. | A statistically significant value (p &lt; 0.05) supports the claim that the residuals are not normal. | . | . In order to test these, we must fit the model and look at its results. statsmodels automatically calculates both the Durbin-Watson and the Jarque-Bera tests for us. . # Add a constant to the X matrix. # This is done so that the y-intercept is not locked at 0. X = sm.add_constant(X) # Make and fit the OLS model. model = sm.OLS(y, X) results = model.fit() summary = results.summary() # Extract summary tables from OLS results summary. def extract_summary(sm_summary): &quot;&quot;&quot;Take a statsmodels summary instance and return a list of DataFrames.&quot;&quot;&quot; tables = [] t_inds = list(range(3)) for i in t_inds: table_as_html = sm_summary.tables[i].as_html() table_df = pd.read_html( table_as_html, header = (0 if i == 1 else None), # Only set a header for table 2. index_col = 0, )[0] tables.append(table_df) if i == 1: # Combine summary table 1 with the VIF column. table_df = pd.concat([table_df, vif_df], axis = 1) table_df.rename_axis(&quot;label&quot;, inplace = True) else: # For tables 0 and 2, turn the index back into a column. table_df = tables[i].reset_index() tables[i] = table_df return tables tables = extract_summary(summary) # Display the third table. tables[2] . C: Users migs anaconda3 envs new_streamlit_env2 lib site-packages statsmodels tsa tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument &#39;objs&#39; will be keyword-only x = pd.concat(x[::order], 1) . 0 1 2 3 . 0 Omnibus: | 28.251 | Durbin-Watson: | 1.660000e+00 | . 1 Prob(Omnibus): | 0.000 | Jarque-Bera (JB): | 6.075100e+01 | . 2 Skew: | 0.378 | Prob(JB): | 6.430000e-14 | . 3 Kurtosis: | 4.751 | Cond. No. | 5.040000e+02 | . The Durbin-Watson test statistic is 1.66. This is less than 2, so it indicates positive autocorrelation among residuals. (That is, each residual has a positive correlation with the residual before it). However, it falls within the acceptable range (1.5 to 2.5), so it is not a cause for concern. . The Jarque-Bera test significance, on the other hand, is $6.43 cdot 10^{-14}$, which is very close to 0. It is statistically significant, so the residuals are not normally distributed. However, since the sample size (401) is greater than 30, OLS regression is robust against deviations from normality. Thus, this is not a cause for concern either. . Let&#39;s plot a histogram of residuals to inspect the deviation from normality, just to be sure. . residuals = results.resid #bin_edges = get_bin_edges(residuals) plt.hist(residuals, )#bins = bin_edges) plt.title(&quot;Histogram of Residuals&quot;) plt.xlabel(&quot;Residual Error&quot;) plt.ylabel(&quot;Frequency&quot;) plt.grid(True) plt.show() . Though this isn&#39;t normal, it is still somewhat bell-shaped. Most of the data converges around a residual error of 0, with fewer values going farther away. Thus, the distribution isn&#39;t problematic. . We can also visualize normality of residuals using a Q-Q (quantile-quantile) plot. . sm.qqplot( data = residuals, line = &quot;r&quot;, # Regression line ) plt.title(&quot;Q-Q Plot of Residuals&quot;) plt.legend( labels = [&quot;Actual data&quot;, &quot;Theoretical quantiles line&quot;], loc = &quot;best&quot;, ) plt.grid(True) plt.show() . C: Users migs anaconda3 envs new_streamlit_env2 lib site-packages statsmodels graphics gofplots.py:993: UserWarning: marker is redundantly defined by the &#39;marker&#39; keyword argument and the fmt string &#34;bo&#34; (-&gt; marker=&#39;o&#39;). The keyword argument will take precedence. ax.plot(x, y, fmt, **plot_style) . The red line is our reference for normality. The blue dots should ideally follow this line. We can see that they do, though not perfectly, and the data are less normal at the higher quantiles. Generally, though, the data is close enough to being normal. . Lastly, for IID residuals, we can plot the residuals against the fitted response variable. . fitted = results.fittedvalues residuals = results.resid # Horizontal line on the x axis. plt.axhline(color = &quot;r&quot;, label = &quot;Fitted SAT Score&quot;) # Scatter plot. plt.scatter(fitted, residuals, label = &quot;Residual Error&quot;) plt.title(&quot;Residuals against Fitted SAT Score&quot;) plt.xlabel(&quot;Fitted SAT Score&quot;) plt.ylabel(&quot;Residual Error&quot;) plt.legend(loc = &quot;best&quot;) plt.grid(True) plt.show() . In this graph, we ideally want to see the points cluster uniformly around the red line, without any trends. . However, if we follow the outline of the general mass of points, these seem to follow a curve. However, the points on the right side are loosely scattered, so we cannot be sure whether there is truly a curve or it is merely due to outliers. . Anyway, we saw earlier that the Durbin-Watson test statistic was within the acceptable range, so the distribution of residuals is alright. We can move on to the next step. . Linearity . The next assumption that we will test is linearity. The predictors must be linearly correlated with the response variable; the relationship should not be too curved or abnormal. . We can test this visually by plotting the actual average SAT scores against the fitted scores which were predicted by our model. . fitted = results.fittedvalues # Scatter plot plt.scatter(fitted, y, label = &quot;Actual data&quot;) # Use OLS model results to get trend line. x_fitting = sm.add_constant(fitted) trend_model = sm.OLS(y, x_fitting) trend_results = trend_model.fit() trend_params = trend_results.params trend_const = trend_params[&quot;const&quot;] trend_coeff = trend_params[0] y_plot = trend_coeff * fitted + trend_const plt.plot(fitted, y_plot, color = &quot;r&quot;, label = &quot;Trend line&quot;) # Extra plot info plt.title(&quot;&quot;&quot;Actual SAT scores against Fitted SAT scores&quot;&quot;&quot;) plt.xlabel(&quot;Fitted SAT Score&quot;) plt.ylabel(&quot;Actual SAT Score&quot;) plt.legend(loc = &quot;best&quot;) plt.grid(True) plt.show() . C: Users migs anaconda3 envs new_streamlit_env2 lib site-packages statsmodels tsa tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument &#39;objs&#39; will be keyword-only x = pd.concat(x[::order], 1) . The trend line above fits the actual data quite well. Moreover, the actual data seems to follow a mostly linear shape. Thus, there is no cause for concern here. . Homoscedasticity . The last assumption that we will test is homoscedasticity. This means that the variance is consistent across all values of residuals. If this is not the case, the model has heteroscedasticity, which is problematic. . This can be tested using the Breusch-Pagan test. The null hypothesis is that heteroscedasticity is not present (which is ideal). A statistically significant p-value supports the claim that there is heteroscedasticity, in which case we would have to adjust the model. . b_labels = [&#39;Lagrange multiplier statistic&#39;, &#39;p-value&#39;, &#39;f-value&#39;, &#39;f p-value&#39;] b_test = sms.het_breuschpagan( residuals, model.exog, ) b_results = pd.DataFrame(zip(b_labels, b_test)) b_results.columns = [&quot;Information&quot;, &quot;Value&quot;] b_results . Information Value . 0 Lagrange multiplier statistic | 1.028960e+02 | . 1 p-value | 6.237518e-20 | . 2 f-value | 2.266603e+01 | . 3 f p-value | 5.711239e-23 | . Both p-values have a factor of around $10^{-20}$. In other words, these are very close to 0, which means that these are significant. Thus, the model is heteroscedastic. . We can make the results robust against heteroscedasticity by specifying an HCCM (Heteroscedasticity Consistent Covariance Matrix). We will specifically choose HC0. It doesn&#39;t perform well for sample sizes under 250 (Python for Data Science), but our sample size is 401, so we can use it. . model = sm.OLS(y, X) results = model.fit( cov_type = &#39;HC0&#39;, # Specify an HCCM. ) summary = results.summary() tables = extract_summary(summary) . Now that we&#39;ve tested all of the assumptions and adjusted for heteroscedasticity, we can move on to partial regression plots. . Partial regression plots . Partial regression plots) are used to visualize the relationship between 1 predictor and the response variable while holding all other predictors constant. . The plots are made below. . def par_reg_plot(data, label, show_dbn = False): &quot;&quot;&quot;Take a predictor label and plot its partial regression plot, which takes the other predictors into account. Note: This function references objects from the global namespace.&quot;&quot;&quot; title = &quot;Partial Regression Plot nSAT Score against `{}`&quot;.format(label) x_label = &quot;`{}` nPartial Residuals&quot;.format(label) # List of all IVs other than the current one. others = [col for col in iv_list if col != label] fig = sm.graphics.plot_partregress( endog = &quot;R_sat_score&quot;, exog_i = label, exog_others = others, data = data, obs_labels = show_dbn, ) plt.title(title) plt.xlabel(x_label) plt.ylabel(&quot;SAT Score nPartial Residuals&quot;) plt.grid(True) plt.legend( labels = [&quot;Actual partial residuals&quot;, &quot;Trend line&quot;], loc = &quot;best&quot;, ) plt.tight_layout( h_pad = 2, ) plt.show() for label in iv_list: par_reg_plot(regression_data, label) . We can observe that: . English Language Learner percentage and Special Education percentage fit the linear trend best. | Female percentage residuals all cluster around one spot and don&#39;t follow a trend. | The other variables, which involve race percentages, seem to be affected by outliers. | . We should keep these plots in mind as we interpret the model results since these don&#39;t show whether outliers may have led to an overestimation of the significance of a predictor. . Interpretation of Results . Below is the first table of model results. . tables[0] . 0 1 2 3 . 0 Dep. Variable: | R_sat_score | R-squared: | 7.860000e-01 | . 1 Model: | OLS | Adj. R-squared: | 7.820000e-01 | . 2 Method: | Least Squares | F-statistic: | 9.644000e+01 | . 3 Date: | Mon, 13 Dec 2021 | Prob (F-statistic): | 3.410000e-74 | . 4 Time: | 10:27:39 | Log-Likelihood: | -2.330700e+03 | . 5 No. Observations: | 401 | AIC: | 4.675000e+03 | . 6 Df Residuals: | 394 | BIC: | 4.703000e+03 | . 7 Df Model: | 6 | NaN | NaN | . 8 Covariance Type: | HC0 | NaN | NaN | . Multiple regression analysis was used to test if a school&#39;s percentages of English Language Learners, Special Education students, female students Asian students, black students, and white students significantly predicted the school&#39;s average total SAT score. . The six predictors explained 78.6% of the variance (R2= 0.786, F(6, 394) = 96.44, p &lt; 0.01). . Next is the second table of results, which shows the beta coefficient, p-value, etc. for each predictor variable. . tables[1] . coef std err z P&gt;|z| [0.025 0.975] VIF . label . const 1430.1978 | 32.665 | 43.784 | 0.00 | 1366.175 | 1494.220 | NaN | . D_ell_percent -5.2206 | 0.320 | -16.290 | 0.00 | -5.849 | -4.592 | 1.600755 | . D_sped_percent -8.6750 | 0.861 | -10.076 | 0.00 | -10.362 | -6.988 | 3.824473 | . D_female_per -0.8097 | 0.316 | -2.564 | 0.01 | -1.429 | -0.191 | 7.289758 | . D_asian_per 3.5967 | 0.524 | 6.861 | 0.00 | 2.569 | 4.624 | 1.788134 | . D_black_per -1.3553 | 0.182 | -7.446 | 0.00 | -1.712 | -0.999 | 3.926733 | . D_white_per 3.8196 | 0.604 | 6.329 | 0.00 | 2.637 | 5.002 | 1.777499 | . The predicted average SAT score in a school is approximately equal to 1430 - 5.22(ELL percentage) - 8.68(SpEd percentage) - 0.81(female percentage) + 3.60(Asian percentage) - 1.36(black percentage) + 3.82(white percentage). . All of the independent variables used in the model were significant predictors of the school&#39;s average SAT score, with p &lt; 0.05. However, since the female percentage variable&#39;s VIF value was greater than 5, strong multicollinearity was present, so its significance value may not be reliably interpreted. . The coefficients for ELL percentage, SpEd percentage, and black student percentage were negative. This means that these factors are associated with lower average SAT score in an NYC high school. . On the other hand, the coefficients for Asian percentage and white percentage were positive. This means that these factors are associated with higher average SAT score in an NYC high school. However, the linear relationship in the partial regression plots appeared to be affected by outliers, so the effect of these variables may not be strong for the majority of high schools. . In general, however, these results present initial evidence that race percentages in an NYC high school may significantly affect its average SAT score. . Conclusion . In this project, we cleaned, transformed, and combined multiple datasets about New York City high schools. We then conducted exploratory data analysis using Pearson&#39;s correlation coefficients and bivariate scatterplots. . Then, multiple linear regression was performed. The model explained 78.6% of the variance (R2= 0.786, F(6, 394) = 96.44, p &lt; 0.01). This supports the claim that the percentage of each race in a school has an effect on the school&#39;s average SAT score. . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/statsmodels/2021/06/13/How-Student-Demographic-Affects-SAT-Performance-NYC.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/statsmodels/2021/06/13/How-Student-Demographic-Affects-SAT-Performance-NYC.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Employee Exit Survey Data Cleaning and Aggregation",
            "content": ". Unsplash | Kon Karampelas Overview . This project deals with messy data from employee exit surveys from 2 government institutions in Queensland, Australia: . Department of Education, Training and Employment (DETE) | Technical and Further Education (TAFE) | . The project aims to determine what percentage of the resignees was dissatisfied with work: . based on age group | based on career stage | . On the technical side, this project is intended to showcase the use of several intermediate Pandas techniques for data cleaning and manipulation, including vectorized methods, mapping functions across data, dropping rows and columns, and combining DataFrames. . . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Clean and Analyze Employee Exit Surveys. The general project flow and research questions came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data Overview . TAFE Survey . The TAFE employee exit survey data can be found here. . Unfortunately, the CSV file itself is no longer available on the Australian government&#39;s websites, so I used a copy that I downloaded from Dataquest. . Columns . Below is information about the TAFE columns. . tafe = pd.read_csv(&quot;./private/2021-06-01-EES-Files/tafe_survey.csv&quot;) tafe.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 702 entries, 0 to 701 Data columns (total 72 columns): # Column Non-Null Count Dtype -- -- 0 Record ID 702 non-null float64 1 Institute 702 non-null object 2 WorkArea 702 non-null object 3 CESSATION YEAR 695 non-null float64 4 Reason for ceasing employment 701 non-null object 5 Contributing Factors. Career Move - Public Sector 437 non-null object 6 Contributing Factors. Career Move - Private Sector 437 non-null object 7 Contributing Factors. Career Move - Self-employment 437 non-null object 8 Contributing Factors. Ill Health 437 non-null object 9 Contributing Factors. Maternity/Family 437 non-null object 10 Contributing Factors. Dissatisfaction 437 non-null object 11 Contributing Factors. Job Dissatisfaction 437 non-null object 12 Contributing Factors. Interpersonal Conflict 437 non-null object 13 Contributing Factors. Study 437 non-null object 14 Contributing Factors. Travel 437 non-null object 15 Contributing Factors. Other 437 non-null object 16 Contributing Factors. NONE 437 non-null object 17 Main Factor. Which of these was the main factor for leaving? 113 non-null object 18 InstituteViews. Topic:1. I feel the senior leadership had a clear vision and direction 608 non-null object 19 InstituteViews. Topic:2. I was given access to skills training to help me do my job better 613 non-null object 20 InstituteViews. Topic:3. I was given adequate opportunities for personal development 610 non-null object 21 InstituteViews. Topic:4. I was given adequate opportunities for promotion within %Institute]Q25LBL% 608 non-null object 22 InstituteViews. Topic:5. I felt the salary for the job was right for the responsibilities I had 615 non-null object 23 InstituteViews. Topic:6. The organisation recognised when staff did good work 607 non-null object 24 InstituteViews. Topic:7. Management was generally supportive of me 614 non-null object 25 InstituteViews. Topic:8. Management was generally supportive of my team 608 non-null object 26 InstituteViews. Topic:9. I was kept informed of the changes in the organisation which would affect me 610 non-null object 27 InstituteViews. Topic:10. Staff morale was positive within the Institute 602 non-null object 28 InstituteViews. Topic:11. If I had a workplace issue it was dealt with quickly 601 non-null object 29 InstituteViews. Topic:12. If I had a workplace issue it was dealt with efficiently 597 non-null object 30 InstituteViews. Topic:13. If I had a workplace issue it was dealt with discreetly 601 non-null object 31 WorkUnitViews. Topic:14. I was satisfied with the quality of the management and supervision within my work unit 609 non-null object 32 WorkUnitViews. Topic:15. I worked well with my colleagues 605 non-null object 33 WorkUnitViews. Topic:16. My job was challenging and interesting 607 non-null object 34 WorkUnitViews. Topic:17. I was encouraged to use my initiative in the course of my work 610 non-null object 35 WorkUnitViews. Topic:18. I had sufficient contact with other people in my job 613 non-null object 36 WorkUnitViews. Topic:19. I was given adequate support and co-operation by my peers to enable me to do my job 609 non-null object 37 WorkUnitViews. Topic:20. I was able to use the full range of my skills in my job 609 non-null object 38 WorkUnitViews. Topic:21. I was able to use the full range of my abilities in my job. ; Category:Level of Agreement; Question:YOUR VIEWS ABOUT YOUR WORK UNIT] 608 non-null object 39 WorkUnitViews. Topic:22. I was able to use the full range of my knowledge in my job 608 non-null object 40 WorkUnitViews. Topic:23. My job provided sufficient variety 611 non-null object 41 WorkUnitViews. Topic:24. I was able to cope with the level of stress and pressure in my job 610 non-null object 42 WorkUnitViews. Topic:25. My job allowed me to balance the demands of work and family to my satisfaction 611 non-null object 43 WorkUnitViews. Topic:26. My supervisor gave me adequate personal recognition and feedback on my performance 606 non-null object 44 WorkUnitViews. Topic:27. My working environment was satisfactory e.g. sufficient space, good lighting, suitable seating and working area 610 non-null object 45 WorkUnitViews. Topic:28. I was given the opportunity to mentor and coach others in order for me to pass on my skills and knowledge prior to my cessation date 609 non-null object 46 WorkUnitViews. Topic:29. There was adequate communication between staff in my unit 603 non-null object 47 WorkUnitViews. Topic:30. Staff morale was positive within my work unit 606 non-null object 48 Induction. Did you undertake Workplace Induction? 619 non-null object 49 InductionInfo. Topic:Did you undertake a Corporate Induction? 432 non-null object 50 InductionInfo. Topic:Did you undertake a Institute Induction? 483 non-null object 51 InductionInfo. Topic: Did you undertake Team Induction? 440 non-null object 52 InductionInfo. Face to Face Topic:Did you undertake a Corporate Induction; Category:How it was conducted? 555 non-null object 53 InductionInfo. On-line Topic:Did you undertake a Corporate Induction; Category:How it was conducted? 555 non-null object 54 InductionInfo. Induction Manual Topic:Did you undertake a Corporate Induction? 555 non-null object 55 InductionInfo. Face to Face Topic:Did you undertake a Institute Induction? 530 non-null object 56 InductionInfo. On-line Topic:Did you undertake a Institute Induction? 555 non-null object 57 InductionInfo. Induction Manual Topic:Did you undertake a Institute Induction? 553 non-null object 58 InductionInfo. Face to Face Topic: Did you undertake Team Induction; Category? 555 non-null object 59 InductionInfo. On-line Topic: Did you undertake Team Induction?process you undertook and how it was conducted.] 555 non-null object 60 InductionInfo. Induction Manual Topic: Did you undertake Team Induction? 555 non-null object 61 Workplace. Topic:Did you and your Manager develop a Performance and Professional Development Plan (PPDP)? 608 non-null object 62 Workplace. Topic:Does your workplace promote a work culture free from all forms of unlawful discrimination? 594 non-null object 63 Workplace. Topic:Does your workplace promote and practice the principles of employment equity? 587 non-null object 64 Workplace. Topic:Does your workplace value the diversity of its employees? 586 non-null object 65 Workplace. Topic:Would you recommend the Institute as an employer to others? 581 non-null object 66 Gender. What is your Gender? 596 non-null object 67 CurrentAge. Current Age 596 non-null object 68 Employment Type. Employment Type 596 non-null object 69 Classification. Classification 596 non-null object 70 LengthofServiceOverall. Overall Length of Service at Institute (in years) 596 non-null object 71 LengthofServiceCurrent. Length of Service at current workplace (in years) 596 non-null object dtypes: float64(2), object(70) memory usage: 395.0+ KB . The formatting is different because some column names are apparently too long. However, we can see that: . There are 702 rows and 72 columns. | A few columns contain decimals, and most contain text. | Many of the columns have missing values. | . Dataquest notes a few columns in this dataset: . &quot;Record ID: An id used to identify the participant of the survey&quot; | &quot;Reason for ceasing employment: The reason why the person&#39;s employment ended&quot; | &quot;LengthofServiceOverall. Overall Length of Service at Institute (in years): The length of the person&#39;s employment (in years)&quot; | . Additionally, there are groups of columns that all start with the same name. These group names are: . Contributing Factors | Institute Views | Work Unit Views | Induction Info | Workplace | . Currently, there are too many columns for analysis. However, since the Contributing Factors columns are directly related to the employee&#39;s resignation, we could just keep those and remove the other 4 groups of columns. . Descriptive Statistics . Below are descriptive statistics for the columns. . tafe.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . Record ID Institute WorkArea CESSATION YEAR Reason for ceasing employment Contributing Factors. Career Move - Public Sector Contributing Factors. Career Move - Private Sector Contributing Factors. Career Move - Self-employment Contributing Factors. Ill Health Contributing Factors. Maternity/Family ... Workplace. Topic:Does your workplace promote a work culture free from all forms of unlawful discrimination? Workplace. Topic:Does your workplace promote and practice the principles of employment equity? Workplace. Topic:Does your workplace value the diversity of its employees? Workplace. Topic:Would you recommend the Institute as an employer to others? Gender. What is your Gender? CurrentAge. Current Age Employment Type. Employment Type Classification. Classification LengthofServiceOverall. Overall Length of Service at Institute (in years) LengthofServiceCurrent. Length of Service at current workplace (in years) . count 7.020000e+02 | 702 | 702 | 695.000000 | 701 | 437 | 437 | 437 | 437 | 437 | ... | 594 | 587 | 586 | 581 | 596 | 596 | 596 | 596 | 596 | 596 | . unique NaN | 12 | 2 | NaN | 6 | 2 | 2 | 2 | 2 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 9 | 5 | 9 | 7 | 7 | . top NaN | Brisbane North Institute of TAFE | Non-Delivery (corporate) | NaN | Resignation | - | - | - | - | - | ... | Yes | Yes | Yes | Yes | Female | 56 or older | Permanent Full-time | Administration (AO) | Less than 1 year | Less than 1 year | . freq NaN | 161 | 432 | NaN | 340 | 375 | 336 | 420 | 403 | 411 | ... | 536 | 512 | 488 | 416 | 389 | 162 | 237 | 293 | 147 | 177 | . mean 6.346026e+17 | NaN | NaN | 2011.423022 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . std 2.515071e+14 | NaN | NaN | 0.905977 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . min 6.341330e+17 | NaN | NaN | 2009.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 25% 6.343954e+17 | NaN | NaN | 2011.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 50% 6.345835e+17 | NaN | NaN | 2011.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 75% 6.348005e+17 | NaN | NaN | 2012.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . max 6.350730e+17 | NaN | NaN | 2013.000000 | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 11 rows × 72 columns . Some text columns appear to have values that only contain a single hyphen (-); these will have to be investigated later. . DETE Survey . The DETE employee exit survey can be found here. The copy used in this project is a slightly modified version downloaded from Dataquest for convenience. It is still complete in terms of the number of entries. . Columns . Below are the column names and types. . dete = pd.read_csv(&quot;./private/2021-06-01-EES-Files/dete_survey.csv&quot;) dete.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 822 entries, 0 to 821 Data columns (total 56 columns): # Column Non-Null Count Dtype -- -- 0 ID 822 non-null int64 1 SeparationType 822 non-null object 2 Cease Date 822 non-null object 3 DETE Start Date 822 non-null object 4 Role Start Date 822 non-null object 5 Position 817 non-null object 6 Classification 455 non-null object 7 Region 822 non-null object 8 Business Unit 126 non-null object 9 Employment Status 817 non-null object 10 Career move to public sector 822 non-null bool 11 Career move to private sector 822 non-null bool 12 Interpersonal conflicts 822 non-null bool 13 Job dissatisfaction 822 non-null bool 14 Dissatisfaction with the department 822 non-null bool 15 Physical work environment 822 non-null bool 16 Lack of recognition 822 non-null bool 17 Lack of job security 822 non-null bool 18 Work location 822 non-null bool 19 Employment conditions 822 non-null bool 20 Maternity/family 822 non-null bool 21 Relocation 822 non-null bool 22 Study/Travel 822 non-null bool 23 Ill Health 822 non-null bool 24 Traumatic incident 822 non-null bool 25 Work life balance 822 non-null bool 26 Workload 822 non-null bool 27 None of the above 822 non-null bool 28 Professional Development 808 non-null object 29 Opportunities for promotion 735 non-null object 30 Staff morale 816 non-null object 31 Workplace issue 788 non-null object 32 Physical environment 817 non-null object 33 Worklife balance 815 non-null object 34 Stress and pressure support 810 non-null object 35 Performance of supervisor 813 non-null object 36 Peer support 812 non-null object 37 Initiative 813 non-null object 38 Skills 811 non-null object 39 Coach 767 non-null object 40 Career Aspirations 746 non-null object 41 Feedback 792 non-null object 42 Further PD 768 non-null object 43 Communication 814 non-null object 44 My say 812 non-null object 45 Information 816 non-null object 46 Kept informed 813 non-null object 47 Wellness programs 766 non-null object 48 Health &amp; Safety 793 non-null object 49 Gender 798 non-null object 50 Age 811 non-null object 51 Aboriginal 16 non-null object 52 Torres Strait 3 non-null object 53 South Sea 7 non-null object 54 Disability 23 non-null object 55 NESB 32 non-null object dtypes: bool(18), int64(1), object(37) memory usage: 258.6+ KB . ID contains integers, whereas all other columns either contain text or booleans. Also, many of the columns have missing values. . There are 822 rows (employees) and 56 columns (survey questions). Dataquest describes a few notable columns as follows: . &quot;ID: An id used to identify the participant of the survey&quot; | &quot;SeparationType: The reason why the person&#39;s employment ended&quot; | &quot;Cease Date: The year or month the person&#39;s employment ended&quot; | &quot;DETE Start Date: The year the person began employment with the DETE&quot; | . Also, if we look closely, we can see that many of the columns in the DETE dataset match columns in the TAFE dataset. For example: . print(&quot;TAFE Columns&quot;) print(list(tafe.columns[5:17])) print(&quot; nDETE Columns&quot;) print(list(dete.columns[10:28])) . TAFE Columns [&#39;Contributing Factors. Career Move - Public Sector &#39;, &#39;Contributing Factors. Career Move - Private Sector &#39;, &#39;Contributing Factors. Career Move - Self-employment&#39;, &#39;Contributing Factors. Ill Health&#39;, &#39;Contributing Factors. Maternity/Family&#39;, &#39;Contributing Factors. Dissatisfaction&#39;, &#39;Contributing Factors. Job Dissatisfaction&#39;, &#39;Contributing Factors. Interpersonal Conflict&#39;, &#39;Contributing Factors. Study&#39;, &#39;Contributing Factors. Travel&#39;, &#39;Contributing Factors. Other&#39;, &#39;Contributing Factors. NONE&#39;] DETE Columns [&#39;Career move to public sector&#39;, &#39;Career move to private sector&#39;, &#39;Interpersonal conflicts&#39;, &#39;Job dissatisfaction&#39;, &#39;Dissatisfaction with the department&#39;, &#39;Physical work environment&#39;, &#39;Lack of recognition&#39;, &#39;Lack of job security&#39;, &#39;Work location&#39;, &#39;Employment conditions&#39;, &#39;Maternity/family&#39;, &#39;Relocation&#39;, &#39;Study/Travel&#39;, &#39;Ill Health&#39;, &#39;Traumatic incident&#39;, &#39;Work life balance&#39;, &#39;Workload&#39;, &#39;None of the above&#39;] . These two sets of columns represent the &quot;Contributing Factors&quot; group in each survey. These are some of the columns that we want to keep for analysis; this will be addressed in data cleaning later. . Descriptive Statistics . We can view descriptive statistics for all columns below. . dete.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . ID SeparationType Cease Date DETE Start Date Role Start Date Position Classification Region Business Unit Employment Status ... Kept informed Wellness programs Health &amp; Safety Gender Age Aboriginal Torres Strait South Sea Disability NESB . count 822.000000 | 822 | 822 | 822 | 822 | 817 | 455 | 822 | 126 | 817 | ... | 813 | 766 | 793 | 798 | 811 | 16 | 3 | 7 | 23 | 32 | . unique NaN | 9 | 25 | 51 | 46 | 15 | 8 | 9 | 14 | 5 | ... | 6 | 6 | 6 | 2 | 10 | 1 | 1 | 1 | 1 | 1 | . top NaN | Age Retirement | 2012 | Not Stated | Not Stated | Teacher | Primary | Metropolitan | Education Queensland | Permanent Full-time | ... | A | A | A | Female | 61 or older | Yes | Yes | Yes | Yes | Yes | . freq NaN | 285 | 344 | 73 | 98 | 324 | 161 | 135 | 54 | 434 | ... | 401 | 253 | 386 | 573 | 222 | 16 | 3 | 7 | 23 | 32 | . mean 411.693431 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . std 237.705820 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . min 1.000000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 25% 206.250000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 50% 411.500000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 75% 616.750000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . max 823.000000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 11 rows × 56 columns . The Cease Date, DETE Start Date, and Role Start Date columns are interesting because these are in text format and some of the values are &quot;Not Stated&quot;. . Also, based on counts of non-null values shown earlier, some columns have many missing values. These will have to be addressed in data cleaning. . Data Cleaning . Placeholders for Missing Values . Earlier, we noticed that some columns in the DETE data contain &quot;Not Stated&quot; values. These are likely to be placeholders for missing data. . Therefore, we can replace all &quot;Not Stated&quot; values with np.nan null values. . dete = dete.replace(&quot;Not Stated&quot;, np.nan) . Dropping Columns . In the TAFE dataset, there are 4 other big groups of columns other than Contributing Factors: . Institute Views | Work Unit Views | Induction Info | Workplace | . We want to remove these columns in order to limit the columns in our dataset to the most relevant ones. This is done below. . tafe = tafe.drop( labels = tafe.columns[17:66], axis = 1, ) list(tafe.columns) . [&#39;Record ID&#39;, &#39;Institute&#39;, &#39;WorkArea&#39;, &#39;CESSATION YEAR&#39;, &#39;Reason for ceasing employment&#39;, &#39;Contributing Factors. Career Move - Public Sector &#39;, &#39;Contributing Factors. Career Move - Private Sector &#39;, &#39;Contributing Factors. Career Move - Self-employment&#39;, &#39;Contributing Factors. Ill Health&#39;, &#39;Contributing Factors. Maternity/Family&#39;, &#39;Contributing Factors. Dissatisfaction&#39;, &#39;Contributing Factors. Job Dissatisfaction&#39;, &#39;Contributing Factors. Interpersonal Conflict&#39;, &#39;Contributing Factors. Study&#39;, &#39;Contributing Factors. Travel&#39;, &#39;Contributing Factors. Other&#39;, &#39;Contributing Factors. NONE&#39;, &#39;Gender. What is your Gender?&#39;, &#39;CurrentAge. Current Age&#39;, &#39;Employment Type. Employment Type&#39;, &#39;Classification. Classification&#39;, &#39;LengthofServiceOverall. Overall Length of Service at Institute (in years)&#39;, &#39;LengthofServiceCurrent. Length of Service at current workplace (in years)&#39;] . There are now only 23 columns in the TAFE dataset. . Earlier, in the Data Overview, we mentioned that DETE has similar columns to the ones in TAFE. For example, look at the columns from indices 28 to 48: . dete.columns[28:49] . Index([&#39;Professional Development&#39;, &#39;Opportunities for promotion&#39;, &#39;Staff morale&#39;, &#39;Workplace issue&#39;, &#39;Physical environment&#39;, &#39;Worklife balance&#39;, &#39;Stress and pressure support&#39;, &#39;Performance of supervisor&#39;, &#39;Peer support&#39;, &#39;Initiative&#39;, &#39;Skills&#39;, &#39;Coach&#39;, &#39;Career Aspirations&#39;, &#39;Feedback&#39;, &#39;Further PD&#39;, &#39;Communication&#39;, &#39;My say&#39;, &#39;Information&#39;, &#39;Kept informed&#39;, &#39;Wellness programs&#39;, &#39;Health &amp; Safety&#39;], dtype=&#39;object&#39;) . These are equivalent to the TAFE columns under the Institute Views, Work Unit Views, Induction Info, and Workspace groups. We don&#39;t need these groups since the Contributing Factors group is directly related to the reason why the employees resigned. . Thus, we will remove the columns shown above from the DETE dataset. . dete = dete.drop( dete.columns[28:49], axis = 1, ) len(dete.columns) . 35 . Matching Columns in TAFE and DETE . Below are some important columns in TAFE and DETE which have matching information. . DETE Survey TAFE Survey . ID | Record ID | . SeparationType | Reason for ceasing employment | . Cease Date | CESSATION YEAR | . DETE Start Date | LengthofServiceOverall. Overall Length of Service at Institute (in years) | . Age | CurrentAge. Current Age | . Gender | Gender. What is your Gender? | . Notably, DETE Start Date and LengthofServiceOverall are matching columns because one can tell how long the employee has been working based on the date when they first started working. . We want to make the column names the same between the two datasets. . Before we do that, we will simplify the names in DETE. . dete.columns = ( dete.columns .str.lower() # All lowercase .str.strip() # Remove whitespace on sides .str.replace(&quot; &quot;, &quot;_&quot;) # Replace spaces with underscores ) dete.columns . Index([&#39;id&#39;, &#39;separationtype&#39;, &#39;cease_date&#39;, &#39;dete_start_date&#39;, &#39;role_start_date&#39;, &#39;position&#39;, &#39;classification&#39;, &#39;region&#39;, &#39;business_unit&#39;, &#39;employment_status&#39;, &#39;career_move_to_public_sector&#39;, &#39;career_move_to_private_sector&#39;, &#39;interpersonal_conflicts&#39;, &#39;job_dissatisfaction&#39;, &#39;dissatisfaction_with_the_department&#39;, &#39;physical_work_environment&#39;, &#39;lack_of_recognition&#39;, &#39;lack_of_job_security&#39;, &#39;work_location&#39;, &#39;employment_conditions&#39;, &#39;maternity/family&#39;, &#39;relocation&#39;, &#39;study/travel&#39;, &#39;ill_health&#39;, &#39;traumatic_incident&#39;, &#39;work_life_balance&#39;, &#39;workload&#39;, &#39;none_of_the_above&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;aboriginal&#39;, &#39;torres_strait&#39;, &#39;south_sea&#39;, &#39;disability&#39;, &#39;nesb&#39;], dtype=&#39;object&#39;) . The DETE columns have been simplified. . Next, we will change some of the TAFE column labels to match the ones in the DETE dataset. . new_columns = { &#39;Record ID&#39;: &#39;id&#39;, &#39;CESSATION YEAR&#39;: &#39;cease_date&#39;, &#39;Reason for ceasing employment&#39;: &#39;separationtype&#39;, &#39;Gender. What is your Gender?&#39;: &#39;gender&#39;, &#39;CurrentAge. Current Age&#39;: &#39;age&#39;, &#39;Employment Type. Employment Type&#39;: &#39;employment_status&#39;, &#39;Classification. Classification&#39;: &#39;position&#39;, &#39;LengthofServiceOverall. Overall Length of Service at Institute (in years)&#39;: &#39;institute_service&#39;, &#39;LengthofServiceCurrent. Length of Service at current workplace (in years)&#39;: &#39;role_service&#39;, } tafe = tafe.rename( new_columns, axis = 1, ) list(tafe.columns) . [&#39;id&#39;, &#39;Institute&#39;, &#39;WorkArea&#39;, &#39;cease_date&#39;, &#39;separationtype&#39;, &#39;Contributing Factors. Career Move - Public Sector &#39;, &#39;Contributing Factors. Career Move - Private Sector &#39;, &#39;Contributing Factors. Career Move - Self-employment&#39;, &#39;Contributing Factors. Ill Health&#39;, &#39;Contributing Factors. Maternity/Family&#39;, &#39;Contributing Factors. Dissatisfaction&#39;, &#39;Contributing Factors. Job Dissatisfaction&#39;, &#39;Contributing Factors. Interpersonal Conflict&#39;, &#39;Contributing Factors. Study&#39;, &#39;Contributing Factors. Travel&#39;, &#39;Contributing Factors. Other&#39;, &#39;Contributing Factors. NONE&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;employment_status&#39;, &#39;position&#39;, &#39;institute_service&#39;, &#39;role_service&#39;] . The Contributing Factors columns&#39; names haven&#39;t been changed yet, but this will be dealt with later. . Identifying Employees who Resigned . The DETE and TAFE exit surveys were given to all employees who left the institutions. Some of them were terminated from employment, some resigned, and some retired. . The goal of this project is to find out why employees resigned. Thus, we have to find out who resigned, and drop the data for the rest. . Let&#39;s do this for the DETE dataset first. The separationtype column explains the reason why the employee ceased to work at the institution. What values does this column contain? . dete[&quot;separationtype&quot;].value_counts() . Age Retirement 285 Resignation-Other reasons 150 Resignation-Other employer 91 Resignation-Move overseas/interstate 70 Voluntary Early Retirement (VER) 67 Ill Health Retirement 61 Other 49 Contract Expired 34 Termination 15 Name: separationtype, dtype: int64 . It looks like the values that are relevant to us are the ones that start with &quot;Resignation.&quot; We&#39;ll keep the rows which indicate resignation and drop the rest. . dete = dete.loc[ # Check if the string starts with &quot;Resignation.&quot; dete[&quot;separationtype&quot;].str.startswith(&quot;Resignation&quot;) ] dete.shape . (311, 35) . After we dropped non-resignation rows, the DETE dataset was left with 311 rows. . Next, we&#39;ll do the same for the TAFE dataset. What are the values in its separationtype column? . tafe[&quot;separationtype&quot;].value_counts() . Resignation 340 Contract Expired 127 Retrenchment/ Redundancy 104 Retirement 82 Transfer 25 Termination 23 Name: separationtype, dtype: int64 . This time, there is only one value which indicates resignation. We&#39;ll use that to identify the rows to keep. . tafe = tafe.loc[ # Check if the value is &quot;Resignation.&quot; tafe[&quot;separationtype&quot;] == &quot;Resignation&quot; ] tafe.shape . (340, 23) . Dropping rows resulted in having 340 rows left in the TAFE dataset. . Now, both datasets have been narrowed down to data about employees who intentionally left the institutions. . Date Columns . Next, we&#39;ll clean and inspect date columns. Specifically, these are: . DETE dataset dete_start_date: The date when the employee started to work at DETE. | cease_date: The date when the employee ceased to work at DETE. | . | TAFE dataset cease_date: The date when the employee ceased to work at TAFE. | . | . Let&#39;s start with DETE&#39;s cease_date. What are its values? . dete[&quot;cease_date&quot;].value_counts() . 2012 126 2013 74 01/2014 22 12/2013 17 06/2013 14 09/2013 11 07/2013 9 11/2013 9 10/2013 6 08/2013 4 05/2012 2 05/2013 2 07/2012 1 2010 1 09/2010 1 07/2006 1 Name: cease_date, dtype: int64 . We can see that some values only contain years, and others state a month before the year. Since we can&#39;t assume the month for entries without one, we will remove all of the months. Only the years will remain, and we will store them as numerical data. . dete[&quot;cease_date&quot;] = ( dete[&quot;cease_date&quot;] .str.extract(&quot;(20[0-1][0-9])&quot;) # Extract year using a regular expression .astype(np.float64) # Turn years into decimals ) dete[&quot;cease_date&quot;].value_counts().sort_index() . 2006.0 1 2010.0 2 2012.0 129 2013.0 146 2014.0 22 Name: cease_date, dtype: int64 . The DETE cease_date column now only contains year values. These range from 2006 to 2014. . Next, let&#39;s look at DETE&#39;s dete_start_date column. . dete[&quot;dete_start_date&quot;].value_counts().sort_index() . 1963 1 1971 1 1972 1 1973 1 1974 2 1975 1 1976 2 1977 1 1980 5 1982 1 1983 2 1984 1 1985 3 1986 3 1987 1 1988 4 1989 4 1990 5 1991 4 1992 6 1993 5 1994 6 1995 4 1996 6 1997 5 1998 6 1999 8 2000 9 2001 3 2002 6 2003 6 2004 14 2005 15 2006 13 2007 21 2008 22 2009 13 2010 17 2011 24 2012 21 2013 10 Name: dete_start_date, dtype: int64 . The column contains only years, no months, so it is quite clean. Most of the values are from 2004 to 2013. The years in the late 1900&#39;s don&#39;t seem like outliers because there are many values spread throughout those years. . It also makes sense that there are no dete_start_date values after 2014, since the latest cease_date is 2014. . There&#39;s no need to clean this column, but let&#39;s convert it to numerical data for consistency. . dete[&quot;dete_start_date&quot;] = dete[&quot;dete_start_date&quot;].astype(np.float64) . Lastly, let&#39;s look at the TAFE dataset&#39;s cease_date column. . tafe[&quot;cease_date&quot;].value_counts().sort_index() . 2009.0 2 2010.0 68 2011.0 116 2012.0 94 2013.0 55 Name: cease_date, dtype: int64 . The data here looks like it&#39;s already clean. The years are expressed as decimals, and there are no outliers to clean up. This column won&#39;t be changed. . The TAFE cease_date years range from 2009 to 2013. DETE&#39;s cease_date values range from 2006 to 2014. Therefore, both datasets give information about roughly the same period in time. . Years of Service . Remember that one of the goals of the project is to compare dissatisfaction rates between resignees who had worked for a short time and those who had worked for a longer time. Thus, we need to know how many years of service each employee has had. . The TAFE dataset already has a column called institute_service which gives information on this. . tafe[&quot;institute_service&quot;].value_counts().sort_index() . 1-2 64 11-20 26 3-4 63 5-6 33 7-10 21 Less than 1 year 73 More than 20 years 10 Name: institute_service, dtype: int64 . These values are somewhat difficult to use since these indicate ranges of years of service. This will be dealt with later, but for now, we have to make a matching column in the DETE dataset. . In order to do this, we will subtract dete_start_date from cease_date. This will result in the number of years that each employee has spent working at DETE. The new column will be called institute_service like in the TAFE dataset. . dete[&quot;institute_service&quot;] = dete[&quot;cease_date&quot;] - dete[&quot;dete_start_date&quot;] dete[&quot;institute_service&quot;].value_counts(bins = 10) . (-0.05, 4.9] 92 (4.9, 9.8] 75 (9.8, 14.7] 30 (14.7, 19.6] 26 (19.6, 24.5] 24 (24.5, 29.4] 8 (29.4, 34.3] 8 (34.3, 39.2] 7 (39.2, 44.1] 2 (44.1, 49.0] 1 Name: institute_service, dtype: int64 . It can be seen that DETE employees&#39; years of service range from under 4.9 to over 44.1. Let&#39;s view the distribution in a histogram. . sns.histplot( data = dete, x = &quot;institute_service&quot;, ) plt.title(&quot;DETE Employees&#39; Years of Service&quot;) plt.xlabel(&quot;Years of Service&quot;) plt.grid(True) plt.show() . The distribution is right-skewed. Most employees who resigned had worked at DETE for under 10 years. . &quot;Contributing Factors&quot; Columns . The Contributing Factors columns are about factors which may have influenced the employee&#39;s choice to resign. In the TAFE dataset, these columns have hyphen (&quot;-&quot;) values. This leads us to wonder what they represent, and whether or not we have to clean them. . Let&#39;s look at column 5, one of the columns with hyphens. . tafe.iloc[:, 5].value_counts(dropna = False) . - 284 Career Move - Public Sector 48 NaN 8 Name: Contributing Factors. Career Move - Public Sector , dtype: int64 . Most of the values are hyphens. The rest are null values or &quot;Career Move - Public Sector&quot;. . We can infer that the Contributing Factors group of columns represent options in a checkbox item in the survey. That&#39;s why each column only has 2 valid values: . &quot;-&quot; means that the option was not selected. | &quot;Career Move - Public Sector&quot; means that the option was selected. | . Thus, we can change the values in column 5 into True (selected) and False (not selected) for ease of use. . def identify_selection(value): if pd.isnull(value): return np.nan else: return value != &quot;-&quot; # Apply the function elementwise. tafe.iloc[:, 5] = tafe.iloc[:, 5].apply(identify_selection) tafe.iloc[:, 5].value_counts(dropna = False) . False 284 True 48 NaN 8 Name: Contributing Factors. Career Move - Public Sector , dtype: int64 . Now, the column only has True, False, and NaN values. . Let us apply this transformaton to the entire group of &quot;Contributing Factors&quot; columns (5 to 16). . tafe.iloc[:, 6:17] = tafe.iloc[:, 6:17].applymap(identify_selection) tafe.iloc[:, 5:17].head() . Contributing Factors. Career Move - Public Sector Contributing Factors. Career Move - Private Sector Contributing Factors. Career Move - Self-employment Contributing Factors. Ill Health Contributing Factors. Maternity/Family Contributing Factors. Dissatisfaction Contributing Factors. Job Dissatisfaction Contributing Factors. Interpersonal Conflict Contributing Factors. Study Contributing Factors. Travel Contributing Factors. Other Contributing Factors. NONE . 3 False | False | False | False | False | False | False | False | False | True | False | False | . 4 False | True | False | False | False | False | False | False | False | False | False | False | . 5 False | False | False | False | False | False | False | False | False | False | True | False | . 6 False | True | False | False | True | False | False | False | False | False | True | False | . 7 False | False | False | False | False | False | False | False | False | False | True | False | . Since these columns are now boolean columns, these will be easier to use in analysis. . It is worth noting that what we did matches the format used for Contributing Factors columns in DETE: . dete.iloc[:5, 10:28] . career_move_to_public_sector career_move_to_private_sector interpersonal_conflicts job_dissatisfaction dissatisfaction_with_the_department physical_work_environment lack_of_recognition lack_of_job_security work_location employment_conditions maternity/family relocation study/travel ill_health traumatic_incident work_life_balance workload none_of_the_above . 3 False | True | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | . 5 False | True | False | False | False | False | False | False | False | True | True | False | False | False | False | False | False | False | . 8 False | True | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | . 9 False | False | True | True | True | False | False | False | False | False | False | False | False | False | False | False | False | False | . 11 False | False | False | False | False | False | False | False | False | False | True | True | False | False | False | False | False | False | . Thus, we will be able to use similar techniques to identify dissatisfaction in both datasets. . Identifying Dissatisfaction . This project focuses on employees who resigned due to dissatisfaction with their work at the government institute. Thus, we have to identify which employees were dissatisfied. . In the TAFE dataset, the following columns indicate dissatisfaction: . Contributing Factors. Dissatisfaction | Contributing Factors. Job Dissatisfaction | . We will create a new dissatisfaction column. It will be a boolean column that contains True if at least 1 of the above columns has a value of True. . tafe[&quot;dissatisfaction&quot;] = ( tafe[[ &quot;Contributing Factors. Dissatisfaction&quot;, &quot;Contributing Factors. Job Dissatisfaction&quot;, ]] .any(axis = 1, skipna = False) ) tafe[&quot;dissatisfaction&quot;].value_counts(dropna = False) . False 241 True 99 Name: dissatisfaction, dtype: int64 . It looks like 91 employees in the TAFE dataset resigned due to dissatisfaction. . As for the DETE dataset, there are many relevant columns. . job_dissatisfaction | dissatisfaction_with_the_department | physical_work_environment | lack_of_recognition | lack_of_job_security | work_location | employment_conditions | work_life_balance | workload | . Remember that in the survey, these phrases were options in a checkbox item asking why the employee left. A True value in any of these columns means that the employee considered it a reason why he/she resigned. . We will create a new dissatisfaction column in the DETE dataset in the same way as we did for the TAFE dataset. If at least 1 of the above columns is True, the corresponding value in the new column will be True. . dete[&quot;dissatisfaction&quot;] = ( dete[[ &#39;job_dissatisfaction&#39;, &#39;dissatisfaction_with_the_department&#39;, &#39;physical_work_environment&#39;, &#39;lack_of_recognition&#39;, &#39;lack_of_job_security&#39;, &#39;work_location&#39;, &#39;employment_conditions&#39;, &#39;work_life_balance&#39;, &#39;workload&#39;, ]] .any(axis = 1, skipna = False) ) dete[&quot;dissatisfaction&quot;].value_counts(dropna = False) . False 162 True 149 Name: dissatisfaction, dtype: int64 . The results show that 149 of DETE employees who resigned had been dissatisfied. . Combining DETE and TAFE . At this point, the data cleaning we&#39;ve done is sufficient for us to combine the two datasets. We will stack them vertically; they will share columns with identical names. . We will still need to be able to differentiate between DETE and TAFE employees, so we will indicate this in a new column. . In the DETE dataset, the new institute column will contain the string &quot;DETE&quot;. . dete[&quot;institute&quot;] = &quot;DETE&quot; dete[&quot;institute&quot;].head() . 3 DETE 5 DETE 8 DETE 9 DETE 11 DETE Name: institute, dtype: object . The value in the TAFE dataset will be &quot;TAFE&quot;. . tafe[&quot;institute&quot;] = &quot;TAFE&quot; tafe[&quot;institute&quot;].head() . 3 TAFE 4 TAFE 5 TAFE 6 TAFE 7 TAFE Name: institute, dtype: object . Let&#39;s now concatenate the 2 datasets vertically. . combined = pd.concat( [dete, tafe], axis = 0, # Vertical concatenation ignore_index = True, ) print(combined.shape) combined.head() . (651, 53) . id separationtype cease_date dete_start_date role_start_date position classification region business_unit employment_status ... Contributing Factors. Ill Health Contributing Factors. Maternity/Family Contributing Factors. Dissatisfaction Contributing Factors. Job Dissatisfaction Contributing Factors. Interpersonal Conflict Contributing Factors. Study Contributing Factors. Travel Contributing Factors. Other Contributing Factors. NONE role_service . 0 4.0 | Resignation-Other reasons | 2012.0 | 2005.0 | 2006 | Teacher | Primary | Central Queensland | NaN | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 6.0 | Resignation-Other reasons | 2012.0 | 1994.0 | 1997 | Guidance Officer | NaN | Central Office | Education Queensland | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 9.0 | Resignation-Other reasons | 2012.0 | 2009.0 | 2009 | Teacher | Secondary | North Queensland | NaN | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 10.0 | Resignation-Other employer | 2012.0 | 1997.0 | 2008 | Teacher Aide | NaN | NaN | NaN | Permanent Part-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 12.0 | Resignation-Move overseas/interstate | 2012.0 | 2009.0 | 2009 | Teacher | Secondary | Far North Queensland | NaN | Permanent Full-time | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 53 columns . The combined dataset has 651 rows (employees) and 53 columns. . Given our research questions, the most important columns to keep are institute_service, age, and dissatisfaction. We would only keep the other columns if we wanted to group the employees on certain characteristics. . Thus, we will remove columns that have under 500 non-null values. These columns wouldn&#39;t have enough useful information for us to use. . combined.dropna( axis = 1, thresh = 500, inplace = True, ) print(combined.shape) combined.head() . (651, 10) . id separationtype cease_date position employment_status gender age institute_service dissatisfaction institute . 0 4.0 | Resignation-Other reasons | 2012.0 | Teacher | Permanent Full-time | Female | 36-40 | 7.0 | False | DETE | . 1 6.0 | Resignation-Other reasons | 2012.0 | Guidance Officer | Permanent Full-time | Female | 41-45 | 18.0 | True | DETE | . 2 9.0 | Resignation-Other reasons | 2012.0 | Teacher | Permanent Full-time | Female | 31-35 | 3.0 | False | DETE | . 3 10.0 | Resignation-Other employer | 2012.0 | Teacher Aide | Permanent Part-time | Female | 46-50 | 15.0 | True | DETE | . 4 12.0 | Resignation-Move overseas/interstate | 2012.0 | Teacher | Permanent Full-time | Male | 31-35 | 3.0 | False | DETE | . Only 10 columns were left in the dataset. The 3 most important columns were kept, along with a few columns about useful demographic data. . Cleaning Age Data . One of the goals of this project involves the age data. Thus, we need to ensure that this data is clean. . Let us view the unique values in the column. . combined[&quot;age&quot;].value_counts().sort_index() . 20 or younger 10 21 25 33 21-25 29 26 30 32 26-30 35 31 35 32 31-35 29 36 40 32 36-40 41 41 45 45 41-45 48 46 50 39 46-50 42 51-55 71 56 or older 29 56-60 26 61 or older 23 Name: age, dtype: int64 . We can see that: . All values represent a range of ages. | Some have hyphens, and others have spaces. | A few are phrases that say &quot;or younger&quot; or &quot;or older.&quot; | . Therefore, we will clean the data by defining the following function and applying it elementwise to the column: . def fix_age(text): if pd.isnull(text): result = np.nan elif &quot; &quot; in text: result = text.replace(&quot; &quot;, &quot;-&quot;) elif text in [&quot;56-60&quot;, &quot;61 or older&quot;]: result = &quot;56 or older&quot; else: result = text return result combined[&quot;age&quot;] = combined[&quot;age&quot;].apply(fix_age) combined[&quot;age&quot;].value_counts().sort_index() . 20 or younger 10 21-25 62 26-30 67 31-35 61 36-40 73 41-45 93 46-50 81 51-55 71 56 or older 78 Name: age, dtype: int64 . Now, the values in the age column are consistent. . Categorizing Years of Service . Remember that DETE and TAFE differed in the format of their institute_service columns: . DETE had decimal numbers representing the number of years. | TAFE had strings, each of which represented a range of years. | . We could take the TAFE ranges and replace them with the middle value of each range. However, this would make the data inaccurate. . Instead, we will transform the data to become more general. We&#39;ll group the data into categories which represent ranges of years. This will apply to the entire institute_service column in the combined dataset. . The article &quot;Age is Just a Number: Engage Employees by Career Stage, Too&quot; states that employee engagement can be better understood in the context of career stage, i.e., the number of years working at the company. Career stage influences an employee&#39;s work attitude and virtues that they value in the workplace. . We can also say that career stage influences employees&#39; decisions to resign. The most obvious example of this is that a new employee isn&#39;t very invested in the company and would be more likely to leave due to initial dissatisfaction. . The article gives the following 4 career stages: . Newbie (0 to 3 years) | Sophomore (3 to 7 years) | Tenured (7 to 11 years) | Sage (11 or more years) | . We will use the above career stages as categories in the institute_service column. But first, we have to be prepared for what kinds of values we will have to transform. . combined[&quot;institute_service&quot;].unique() . array([7.0, 18.0, 3.0, 15.0, 14.0, 5.0, nan, 30.0, 32.0, 39.0, 17.0, 9.0, 6.0, 1.0, 35.0, 38.0, 36.0, 19.0, 4.0, 26.0, 10.0, 8.0, 2.0, 0.0, 23.0, 13.0, 16.0, 12.0, 21.0, 20.0, 24.0, 33.0, 22.0, 28.0, 49.0, 11.0, 41.0, 27.0, 42.0, 25.0, 29.0, 34.0, 31.0, &#39;3-4&#39;, &#39;7-10&#39;, &#39;1-2&#39;, &#39;Less than 1 year&#39;, &#39;11-20&#39;, &#39;5-6&#39;, &#39;More than 20 years&#39;], dtype=object) . The values above include: . Decimal numbers | A range of years (&quot;x-y&quot;) | A phrase describing a range of years (&quot;Less than x years&quot;) | . Extracting Numbers . Based on the values present, we will extract the number of years using the regular expression ([0-9]{1,2}). This will capture the first 1 or 2 digit whole number in a string. . For decimals, the part of the number before the decimal point will be extracted. | For ranges of years, the minimum year will be extracted. | For phrases, the first number to appear will be extracted. | . combined[&quot;service_num&quot;] = ( combined[&quot;institute_service&quot;] .astype(str) # Convert to strings. .str.extract(&quot;([0-9]{1,2})&quot;) # Extract 1 or 2-digit number. .astype(np.float64) # Convert to decimals. ) combined[&quot;service_num&quot;].unique() . array([ 7., 18., 3., 15., 14., 5., nan, 30., 32., 39., 17., 9., 6., 1., 35., 38., 36., 19., 4., 26., 10., 8., 2., 0., 23., 13., 16., 12., 21., 20., 24., 33., 22., 28., 49., 11., 41., 27., 42., 25., 29., 34., 31.]) . This worked for the decimals, as seen below. The initial and final values are identical. . combined.loc[ combined[&quot;institute_service&quot;].astype(str).str.contains(&quot;.&quot;), [&quot;institute_service&quot;, &quot;service_num&quot;] ].head() . institute_service service_num . 0 7.0 | 7.0 | . 1 18.0 | 18.0 | . 2 3.0 | 3.0 | . 3 15.0 | 15.0 | . 4 3.0 | 3.0 | . We can tell that this worked for the ranges because the minimum value was extracted. For example, for &quot;3-4&quot;, the number 3 was extracted. . combined.loc[ combined[&quot;institute_service&quot;].astype(str).str.contains(&quot;-&quot;), [&quot;institute_service&quot;, &quot;service_num&quot;] ].head() . institute_service service_num . 312 3-4 | 3.0 | . 313 7-10 | 7.0 | . 314 3-4 | 3.0 | . 315 3-4 | 3.0 | . 316 3-4 | 3.0 | . It also worked for the phrases, as seen below. The number in the phrase was extracted. . combined.loc[ combined[&quot;institute_service&quot;].astype(str).str.contains(&quot;than&quot;), [&quot;institute_service&quot;, &quot;service_num&quot;] ].head() . institute_service service_num . 318 Less than 1 year | 1.0 | . 329 Less than 1 year | 1.0 | . 332 More than 20 years | 20.0 | . 333 Less than 1 year | 1.0 | . 334 Less than 1 year | 1.0 | . Mapping Numbers to Categories . Now that we have numbers, we can map them to the career stages mentioned earlier: . Newbie (0 to 3 years) | Sophomore (3 to 7 years) | Tenured (7 to 11 years) | Sage (11 or more years) | . We&#39;ll do this by defining a function then applying it elementwise to the column. . def career_stage(years): if pd.isnull(years): stage = np.nan elif years &lt; 3.0: stage = &quot;Newbie&quot; elif years &lt; 7.0: stage = &quot;Sophomore&quot; elif years &lt; 11.0: stage = &quot;Tenured&quot; elif years &gt;= 11.0: stage = &quot;Sage&quot; return stage # Apply the function elementwise and make a new column. combined[&quot;service_cat&quot;] = combined[&quot;service_num&quot;].apply(career_stage) combined[&quot;service_cat&quot;].value_counts() . Newbie 193 Sophomore 172 Sage 136 Tenured 62 Name: service_cat, dtype: int64 . The results show that most of the employees who resigned were Newbies. . Data cleaning is now complete, so we can go to data analysis. . Data Analysis . Dissatisfaction by Age Group . First, we can investigate the dissatisfaction rates of resignees by their age group. . df = combined.dropna(subset = [&quot;dissatisfaction&quot;, &quot;age&quot;]).copy() # Cast booleans to integers. df[&quot;dissatisfaction&quot;] = df[&quot;dissatisfaction&quot;].astype(int) table_3 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;age&quot;], aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0. ).reset_index() table_3 . age dissatisfaction . 0 20 or younger | 0.200000 | . 1 21-25 | 0.306452 | . 2 26-30 | 0.417910 | . 3 31-35 | 0.377049 | . 4 36-40 | 0.342466 | . 5 41-45 | 0.376344 | . 6 46-50 | 0.382716 | . 7 51-55 | 0.422535 | . 8 56 or older | 0.423077 | . This table is visualized in the bar graph below. . sns.barplot( data = table_3, x = &quot;age&quot;, y = &quot;dissatisfaction&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Age Group&quot;) plt.xlabel(&quot;Age Group&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 30, ha = &quot;right&quot;) plt.grid(True) plt.show() . Notably, the dissatisfaction rate is: . Lowest (20%) among resignees aged 20 or younger. | Above 30% among resignees aged 21 or older. | Highest (42%) among resignees aged 56 or older, 51-55, or 26-30. | . Let&#39;s group the data further using the institute (DETE or TAFE). . table_4 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;age&quot;, &quot;institute&quot;], aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0. ).reset_index() table_4 . age institute dissatisfaction . 0 20 or younger | DETE | 0.000000 | . 1 20 or younger | TAFE | 0.222222 | . 2 21-25 | DETE | 0.310345 | . 3 21-25 | TAFE | 0.303030 | . 4 26-30 | DETE | 0.571429 | . 5 26-30 | TAFE | 0.250000 | . 6 31-35 | DETE | 0.551724 | . 7 31-35 | TAFE | 0.218750 | . 8 36-40 | DETE | 0.390244 | . 9 36-40 | TAFE | 0.281250 | . 10 41-45 | DETE | 0.479167 | . 11 41-45 | TAFE | 0.266667 | . 12 46-50 | DETE | 0.452381 | . 13 46-50 | TAFE | 0.307692 | . 14 51-55 | DETE | 0.593750 | . 15 51-55 | TAFE | 0.282051 | . 16 56 or older | DETE | 0.551020 | . 17 56 or older | TAFE | 0.206897 | . The table is visualized in the bar graph below. . sns.barplot( data = table_4, x = &quot;age&quot;, y = &quot;dissatisfaction&quot;, hue = &quot;institute&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Age Group&quot;) plt.xlabel(&quot;Age Group&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 30, ha = &quot;right&quot;) plt.grid(True) plt.show() . We can see that employees of DETE generally had much higher dissatisfaction rates than employees of TAFE across all age groups. . The exception is the &quot;20 or younger&quot; age group, which has a 0% dissatisfaction rate for DETE employees. However, this is due to the fact that only one resignee from DETE was 20 years old or younger. . ( combined .loc[combined[&quot;institute&quot;] == &quot;DETE&quot;, &quot;age&quot;] .value_counts() .sort_index() ) . 20 or younger 1 21-25 29 26-30 35 31-35 29 36-40 41 41-45 48 46-50 42 51-55 32 56 or older 49 Name: age, dtype: int64 . Thus, we can generally say that the dissatisfaction rate was much higher among DETE resignees compared to TAFE resignees. . Also, the peak dissatisfaction rates occurred in age groups around 26-30 years old and 51-55 years old for both institutes. . Dissatisfaction by Career Stage . Next, we will determine what percentage of the resignees was dissatisfied with work based on career stage. . df = combined.dropna(subset = [&quot;dissatisfaction&quot;, &quot;service_cat&quot;]).copy() # Cast booleans to integers. df[&quot;dissatisfaction&quot;] = df[&quot;dissatisfaction&quot;].astype(int) table_1 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;service_cat&quot;], aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0. ).reset_index() table_1 . service_cat dissatisfaction . 0 Newbie | 0.295337 | . 1 Sage | 0.485294 | . 2 Sophomore | 0.343023 | . 3 Tenured | 0.516129 | . The table is visualized in the figure below. . sns.barplot( data = table_1, x = &quot;service_cat&quot;, y = &quot;dissatisfaction&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Career Stage&quot;) plt.xlabel(&quot;Career Stage&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 0) plt.grid(True) plt.show() . Interestingly, the dissatisfaction rate is highest (around 50%) within the Tenured and Sage groups of resignees. This is surprising since these are the groups of employees who have been working at the institute for the longest time. . One explanation could be that they became dissatisfied because they spent so much time at the company without career growth or without sufficient variety in their work. . Next, we can group the data further by the specific institute of the employees: . table_2 = df.pivot_table( values = [&quot;dissatisfaction&quot;], index = [&quot;service_cat&quot;, &quot;institute&quot;], # Group on institute too aggfunc = np.mean, ).reset_index() table_2 . service_cat institute dissatisfaction . 0 Newbie | DETE | 0.375000 | . 1 Newbie | TAFE | 0.262774 | . 2 Sage | DETE | 0.560000 | . 3 Sage | TAFE | 0.277778 | . 4 Sophomore | DETE | 0.460526 | . 5 Sophomore | TAFE | 0.250000 | . 6 Tenured | DETE | 0.609756 | . 7 Tenured | TAFE | 0.333333 | . This table is visualized below. . sns.barplot( data = table_2, x = &quot;service_cat&quot;, y = &quot;dissatisfaction&quot;, hue = &quot;institute&quot;, estimator = np.mean, ci = None, ) plt.title(&quot;Dissatisfaction Rates of Resignees by Career Stage&quot;) plt.xlabel(&quot;Career Stage&quot;) plt.ylabel(&quot;Percentage of Resignees who were Dissatisfied&quot;) plt.xticks(rotation = 0) plt.grid(True) plt.show() . The chart shows that the trend is generally consistent between DETE and TAFE. Tenured and Sage resignees have a higher dissatisfaction rate than other groups. . However, it looks like the dissatisfaction rates in DETE are also much higher than the rates in TAFE. It can be said that dissatisfaction influences the resignation of a greater percentage of people in DETE than it does in TAFE. . Conclusion . In this project, we worked with 2 datasets of employee exit survey data from the DETE and TAFE government institutes in Australia. We cleaned, transformed, and combined these datasets. Then, we analyzed dissatisfaction rates of resignees based on age and based on career stage. . We found the following notable points: . Dissatisfaction rate was highest among resignees in age groups around 26-30 years old and 51-55 years old for both institutes. | Dissatisfaction rate was highest among resignees who had been working at the institute for over 7 years. | The dissatisfaction rate was much higher among DETE resignees compared to TAFE resignees. This may have to do with the nature or conditions of the work of DETE employees. | . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/2021/06/01/Employee-Exit-Survey-Data-Cleaning-Aggregation.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/2021/06/01/Employee-Exit-Survey-Data-Cleaning-Aggregation.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
            "content": "Overview . &quot;Explanatory&quot; visualizations are created in order to explain something to other people. These are made to inform general, non-technical audiences. Thus, we want to make a chart with the following characteristics: . Eye-catching | Visually pleasing | Easy to understand | Has a clear main point, as opposed to being too detailed | . Put simply, it is similar to an infographic. However, instead of being a standalone image, it is usually put in an article with accompanying text. . In this project, I detail the process of designing an explanatory chart of USD-PHP exchange rates. I used the &quot;Forex data since 2011-1-1&quot; dataset, which was uploaded by user emrecanaltinsoy on Kaggle. By the end of the project, I was able to make the following chart. . . . Tip: The larger implications of exchange rate trends are beyond the scope of this project. For Filipinos, I suggest reading &quot;[ANALYSIS] Why the stronger peso mirrors a weaker PH economy&quot; (Punongbayan 2020), which is an interesting recent article. . . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Storytelling Data Visualization on Exchange Rates. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import seaborn as sns import datetime as dt import matplotlib.pyplot as plt import matplotlib.style as style import matplotlib.lines as lines # Use matplotlib&#39;s default style. style.use(&quot;default&quot;) . Data Overview . The &quot;Forex data since 2011-1-1&quot; dataset contains USD exchange rates for various currencies, each in a different column. Every row corresponds to one day, and the dataset has been updated every day since January 1, 2011. My copy was downloaded on May 27, 2021. . Below are the first 5 rows of the dataset. . exchange = pd.read_csv(&quot;./private/2021-05-28-MEC-Files/forex_usd_data.csv&quot;) exchange.head() . date(y-m-d) Argentine Peso Australian Dollar Bahraini Dinar Botswana Pula Brazilian Real Bruneian Dollar Bulgarian Lev Canadian Dollar Chilean Peso ... Sri Lankan Rupee Swedish Krona Swiss Franc Taiwan New Dollar Thai Baht Trinidadian Dollar Turkish Lira Emirati Dirham British Pound Venezuelan Bolivar . 0 2011-01-01 | 3.9690 | 0.977326 | 0.377050 | 6.472492 | 1.659500 | 1.284500 | 1.463830 | 0.997700 | 467.750000 | ... | 110.940002 | 6.721450 | 0.934500 | 29.140000 | 30.020000 | 6.34 | 1.537400 | 3.67310 | 0.640553 | 4.3 | . 1 2011-01-02 | 3.9690 | 0.977326 | 0.377050 | 6.472492 | 1.659500 | 1.283500 | 1.463830 | 0.997700 | 467.750000 | ... | 110.940002 | 6.721450 | 0.933800 | 29.099001 | 30.020000 | 6.34 | 1.537400 | 3.67310 | 0.641067 | 4.3 | . 2 2011-01-03 | 3.9735 | 0.980569 | 0.377055 | 6.472492 | 1.646288 | 1.284367 | 1.462799 | 0.990444 | 465.649994 | ... | 110.919998 | 6.693788 | 0.933069 | 29.120000 | 30.084999 | 6.39 | 1.557411 | 3.67320 | 0.645615 | 4.3 | . 3 2011-01-04 | 3.9710 | 0.995580 | 0.377060 | 6.480881 | 1.666747 | 1.287438 | 1.469525 | 0.999076 | 487.850006 | ... | 110.820000 | 6.726967 | 0.947903 | 29.175004 | 30.104903 | 6.36 | 1.547801 | 3.67315 | 0.641558 | 4.3 | . 4 2011-01-05 | 3.9715 | 0.999522 | 0.377050 | 6.548788 | 1.670312 | 1.291450 | 1.485031 | 0.994376 | 495.149993 | ... | 110.820000 | 6.766127 | 0.964490 | 29.170000 | 30.216193 | 6.38 | 1.543853 | 3.67310 | 0.645308 | 4.3 | . 5 rows × 54 columns . We are only interested in USD-PHP exchange rates, so we will take the &quot;Philippine Peso&quot; column. . php = exchange[[&quot;date(y-m-d)&quot;, &quot;Philippine Peso&quot;]].copy() php.head() . date(y-m-d) Philippine Peso . 0 2011-01-01 | 43.639999 | . 1 2011-01-02 | 43.639999 | . 2 2011-01-03 | 43.799999 | . 3 2011-01-04 | 43.550002 | . 4 2011-01-05 | 43.900002 | . Below is more information on the 2 columns. . php.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3798 entries, 0 to 3797 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 date(y-m-d) 3798 non-null object 1 Philippine Peso 3798 non-null float64 dtypes: float64(1), object(1) memory usage: 59.5+ KB . The dataset has 3798 entries and 2 columns. There are no missing values. . The date column contains text, whereas the PHP column contains decimal numbers. . Data Cleaning . Column Labels . The column labels will first be changed so that these are easier to use. . php.columns = [&quot;date&quot;, &quot;usd_php&quot;] php.head() . date usd_php . 0 2011-01-01 | 43.639999 | . 1 2011-01-02 | 43.639999 | . 2 2011-01-03 | 43.799999 | . 3 2011-01-04 | 43.550002 | . 4 2011-01-05 | 43.900002 | . Date Column . The date column contains text with the format {4 digit year}-{2 digit month}-{2 digit day}. . Below, I convert the text to datetime objects for ease of use. . php[&quot;date&quot;] = pd.to_datetime(php[&quot;date&quot;]) php.sort_values( by = &quot;date&quot;, ascending = True, inplace = True, ) php[&quot;date&quot;] . 0 2011-01-01 1 2011-01-02 2 2011-01-03 3 2011-01-04 4 2011-01-05 ... 3793 2021-05-21 3794 2021-05-22 3795 2021-05-23 3796 2021-05-24 3797 2021-05-25 Name: date, Length: 3798, dtype: datetime64[ns] . Descriptive Statistics . Before we can clean the data, we have to view its descriptive statistics. . php.describe(datetime_is_numeric = True) . date usd_php . count 3798 | 3798.000000 | . mean 2016-03-13 12:00:00 | 47.010148 | . min 2011-01-01 00:00:00 | 3.094050 | . 25% 2013-08-07 06:00:00 | 43.639330 | . 50% 2016-03-13 12:00:00 | 46.890740 | . 75% 2018-10-18 18:00:00 | 50.584238 | . max 2021-05-25 00:00:00 | 54.323583 | . std NaN | 3.920411 | . As expected, the dates range from November 1, 2011 to May 25, 2021. . However, the minimum USD-PHP exchange rate in the data is 3.09. This is very low compared to the other percentiles. . Exchange Rate Outliers . There may be some outliers in the data. We can confirm this using a boxplot. . sns.boxplot( data = php, y = &quot;usd_php&quot; ) plt.title(&quot;USD-PHP Exchange Rate Distribution&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . Indeed, most of the values fall between 40 and 55, whereas a few outliers exist below 10. These may be inaccurate data. . How many outliers are there? . (php[&quot;usd_php&quot;] .value_counts(bins = 10) .sort_index() ) . (3.0420000000000003, 8.217] 2 (8.217, 13.34] 0 (13.34, 18.463] 0 (18.463, 23.586] 0 (23.586, 28.709] 0 (28.709, 33.832] 0 (33.832, 38.955] 0 (38.955, 44.078] 1157 (44.078, 49.201] 1289 (49.201, 54.324] 1350 Name: usd_php, dtype: int64 . There are only 2 values less than 10. It is highly unlikely that these values are accurate. This can be shown using a line chart. . sns.lineplot( data = php, x = &quot;date&quot;, y = &quot;usd_php&quot;, ) plt.title(&quot;USD-PHP Exchange Rate Over Time&quot;) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . The outliers exist somewhere in the 2014 data, and they create an unnatural dip in the chart. . Thus, the inaccurate datapoints will be dropped from the dataset. . php = php.loc[php[&quot;usd_php&quot;] &gt; 10] php.describe(datetime_is_numeric = True) . date usd_php . count 3796 | 3796.000000 | . mean 2016-03-13 22:32:45.015806208 | 47.033285 | . min 2011-01-01 00:00:00 | 40.500000 | . 25% 2013-08-06 18:00:00 | 43.639981 | . 50% 2016-03-14 12:00:00 | 46.892504 | . 75% 2018-10-19 06:00:00 | 50.584680 | . max 2021-05-25 00:00:00 | 54.323583 | . std NaN | 3.789566 | . The minimum USD-PHP rate is now 40.5, which makes more sense. . Exploratory Data Analysis . Basic Line Chart . First, we start with a basic line chart that shows the exchange rates on all days in the dataset. . sns.lineplot( data = php, x = &quot;date&quot;, y = &quot;usd_php&quot;, ) plt.title(&quot;USD-PHP Exchange Rate Over Time&quot;) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . The chart shows that the exchange rate dropped to under 41.0 in 2013. It then steadily climbed up to over 54.0 in 2018 before moving down again afterwards. . For Filipinos, it is ideal for the USD-PHP rate to be lower so that the peso has more power. Thus, it can be said that the exchange rate was better from 2011 to 2015 compared to how it has been in recent years. However, note that the USD-PHP exchange rate is not the only descriptor of the Philippines&#39; economy. . Rolling Average . In order to focus more on general trends than small fluctuations, we can graph the rolling average (or moving average). The rolling average is taken by replacing each datapoint with the mean average of a certain number of the datapoints leading up to it. . Using a rolling average can make a graph look visually cleaner and make general trends easier to see. . The number of datapoints used in each average is called the rolling window. This can be specified in Pandas using pd.Series.rolling(). Below, we use a rolling window of 182 days (around half a year) in order to transform the rate data. . php[&quot;rolling&quot;] = php[&quot;usd_php&quot;].rolling(182).mean() php.tail() . date usd_php rolling . 3793 2021-05-21 | 47.919983 | 48.237840 | . 3794 2021-05-22 | 47.929517 | 48.236031 | . 3795 2021-05-23 | 47.929517 | 48.234226 | . 3796 2021-05-24 | 48.068599 | 48.232923 | . 3797 2021-05-25 | 48.155798 | 48.232809 | . The last 5 rows of the dataset are shown above. The rolling averages are not exactly equal to the original numbers, but these are close enough to show the same trend. . A line chart of the rolling averages is shown below. . sns.lineplot( data = php, x = &quot;date&quot;, y = &quot;rolling&quot;, ) plt.title(&quot;USD-PHP Exchange Rate Over Time: Rolling Average&quot;) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;USD-PHP Exchange Rate&quot;) plt.grid(True) plt.show() . The same trends mentioned earlier are clearer to see in the above chart. . Explanatory Chart . One idea of a useful chart would be to compare the USD-PHP exchange rate trends before and during the COVID-19 pandemic. . Note: A similar idea was suggested in the Dataquest guided project. However, I did not look at Dataquest&#8217;s solution notebook. I wrote my code on my own. . Key Concepts . The following concepts will be used throughout the process of designing the chart. I learned these from Dataquest&#39;s &quot;Data Scientist in Python&quot; course. . Familiarity . Audiences prefer familiar charts since they can understand these easily. Therefore, it is better to use a basic chart as a template than to use something obscure or create something entirely new. . In the case of showing USD-PHP exchange rates over time, it is best to use a basic line chart as a template. . Data-Ink Ratio . When making an explanatory chart, one must maximize the data-ink ratio. . Data refers to the elements that represent data and its relationships, like bars and lines. | Ink refers to the total amount of ink that the chart would use if it were printed on paper. | . Maximizing the data-ink ratio means focusing more on data-related elements and minimizing the use of other, less important elements. . This helps the audience understand the main point without being distracted by other details. . Gestalt Psychology . Gestalt psychology is founded on the idea that people tend to see patterns rather than individual objects. . Under Gestalt psychology, there are several Principles of Grouping. These are ways to visually group elements together. . Proximity: Elements are close to each other | Similarity: Elements are similar due to color, shape, etc. | Enclosure: Elements are enclosed in an outer shape, like a rectangle | Connection: Elements are connected by a form, usually a line | . When designing charts, these are helpful in implying relationships between elements instead of explicitly stating them. . Visual Style . Before we start making the chart, we have to choose a style. . I chose Matplotlib&#39;s built-in &quot;fivethirtyeight&quot; style. It&#39;s based on the style of the charts used on the FiveThirtyEight website by Nate Silver. . Additionally, I used color-hex.com to get hex codes for specific kinds of blue, orange, and dark gray that I want to use in my chart. . style.use(&quot;fivethirtyeight&quot;) # Color hex codes c_blue = &quot;#14c4dd&quot; c_orange = &quot;#ffa500&quot; c_dark_gray = &quot;#d2d2d2&quot; . Setting up Subplots . In order to make the chart fresh and interesting, we have to make it more complex than 1 plot with a line chart. In our case, I have the following ideas: . Show 2 line charts, one on top of the other. | The upper chart shows how the rate changed from 2011 to 2021. | The lower chart zooms into the pandemic portion of the line chart from 2020 to 2021. | . In order to do this, I will create a Matplotlib Figure with two Axes (subplots), as described above. . # There is an upper and lower subplot. fig, (ax1, ax2) = plt.subplots( nrows = 2, ncols = 1, figsize = (10, 10), # 10 inches x 10 inches dpi = 80, ) . The Upper Subplot . Next, we design the upper subplot. Here&#39;s what it looks like with a basic line chart of the raw exchange rate data. . ax1.plot( php[&quot;date&quot;], php[&quot;usd_php&quot;], color = c_blue, ) fig . The line above looks messy. For the general audience, the overall trends are more important than the specific daily values. Thus, we will use the rolling average to make the chart cleaner. . ax1.clear() ax1.plot( php[&quot;date&quot;], php[&quot;rolling&quot;], color = c_blue, ) fig . Additionally, we will split the line into: . The pre-pandemic portion (blue) | The pandemic portion (orange) | . The pandemic portion will also be enclosed in a dark gray box, in order to further separate it from the pre-pandemic portion. . ax1.clear() # Main line chart (2011-2019) php_pre = php.loc[php[&quot;date&quot;].dt.year &lt; 2020] ax1.plot( php_pre[&quot;date&quot;], php_pre[&quot;rolling&quot;], color = c_blue, ) # Pandemic part of line chart (2020-2021) php_pandemic = php.loc[php[&quot;date&quot;].dt.year.between(2020, 2021)] ax1.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;rolling&quot;], color = c_orange, ) # Special background for pandemic portion ax1.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 1, ) fig . Next, we want to maximize the data-ink ratio by removing unnecessary elements. We will do the following: . On the x-axis, show only the labels for 2012, 2016, and 2020. | On the y-axis, show only the labels for 48, 50, and 52. | Remove grid lines. | . We will also add 1 grid line at y = 50 so that it can guide viewers. It would be particularly helpful for Filipino viewers since they commonly think that USD 1 = PHP 50. . ax1.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels ax1.set_xticks([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_xticklabels([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_yticks([48, 50, 52]) ax1.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax1.grid(False) # Horizontal line at y = 50 ax1.axhline(50, linewidth = 1, color = &quot;gray&quot;) fig . Now, the upper subplot is much cleaner; there is less visual noise. . The last step for the upper subplot would be to add informative text: . &quot;Pre-Pandemic&quot; label for the blue line | A comment about the upward trend leading up to 2018 | . ax1.text( x = dt.date(year = 2013, month = 7, day = 1), y = 48, s = &quot;Pre-Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;center&quot;, va = &quot;center&quot;, ) # Comment on upward trend ax1.text( x = dt.date(year = 2018, month = 10, day = 1), y = 46, s = &quot;Rate climbs up to 54 nin Oct 2018&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) fig . The &quot;Pre-Pandemic&quot; label was set to gray because it is a structural element; it is less important. . On the other hand, the long comment was set to black because it states a statistic from the data and helps tell a story about the data. It is more important, so it should be darker. . We have finished designing the upper subplot. . Lower Subplot . Next, the lower subplot will zoom in on the pandemic portion of the data, which is from 2020 to 2021. Since this is the more important part, we should use the raw data for more detail. . ax2.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;usd_php&quot;], color = c_orange, ) fig . Then, we will make changes similar to the ones done for the upper subplot: . On the x-axis, show only the labels for January 2020, July 2020, and January 2021. | On the y-axis, show only the labels for 48, 50, and 52. | Remove grid lines. | Add 1 grid line at y = 50. | Enclose the entire line in a dark gray box. | . ax2.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels. ax2.set_xticks([&quot;2020-01&quot;, &quot;2020-07&quot;, &quot;2021-01&quot;]) ax2.set_xticklabels([&#39;Jan 2020&#39;, &#39;Jul 2020&#39;, &#39;Jan 2021&#39;]) ax2.set_yticks([48, 50, 52]) ax2.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax2.grid(False) # Horizontal line at y = 50 ax2.axhline(50, linewidth = 1, color = &quot;gray&quot;) # Special background for pandemic portion ax2.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 1, ) fig . We are using the principles of enclosure and similarity to visually imply that the pandemic portion in the upper subplot is being shown more closely in the lower subplot. . Enclosure: dark gray boxes | Similarity: orange lines, horizontal grid line, y-axis labels | . Next, we add another comment in black text, this time about the downward trend leading up to the present day. . ax2.text( x = dt.date(year = 2021, month = 1, day = 1), y = 49.25, s = &quot;Rate drops down to 48 nin May 2021&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) fig . We have finished designing the lower subplot. However, we still need to add some finishing touches. . Figure-Level Customization . In this last step, we customize the chart on the level of the Matplotlib Figure. This involves both of the subplots and the space around them. . What we want to do is to use the principles of proximity and connection to make the relationship between the 2 subplots even clearer. . Proximity: Increase the space between the two subplots. | Connection: Draw a line connecting the two dark gray boxes. Add a &quot;COVID-19 Pandemic&quot; label next to the line. | . | . These are done in the code below. . fig.tight_layout(pad = 5) # Line connection between gray boxes fig.add_artist( lines.Line2D( xdata = [0.82, 0.82], ydata = [0.44, 0.585], color = c_dark_gray, alpha = 1, ) ) # &quot;COVID-19&quot; label between subplots fig.text( x = 0.8, y = 0.5, s = &quot;COVID-19 Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;right&quot;, va = &quot;center&quot;, ) fig . Now, when one first reads the chart, it is very clear that the gray boxes contain data about exchange rates in the COVID-19 pandemic. . The last touch would be to add a title and subtitle to the chart. Since the title is typically the first thing a viewer reads on a chart, it is best to state a statistic related to the data, like &quot;USD-PHP Rate Drops to 48 after 1 Year in the Pandemic.&quot; . This way, the title becomes a data element. The data-ink ratio is increased. . fig.text( x = 0.5, y = 0.95, s = &quot;USD-PHP Rate Drops to 48 after 1 Year in the Pandemic&quot;, size = 16, weight = &quot;bold&quot;, ha = &quot;center&quot;, va = &quot;center&quot;, ) # Subtitle fig.text( x = 0.5, y = 0.92, s = &quot;USD-PHP exchange rate over time&quot;, size = 12, ha = &quot;center&quot;, va = &quot;center&quot;, ) fig . That&#39;s it. The explanatory chart is complete. . Full Code . The full code to make the graph is shown below. The comments explain which part does what. . style.use(&quot;fivethirtyeight&quot;) # Color hex codes c_blue = &quot;#14c4dd&quot; c_orange = &quot;#ffa500&quot; c_dark_gray = &quot;#d2d2d2&quot; # Figure has 2 rows and 1 column. # There is an upper and lower subplot. fig, (ax1, ax2) = plt.subplots( nrows = 2, ncols = 1, figsize = (10, 10), dpi = 80, ) # UPPER subplot # Main line chart (2011-2019) php_pre = php.loc[php[&quot;date&quot;].dt.year &lt; 2020] ax1.plot( php_pre[&quot;date&quot;], php_pre[&quot;rolling&quot;], color = c_blue, ) # Pandemic part of line chart (2020-2021) php_pandemic = php.loc[php[&quot;date&quot;].dt.year.between(2020, 2021)] ax1.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;rolling&quot;], color = c_orange, ) # Special background for pandemic portion ax1.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 1, ) # Set tick label color to gray. ax1.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels ax1.set_xticks([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_xticklabels([&quot;2012&quot;, &quot;2016&quot;, &quot;2020&quot;]) ax1.set_yticks([48, 50, 52]) ax1.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax1.grid(False) # Horizontal line at y = 50 ax1.axhline(50, linewidth = 1, color = &quot;gray&quot;) # Text in upper subplot ax1.text( x = dt.date(year = 2018, month = 10, day = 1), y = 46, s = &quot;Rate climbs up to 54 nin Oct 2018&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) ax1.text( x = dt.date(year = 2013, month = 7, day = 1), y = 48, s = &quot;Pre-Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;center&quot;, va = &quot;center&quot;, ) # LOWER subplot # Pandemic portion, zoomed in ax2.plot( php_pandemic[&quot;date&quot;], php_pandemic[&quot;usd_php&quot;], color = c_orange, ) # Set tick labels to gray. ax2.tick_params(axis = &#39;both&#39;, colors = &#39;gray&#39;) # Specific tick labels. ax2.set_xticks([&quot;2020-01&quot;, &quot;2020-07&quot;, &quot;2021-01&quot;]) ax2.set_xticklabels([&#39;Jan 2020&#39;, &#39;Jul 2020&#39;, &#39;Jan 2021&#39;]) ax2.set_yticks([48, 50, 52]) ax2.set_yticklabels([&quot;48&quot;, &quot;50&quot;, &quot;52&quot;]) # Remove grid ax2.grid(False) # Horizontal line at y = 50 ax2.axhline(50, linewidth = 1, color = &quot;gray&quot;) # Special background for pandemic portion ax2.axvspan( &quot;2020-01&quot;, &quot;2021-05-25&quot;, facecolor = c_dark_gray, alpha = 1, ) # Text in lower subplot ax2.text( x = dt.date(year = 2021, month = 1, day = 1), y = 49.25, s = &quot;Rate drops down to 48 nin May 2021&quot;, color = &quot;black&quot;, size = 11, ha = &quot;center&quot;, va = &quot;center&quot;, ) # FIGURE level customization # Add space between the subplots fig.tight_layout(pad = 5) # Line connection between pandemic parts fig.add_artist( lines.Line2D( xdata = [0.82, 0.82], ydata = [0.44, 0.585], color = c_dark_gray, alpha = 1, ) ) # Title with statistic fig.text( x = 0.5, y = 0.95, s = &quot;USD-PHP Rate Drops to 48 after 1 Year in the Pandemic&quot;, size = 16, weight = &quot;bold&quot;, ha = &quot;center&quot;, va = &quot;center&quot;, ) # Subtitle fig.text( x = 0.5, y = 0.92, s = &quot;USD-PHP exchange rate over time&quot;, size = 12, ha = &quot;center&quot;, va = &quot;center&quot;, ) # &quot;COVID-19&quot; label between subplots fig.text( x = 0.8, y = 0.5, s = &quot;COVID-19 Pandemic&quot;, color = &quot;gray&quot;, size = 14, ha = &quot;right&quot;, va = &quot;center&quot;, ) # Save chart locally. plt.savefig(&quot;./private/2021-05-28-MEC-Files/2021-05-28-explanatory-chart.png&quot;) # Show chart plt.show() . Conclusion . In this project, we cleaned and explored data about USD-PHP exchange rates over time. . We then discussed several key concepts in the creation of an explanatory chart, such as Familiarity, Data-Ink Ratio, and Gestalt Psychology. These concepts were applied throughout the process of making an explanatory chart that compared exchange rate trends before and during the pandemic. . We were ultimately able to create a chart that is simple and clean, yet eye-catching and informative. . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/2021/05/28/Making-Explanatory-Chart-USD-PHP-Exchange-Rates.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/2021/05/28/Making-Explanatory-Chart-USD-PHP-Exchange-Rates.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Indicators of Heavy Traffic on the I-94 Highway",
            "content": ". Unsplash | Alexander Popov Overview . Interstate 94 or I-94 is a highway in the USA that stretches from Montana in the west to Michigan in the east. In 2019, John Hogue donated a dataset of traffic volume, weather, and holiday data on I-94 from 2012 to 2018. This can be found on the following UCI Machine Learning Repository page: Metro Interstate Traffic Volume Data Set. . The goal of this project is to determine possible indicators of heavy traffic on I-94. Exploratory data analysis will be conducted with Seaborn visualizations. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Finding Heavy Traffic Indicators on I-94. The general project flow and research questions came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data Overview . The following details about the I-94 dataset are stated in the archive. . holiday: Categorical; US National holidays plus regional holiday, Minnesota State Fair | temp: Numeric; Average temperature (Kelvin) | rain_1h: Numeric; Amount (mm) of rain that occurred in the hour | snow_1h: Numeric; Amount (mm) of snow that occurred in the hour | clouds_all: Numeric; Percentage of cloud cover (%) | weather_main: Categorical; Short textual description of the current weather | weather_description: Categorical; Longer textual description of the current weather | date_time: DateTime; Hour of the data collected in local CST time | traffic_volume: Numeric; Hourly I-94 ATR 301 reported westbound traffic volume . Note: The data were collected from a station somewhere between Minneapolis and St. Paul, Minnesota. Only westbound traffic was recorded, not eastbound. The data are not representative of the entire I-94 highway. | . Below are the first few rows of the dataset. . highway = pd.read_csv(&quot;./private/2021-05-25-IHT-Files/Metro_Interstate_Traffic_Volume.csv&quot;) highway.head() . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . 0 None | 288.28 | 0.0 | 0.0 | 40 | Clouds | scattered clouds | 2012-10-02 09:00:00 | 5545 | . 1 None | 289.36 | 0.0 | 0.0 | 75 | Clouds | broken clouds | 2012-10-02 10:00:00 | 4516 | . 2 None | 289.58 | 0.0 | 0.0 | 90 | Clouds | overcast clouds | 2012-10-02 11:00:00 | 4767 | . 3 None | 290.13 | 0.0 | 0.0 | 90 | Clouds | overcast clouds | 2012-10-02 12:00:00 | 5026 | . 4 None | 291.14 | 0.0 | 0.0 | 75 | Clouds | broken clouds | 2012-10-02 13:00:00 | 4918 | . There are columns about the date, traffic volume, weather, and occurrence of holidays. With this dataset, one could determine how traffic or weather changed over time. One could also determine how weather and holidays affected traffic volume. . Let us view the information on each column below. . highway.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48204 entries, 0 to 48203 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 holiday 48204 non-null object 1 temp 48204 non-null float64 2 rain_1h 48204 non-null float64 3 snow_1h 48204 non-null float64 4 clouds_all 48204 non-null int64 5 weather_main 48204 non-null object 6 weather_description 48204 non-null object 7 date_time 48204 non-null object 8 traffic_volume 48204 non-null int64 dtypes: float64(3), int64(2), object(4) memory usage: 3.3+ MB . It turns out that most of the numeric columns are float64, which refers to decimal numbers. . The integer columns are clouds_all, which is in percentage, and traffic volume, which tells the number of cars. . The date_time column is listed as an object or text column. Let&#39;s convert this to datetime format for ease of use. . highway[&quot;date_time&quot;] = pd.to_datetime(highway[&quot;date_time&quot;]) highway[&quot;date_time&quot;] . 0 2012-10-02 09:00:00 1 2012-10-02 10:00:00 2 2012-10-02 11:00:00 3 2012-10-02 12:00:00 4 2012-10-02 13:00:00 ... 48199 2018-09-30 19:00:00 48200 2018-09-30 20:00:00 48201 2018-09-30 21:00:00 48202 2018-09-30 22:00:00 48203 2018-09-30 23:00:00 Name: date_time, Length: 48204, dtype: datetime64[ns] . Lastly, all of the Non-Null Counts match the total number of datapoints (48204). This could mean that there are no missing values in the dataset. It could also mean that missing values are expressed in a non-conventional way. . Data Cleaning . Duplicate Entries . There may be duplicate entries, i.e., multiple entries for the same date and hour. These must be removed. . highway.drop_duplicates( subset = [&quot;date_time&quot;], keep = &quot;first&quot;, inplace = True, ) highway.shape . (40575, 9) . There were 7629 duplicate entries removed from the original 48204 entries. . Descriptive Statistics . Next, we view the descriptive statistics in order to find abnormal values. . highway.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . count 40575 | 40575.000000 | 40575.000000 | 40575.000000 | 40575.000000 | 40575 | 40575 | 40575 | 40575.000000 | . unique 12 | NaN | NaN | NaN | NaN | 11 | 35 | NaN | NaN | . top None | NaN | NaN | NaN | NaN | Clouds | sky is clear | NaN | NaN | . freq 40522 | NaN | NaN | NaN | NaN | 15123 | 11642 | NaN | NaN | . mean NaN | 281.316763 | 0.318632 | 0.000117 | 44.199162 | NaN | NaN | 2015-12-23 22:16:28.835489792 | 3290.650474 | . min NaN | 0.000000 | 0.000000 | 0.000000 | 0.000000 | NaN | NaN | 2012-10-02 09:00:00 | 0.000000 | . 25% NaN | 271.840000 | 0.000000 | 0.000000 | 1.000000 | NaN | NaN | 2014-02-02 19:30:00 | 1248.500000 | . 50% NaN | 282.860000 | 0.000000 | 0.000000 | 40.000000 | NaN | NaN | 2016-06-02 14:00:00 | 3427.000000 | . 75% NaN | 292.280000 | 0.000000 | 0.000000 | 90.000000 | NaN | NaN | 2017-08-02 23:30:00 | 4952.000000 | . max NaN | 310.070000 | 9831.300000 | 0.510000 | 100.000000 | NaN | NaN | 2018-09-30 23:00:00 | 7280.000000 | . std NaN | 13.816618 | 48.812640 | 0.005676 | 38.683447 | NaN | NaN | NaN | 1984.772909 | . Most of the descriptives make sense, except for a few details: . The minimum temp is 0 Kelvin. This is called absolute zero, the lowest possible temperature of any object, equivalent to $-273.15^{ circ} text{C}$. It is unreasonable for the I-94 highway to reach such a low temperature, even in winter. | The maximum rain_1h is 9,831 mm, or 9.8 meters. Either this indicates a very high flood, or this is inaccurate. | The minimum traffic_volume is 0 cars. This may be possible, but it is still best to inspect the data. | . Temperature Outliers . Let us graph a boxplot in order to find outliers among the temperature values. . sns.boxplot( data = highway, y = &quot;temp&quot;, ) plt.title(&quot;Temperature on I-94 (Kelvin)&quot;) plt.ylabel(&quot;Temperature (Kelvin)&quot;) plt.grid(True) plt.show() . Indeed, most values fall between 250 Kelvin and 300 Kelvin ($-23.15^{ circ} text{C}$ and $26.85^{ circ} text{C}$). The only outliers are at 0 Kelvin. This supports the idea that the zeroes are placeholders for missing values. . How many missing values are there? . (highway[&quot;temp&quot;] .value_counts(bins = 10) .sort_index() ) . (-0.311, 31.007] 10 (31.007, 62.014] 0 (62.014, 93.021] 0 (93.021, 124.028] 0 (124.028, 155.035] 0 (155.035, 186.042] 0 (186.042, 217.049] 0 (217.049, 248.056] 99 (248.056, 279.063] 17372 (279.063, 310.07] 23094 Name: temp, dtype: int64 . Only 10 datapoints have zero-values in temp. Thus, these can be dropped from the dataset. . highway = highway.loc[highway[&quot;temp&quot;] != 0] highway.shape . (40565, 9) . Now, there are 40565 rows in the dataset. Temperature outliers have been removed. . Rain Level Outliers . Similarly, we graph a boxplot below for the rain_1h column. . sns.boxplot( data = highway, y = &quot;rain_1h&quot;, ) plt.title(&quot;Hourly Rain Level on I-94 (mm)&quot;) plt.ylabel(&quot;Hourly Rain Level (mm)&quot;) plt.grid(True) plt.show() . Most of the values are close to 0 mm, and there are only a few outliers near 10,000 mm. How many outliers are there? . highway[&quot;rain_1h&quot;].value_counts(bins = 10).sort_index() . (-9.831999999999999, 983.13] 40564 (983.13, 1966.26] 0 (1966.26, 2949.39] 0 (2949.39, 3932.52] 0 (3932.52, 4915.65] 0 (4915.65, 5898.78] 0 (5898.78, 6881.91] 0 (6881.91, 7865.04] 0 (7865.04, 8848.17] 0 (8848.17, 9831.3] 1 Name: rain_1h, dtype: int64 . There is only 1 outlying datapoint. Since a 9.8 m flood level is so unrealistic given that most of the other values are small, this datapoint will be dropped. . highway = highway.loc[highway[&quot;rain_1h&quot;] &lt; 1000] highway.shape . (40564, 9) . The dataset is left with 40564 rows. . Traffic Volume Outliers . Below is the boxplot of traffic volume values. We want to see if the zero-values are reasonable or if these are distant outliers. . sns.boxplot( data = highway, y = &quot;traffic_volume&quot;, ) plt.title(&quot;Traffic Volume on I-94&quot;) plt.ylabel(&quot;Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The boxplot shows that 0 is within the approximate lower bound. It is not too distant from most of the datapoints to be considered an outlier. . Let us view a histogram to understand the distribution better. . sns.histplot( data = highway, x = &quot;traffic_volume&quot;, ) plt.title(&quot;Traffic Volume on I-94&quot;) plt.xlabel(&quot;Hourly Number of Cars&quot;) plt.grid(True) plt.show() . This is an unusual distribution. There appear to be 3 peaks: . less than 1000 cars | around 3000 cars | around 4500 cars | . It is common that less than 1000 cars pass through this I-94 station per hour. Therefore, it is likely that the 0-values are not outliers and do not need to be dropped from the dataset. . Data cleaning is done, so here are the new descriptive statistics for the dataset. . highway.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . count 40564 | 40564.000000 | 40564.000000 | 40564.000000 | 40564.000000 | 40564 | 40564 | 40564 | 40564.000000 | . unique 12 | NaN | NaN | NaN | NaN | 11 | 35 | NaN | NaN | . top None | NaN | NaN | NaN | NaN | Clouds | sky is clear | NaN | NaN | . freq 40511 | NaN | NaN | NaN | NaN | 15123 | 11632 | NaN | NaN | . mean NaN | 281.385602 | 0.076353 | 0.000117 | 44.209299 | NaN | NaN | 2015-12-24 02:14:28.937974528 | 3291.081402 | . min NaN | 243.390000 | 0.000000 | 0.000000 | 0.000000 | NaN | NaN | 2012-10-02 09:00:00 | 0.000000 | . 25% NaN | 271.850000 | 0.000000 | 0.000000 | 1.000000 | NaN | NaN | 2014-02-03 02:45:00 | 1249.750000 | . 50% NaN | 282.867500 | 0.000000 | 0.000000 | 40.000000 | NaN | NaN | 2016-06-02 19:30:00 | 3429.000000 | . 75% NaN | 292.280000 | 0.000000 | 0.000000 | 90.000000 | NaN | NaN | 2017-08-03 02:15:00 | 4952.000000 | . max NaN | 310.070000 | 55.630000 | 0.510000 | 100.000000 | NaN | NaN | 2018-09-30 23:00:00 | 7280.000000 | . std NaN | 13.092942 | 0.769729 | 0.005677 | 38.682163 | NaN | NaN | NaN | 1984.638849 | . Due to data cleaning, the following have changed: . Minimum temp is 243.39 Kelvin | Maximum rain_1h is 55.63 mm | . These are more reasonable values than before. . Exploratory Data Analysis . Traffic Volume: Day vs. Night . At the end of the data cleaning, we noticed that there were 3 peaks (most common values) in the traffic volume data. It is possible that this can be explained by comparing traffic volume between daytime and nighttime. . In order to do this, we can make a new column half which labels each entry as &quot;day&quot; or &quot;night.&quot; We will consider daytime to be from 6:00 AM to 6:00 PM, or 6:00 to 18:00. . highway[&quot;half&quot;] = ( highway[&quot;date_time&quot;] .dt.hour.between(6, 17) # Boolean Series where True represents day .replace({True: &quot;day&quot;, False: &quot;night&quot;}) # Replace booleans with strings ) highway[&quot;half&quot;].value_counts() . night 20418 day 20146 Name: half, dtype: int64 . There are 20418 nighttime entries and 20146 daytime entries. . Now we can compare the day and night histograms for traffic volume. . sns.histplot( data = highway, x = &quot;traffic_volume&quot;, hue = &quot;half&quot;, palette = &quot;RdBu&quot;, ) plt.title(&quot;Traffic Volume on I-94: Day and Night&quot;) plt.xlabel(&quot;Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The histogram above shows that: . In the nighttime, the traffic volume is commonly under 1000 or around 3000. | In the daytime, the traffic volume is commonly around 5000. | . Therefore, traffic is generally heavier in the daytime, between 6:00 AM and 6:00 PM. . Since we want to determine what influences heavy traffic, it would be best to focus our analysis on the daytime entries. Thus, such entries will be put in a separate DataFrame called daytime. . daytime = ( highway .loc[highway[&quot;half&quot;] == &quot;day&quot;] .drop(columns = &quot;half&quot;) ) daytime.describe( include = &quot;all&quot;, datetime_is_numeric = True, ) . holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume . count 20146 | 20146.000000 | 20146.000000 | 20146.000000 | 20146.000000 | 20146 | 20146 | 20146 | 20146.000000 | . unique 1 | NaN | NaN | NaN | NaN | 10 | 33 | NaN | NaN | . top None | NaN | NaN | NaN | NaN | Clouds | sky is clear | NaN | NaN | . freq 20146 | NaN | NaN | NaN | NaN | 8360 | 4971 | NaN | NaN | . mean NaN | 282.018771 | 0.076210 | 0.000124 | 47.342500 | NaN | NaN | 2015-12-26 11:32:49.582050816 | 4784.630100 | . min NaN | 243.390000 | 0.000000 | 0.000000 | 0.000000 | NaN | NaN | 2012-10-02 09:00:00 | 1.000000 | . 25% NaN | 272.220000 | 0.000000 | 0.000000 | 1.000000 | NaN | NaN | 2014-02-04 08:15:00 | 4311.000000 | . 50% NaN | 283.640000 | 0.000000 | 0.000000 | 40.000000 | NaN | NaN | 2016-06-07 11:30:00 | 4943.000000 | . 75% NaN | 293.370000 | 0.000000 | 0.000000 | 90.000000 | NaN | NaN | 2017-08-06 09:45:00 | 5678.000000 | . max NaN | 310.070000 | 44.450000 | 0.510000 | 100.000000 | NaN | NaN | 2018-09-30 17:00:00 | 7280.000000 | . std NaN | 13.330019 | 0.737429 | 0.005673 | 37.808456 | NaN | NaN | NaN | 1293.502893 | . This DataFrame contains only 20146 rows. The descriptive statistics are naturally somewhat different from before. . Effect of Units of Time . It is possible that traffic volume is influenced by certain units of time. For example, it could be influenced by the month, the day of the week, or the hour of the day. In this section, we investigate these factors. . By Month . First, does the month affect the traffic volume? . Let us make a new column that indicates the month as a number. . daytime[&quot;month&quot;] = daytime[&quot;date_time&quot;].dt.month daytime[&quot;month&quot;] . 0 10 1 10 2 10 3 10 4 10 .. 48191 9 48192 9 48194 9 48196 9 48197 9 Name: month, Length: 20146, dtype: int64 . Then, we can calculate and graph the average traffic volume per month. The median will be used instead of the mean since the data are not normally distributed. . Table: . (daytime .groupby(&quot;month&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . month . 1 4651.5 | . 2 4886.0 | . 3 5062.0 | . 4 5105.0 | . 5 5052.0 | . 6 5050.0 | . 7 4799.5 | . 8 5056.0 | . 9 4925.5 | . 10 5056.0 | . 11 4876.0 | . 12 4722.0 | . Line chart: . sns.lineplot( data = daytime, x = &quot;month&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Month on Traffic Volume&quot;) plt.xlabel(&quot;Month&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The line chart shows that the median traffic volume is highest in April, possibly because this is in spring. Traffic volume is lowest in January and December since these are in the middle of winter. July also has less traffic since it is in the middle of summer. . By Day of the Week . Next, we will investigate the effect of the day of the week on the traffic volume. . daytime[&quot;day&quot;] = daytime[&quot;date_time&quot;].dt.dayofweek (daytime .groupby(&quot;day&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . day . 0 4971.5 | . 1 5268.0 | . 2 5355.0 | . 3 5404.0 | . 4 5399.0 | . 5 4194.0 | . 6 3737.0 | . Note that 0 means Monday and 6 means Sunday. . The corresponding line chart is shown below. . sns.lineplot( data = daytime, x = &quot;day&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Day of Week on Traffic Volume&quot;) plt.xlabel(&quot;Day of the Week&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.grid(True) plt.show() . The line chart shows that the traffic volume is very high from Monday to Friday, then dips by 1000 cars on Saturday and Sunday. It makes sense that traffic is heavier on weekdays and lighter on weekends. . By Hour of the Day . Lastly, we will investigate the effect of the time of day on the traffic volume. Since we have narrowed the dataset down to daytime entries, only hours from 6:00 AM to 6:00 PM are included. . First, though, let&#39;s make a column that indicates weekdays and weekends so that we can compare them. . daytime[&quot;day_type&quot;] = daytime[&quot;day&quot;].replace({ 0: &quot;business day&quot;, 1: &quot;business day&quot;, 2: &quot;business day&quot;, 3: &quot;business day&quot;, 4: &quot;business day&quot;, 5: &quot;weekend&quot;, 6: &quot;weekend&quot;, }) . Next, below is the table of median traffic volume grouped by the day of the week and the type of day. . daytime[&quot;hour&quot;] = daytime[&quot;date_time&quot;].dt.hour (daytime .groupby([&quot;hour&quot;, &quot;day_type&quot;]) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . hour day_type . 6 business day 5588.0 | . weekend 1092.0 | . 7 business day 6320.5 | . weekend 1547.0 | . 8 business day 5751.0 | . weekend 2268.0 | . 9 business day 5053.0 | . weekend 3147.0 | . 10 business day 4484.5 | . weekend 3722.0 | . 11 business day 4714.0 | . weekend 4114.0 | . 12 business day 4921.0 | . weekend 4442.5 | . 13 business day 4919.5 | . weekend 4457.0 | . 14 business day 5232.0 | . weekend 4457.0 | . 15 business day 5740.0 | . weekend 4421.0 | . 16 business day 6403.5 | . weekend 4438.0 | . 17 business day 5987.0 | . weekend 4261.5 | . The hours are in 24-hour time; 17:00 represents the hour from 5:00 PM to 6:00 PM. . In order to understand this table better, we can visualize it in the line chart below. . sns.lineplot( data = daytime, x = &quot;hour&quot;, y = &quot;traffic_volume&quot;, hue = &quot;day_type&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Hour of Day on Traffic Volume&quot;) plt.xlabel(&quot;Hour of the Day&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.legend(title = &quot;Day Type&quot;) plt.grid(True) plt.show() . On business days, traffic is heaviest at 7:00 AM and 4:00 PM. These are the times when people travel to or from work. Traffic is lightest around noontime. . On weekends, traffic volume increases from 6:00 AM to 12:00 PM and plateaus from there on. People are free to travel at any time on weekends since most don&#39;t have work. However, the number of cars is still lower on weekends compared to business days. . Effect of Weather . Up until now, we have investigated the possible effects of different units of time on the traffic volume. In this section, we focus on how traffic is affected by the weather. . The following are the weather-related columns in the dataset: . temp | rain_1h | snow_1h | clouds_all | weather_main | weather_description | . The first 4 are numerical and the last 2 are categorical. . Numerical Weather Columns . Let us inspect the Pearson&#39;s correlation coefficient between traffic volume and each of the numerical weather columns. . daytime.corr().loc[ [&quot;temp&quot;, &quot;rain_1h&quot;, &quot;snow_1h&quot;, &quot;clouds_all&quot;], [&quot;traffic_volume&quot;] ] . traffic_volume . temp 0.124311 | . rain_1h -0.022817 | . snow_1h -0.004145 | . clouds_all 0.000621 | . All 4 numerical weather columns appear to have very weak correlations with traffic volume. . The highest correlation involves temperature, but the coefficient is only 12.43. This indicates a weak positive relationship. As temperature increases, traffic volume also increases, but not consistently. . We can understand the correlation better using a scatter plot. . sns.scatterplot( data = daytime, x = &quot;temp&quot;, y = &quot;traffic_volume&quot;, ci = None, ) plt.title(&quot;Effect of Temperature on Traffic Volume&quot;) plt.xlabel(&quot;Temperature (Kelvin)&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.grid(True) plt.show() . Unfortunately, the datapoints are scattered quite consistently throughout all combinations of temperature and traffic volume. The correlation is weak; temperature is not a reliable indicator of traffic. Neither are the other numerical weather columns, since their coefficients were even weaker. . Categorical Weather Columns . Next, we&#39;ll see if the categorical weather columns can serve as better indicators of heavy traffic. . Short Descriptions of Weather . The weather_main column contains short, 1-word descriptions of the weather. . What are the categories under weather_main? . daytime[&quot;weather_main&quot;].value_counts() . Clouds 8360 Clear 5821 Rain 2392 Mist 1441 Snow 1165 Haze 472 Drizzle 223 Thunderstorm 175 Fog 90 Smoke 7 Name: weather_main, dtype: int64 . Clouds, Clear, and Rain are the most frequent descriptions of the weather. . Next, let us investigate the effect of the weather description on the traffic volume. The table is shown below. . (daytime .groupby(&quot;weather_main&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . weather_main . Clear 4936.0 | . Clouds 4974.5 | . Drizzle 5132.0 | . Fog 5588.0 | . Haze 4817.5 | . Mist 5053.0 | . Rain 4975.0 | . Smoke 4085.0 | . Snow 4570.0 | . Thunderstorm 4875.0 | . This is visualized in the bar plot below. . sns.barplot( data = daytime, x = &quot;weather_main&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Short Weather Description on Traffic Volume&quot;) plt.xlabel(&quot;Short Weather Description&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.xticks(rotation = 45) plt.grid(True) plt.show() . The traffic volume appears to be mostly consistent across short weather descriptions. Notably: . Traffic is heaviest during fog (5588 cars). | Traffic is lightest when there is smoke (4085 cars). | . However, the effects are quite small, reaching only up to a difference of 1000 cars. . Long Descriptions of Weather . The weather_description column contains longer descriptions of the weather. . Below are its categories. . daytime[&quot;weather_description&quot;].value_counts() . sky is clear 4971 broken clouds 2683 overcast clouds 2526 scattered clouds 2068 mist 1441 light rain 1415 few clouds 1083 Sky is Clear 850 light snow 811 moderate rain 666 haze 472 heavy snow 249 heavy intensity rain 207 light intensity drizzle 160 proximity thunderstorm 136 snow 90 fog 90 proximity shower rain 89 drizzle 57 thunderstorm 22 light shower snow 11 light intensity shower rain 7 smoke 7 thunderstorm with light rain 7 very heavy rain 7 heavy intensity drizzle 6 thunderstorm with heavy rain 5 thunderstorm with rain 3 sleet 3 light rain and snow 1 freezing rain 1 proximity thunderstorm with drizzle 1 proximity thunderstorm with rain 1 Name: weather_description, dtype: int64 . Notice that there is a sky is clear value and a Sky is Clear value with different capitalization. It is likely that these two categories mean the same thing, so let us combine them. . daytime[&quot;weather_description&quot;].replace( {&quot;Sky is Clear&quot;: &quot;sky is clear&quot;}, inplace = True, ) daytime[&quot;weather_description&quot;].value_counts().head() . sky is clear 5821 broken clouds 2683 overcast clouds 2526 scattered clouds 2068 mist 1441 Name: weather_description, dtype: int64 . The sky is clear category now has 5821 entries. . Now that that&#39;s cleaned, let&#39;s make a table showing the effect of the long weather description on the traffic volume. . (daytime .groupby(&quot;weather_description&quot;) .median() [[&quot;traffic_volume&quot;]] ) . traffic_volume . weather_description . broken clouds 4925.0 | . drizzle 5132.0 | . few clouds 4977.0 | . fog 5588.0 | . freezing rain 4762.0 | . haze 4817.5 | . heavy intensity drizzle 5824.5 | . heavy intensity rain 4922.0 | . heavy snow 4673.0 | . light intensity drizzle 5086.5 | . light intensity shower rain 4695.0 | . light rain 5011.0 | . light rain and snow 5544.0 | . light shower snow 4324.0 | . light snow 4601.0 | . mist 5053.0 | . moderate rain 4940.0 | . overcast clouds 4968.0 | . proximity shower rain 4910.0 | . proximity thunderstorm 4849.5 | . proximity thunderstorm with drizzle 6667.0 | . proximity thunderstorm with rain 5730.0 | . scattered clouds 5032.5 | . sky is clear 4936.0 | . sleet 5174.0 | . smoke 4085.0 | . snow 4032.0 | . thunderstorm 5578.0 | . thunderstorm with heavy rain 5278.0 | . thunderstorm with light rain 4073.0 | . thunderstorm with rain 4270.0 | . very heavy rain 4802.0 | . The table is visualized in the bar graph below. . plt.figure(figsize = (14, 5)) sns.barplot( data = daytime, x = &quot;weather_description&quot;, y = &quot;traffic_volume&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Long Weather Description on Traffic Volume&quot;) plt.xlabel(&quot;Long Weather Description&quot;), plt.ylabel(&quot;Median Hourly Number of Cars&quot;) plt.xticks(rotation = 45, ha = &quot;right&quot;) plt.grid(True) plt.show() . Similar to the bar graph of short weather descriptions, most of the values are around 5000 cars. Notably: . Traffic is heaviest in a proximity thunderstorm with drizzle (6667 cars). The word &quot;proximity&quot; was likely used to emphasize that the storm was very close to the station. | It makes sense that a drizzling thunderstorm directly over the highway would reduce visibility and make traffic pile up. | . | Traffic is lightest in snow (4032 cars). It is likely that more people choose not to travel outside when it is snowing. | . | . Conclusion . This project aimed to determine reliable indicators of heavy traffic along the I-94 highway. Data were narrowed down to daytime (as opposed to nighttime) entries, which have higher traffic volumes. Exploratory data analysis was conducted using mostly Seaborn visualizations. . The following conclusions were drawn: . Time has various effects on traffic volume. Traffic is heavier when: It is daytime (between 6:00 AM and 6:00 PM). | The month is from March to June. | The day is a business day, as opposed to a weekend. | The hour is 7:00 AM or 4:00 PM on a business day. | . | . . Weather affects traffic volume to a lesser extent. Traffic is heavier when: There is fog on the road (or other visual obstructions). | There is a nearby thunderstorm with drizzle or rain. | . | . . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/2021/05/25/Indicators-Heavy-Traffic-I-94-Highway.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/2021/05/25/Indicators-Heavy-Traffic-I-94-Highway.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Basic Data Cleaning with eBay Car Sales Data",
            "content": ". Unsplash | Parker Gibbs Overview . In 2016, Kaggle user orgezleka scraped data about used car advertisements on eBay Kleinanzeigen, which is the eBay Classifieds website of Germany. The full dataset is still available on Used Cars Data (data.world 2016), with over 370,000 datapoints. . The project aims to answer the following questions: . Which brands of the used cars are most expensive? | Does a used car&#39;s age affect its price? | Does a used car&#39;s mileage affect its price? | Does the presence of unrepaired damage affect a car&#39;s price? | . More importantly, this project aims to showcase basic data cleaning procedures. Thus, even columns beyond the scope of the research questions will be considered for cleaning. The assumption is that analysis will eventually be performed on all columns. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Exploring eBay Car Sales Data. The general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data Overview . Below is the information about each column in the dataset, taken from Used Cars Data (data.world 2016). . dateCrawled : when this ad was first crawled, all field-values are taken from this date | name : &quot;name&quot; of the car | seller : private or dealer | offerType | price : the price on the ad to sell the car | abtest | vehicleType | yearOfRegistration : at which year the car was first registered | gearbox | powerPS : power of the car in PS | model | kilometer : how many kilometers the car has driven | monthOfRegistration : at which month the car was first registered | fuelType | brand | notRepairedDamage : if the car has a damage which is not repaired yet | dateCreated : the date for which the ad at ebay was created | nrOfPictures : number of pictures in the ad | postalCode | lastSeenOnline : when the crawler saw this ad last online | . Let us view the first 5 rows of the dataset below. . autos = pd.read_csv(&quot;./private/2021-05-20-BDC-Files/autos.csv&quot;, encoding = &quot;Windows-1252&quot;) autos.head() . dateCrawled name seller offerType price abtest vehicleType yearOfRegistration gearbox powerPS model kilometer monthOfRegistration fuelType brand notRepairedDamage dateCreated nrOfPictures postalCode lastSeen . 0 2016-03-24 11:52:17 | Golf_3_1.6 | privat | Angebot | 480 | test | NaN | 1993 | manuell | 0 | golf | 150000 | 0 | benzin | volkswagen | NaN | 2016-03-24 00:00:00 | 0 | 70435 | 2016-04-07 03:16:57 | . 1 2016-03-24 10:58:45 | A5_Sportback_2.7_Tdi | privat | Angebot | 18300 | test | coupe | 2011 | manuell | 190 | NaN | 125000 | 5 | diesel | audi | ja | 2016-03-24 00:00:00 | 0 | 66954 | 2016-04-07 01:46:50 | . 2 2016-03-14 12:52:21 | Jeep_Grand_Cherokee_&quot;Overland&quot; | privat | Angebot | 9800 | test | suv | 2004 | automatik | 163 | grand | 125000 | 8 | diesel | jeep | NaN | 2016-03-14 00:00:00 | 0 | 90480 | 2016-04-05 12:47:46 | . 3 2016-03-17 16:54:04 | GOLF_4_1_4__3TÜRER | privat | Angebot | 1500 | test | kleinwagen | 2001 | manuell | 75 | golf | 150000 | 6 | benzin | volkswagen | nein | 2016-03-17 00:00:00 | 0 | 91074 | 2016-03-17 17:40:17 | . 4 2016-03-31 17:25:20 | Skoda_Fabia_1.4_TDI_PD_Classic | privat | Angebot | 3600 | test | kleinwagen | 2008 | manuell | 69 | fabia | 90000 | 7 | diesel | skoda | nein | 2016-03-31 00:00:00 | 0 | 60437 | 2016-04-06 10:17:21 | . We can get additional information about each column using DataFrame.info(). . autos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 371528 entries, 0 to 371527 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 dateCrawled 371528 non-null object 1 name 371528 non-null object 2 seller 371528 non-null object 3 offerType 371528 non-null object 4 price 371528 non-null int64 5 abtest 371528 non-null object 6 vehicleType 333659 non-null object 7 yearOfRegistration 371528 non-null int64 8 gearbox 351319 non-null object 9 powerPS 371528 non-null int64 10 model 351044 non-null object 11 kilometer 371528 non-null int64 12 monthOfRegistration 371528 non-null int64 13 fuelType 338142 non-null object 14 brand 371528 non-null object 15 notRepairedDamage 299468 non-null object 16 dateCreated 371528 non-null object 17 nrOfPictures 371528 non-null int64 18 postalCode 371528 non-null int64 19 lastSeen 371528 non-null object dtypes: int64(7), object(13) memory usage: 56.7+ MB . The dataset has 371,528 rows and 20 columns. Each column contains one of two types of data: . int64: 64-bit integers | object: text strings | . Also, some columns have a non-null count smaller than the number of entries. This means that there are missing values in these columns. The column notRepairedDamagehas the most missing values. . Data Cleaning . Column Labels . As seen earlier, all of the column labels are written in camel case. This means that the first letter of each word is capitalized, like CamelCase. . Column labels should ideally be written in snake_case so that these are easier to use. Thus, I will convert all column labels to snake case. . cols = list(autos.columns) new_cols = [] for label in cols: # List of indices of the first letter of each word. cap_inds = [0] + [index for index, character in enumerate(label) if character.isupper()] # List of 2-tuples. Each tuple contains the start index of the current word # and the start index of the next word. zipped = list(zip(cap_inds, cap_inds[1:] + [None])) # Split the label into a list of words. # Make them lowercase and combine them with underscores. word_list = [label[i:j] for i, j in zipped] word_list = [word.lower() for word in word_list] new_label = &quot;_&quot;.join(word_list) new_cols.append(new_label) autos.columns = new_cols autos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 371528 entries, 0 to 371527 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 date_crawled 371528 non-null object 1 name 371528 non-null object 2 seller 371528 non-null object 3 offer_type 371528 non-null object 4 price 371528 non-null int64 5 abtest 371528 non-null object 6 vehicle_type 333659 non-null object 7 year_of_registration 371528 non-null int64 8 gearbox 351319 non-null object 9 power_p_s 371528 non-null int64 10 model 351044 non-null object 11 kilometer 371528 non-null int64 12 month_of_registration 371528 non-null int64 13 fuel_type 338142 non-null object 14 brand 371528 non-null object 15 not_repaired_damage 299468 non-null object 16 date_created 371528 non-null object 17 nr_of_pictures 371528 non-null int64 18 postal_code 371528 non-null int64 19 last_seen 371528 non-null object dtypes: int64(7), object(13) memory usage: 56.7+ MB . Now, all of the column labels are in snake case. However, I will manually change a few of them to make them neater. . convert = { &quot;power_p_s&quot;: &quot;power_ps&quot;, &quot;nr_of_pictures&quot;: &quot;num_pictures&quot;, &quot;year_of_registration&quot;: &quot;year_reg&quot;, &quot;kilometer&quot;: &quot;mileage_km&quot;, &quot;month_of_registration&quot;: &quot;month_reg&quot;, &quot;not_repaired_damage&quot;: &quot;damage&quot;, } autos.columns = pd.Series(autos.columns).replace(convert) autos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 371528 entries, 0 to 371527 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 date_crawled 371528 non-null object 1 name 371528 non-null object 2 seller 371528 non-null object 3 offer_type 371528 non-null object 4 price 371528 non-null int64 5 abtest 371528 non-null object 6 vehicle_type 333659 non-null object 7 year_reg 371528 non-null int64 8 gearbox 351319 non-null object 9 power_ps 371528 non-null int64 10 model 351044 non-null object 11 mileage_km 371528 non-null int64 12 month_reg 371528 non-null int64 13 fuel_type 338142 non-null object 14 brand 371528 non-null object 15 damage 299468 non-null object 16 date_created 371528 non-null object 17 num_pictures 371528 non-null int64 18 postal_code 371528 non-null int64 19 last_seen 371528 non-null object dtypes: int64(7), object(13) memory usage: 56.7+ MB . All of the column labels are now easy to use. . Uninformative Columns . Next, we will look for columns filled with mostly 1 value. The other values in them are underrepresented, so we wouldn&#39;t be able to compare groups reliably. . We can inspect the columns using DataFrame.describe(). . autos.describe(include = &quot;all&quot;) # Include all columns, including text. . date_crawled name seller offer_type price abtest vehicle_type year_reg gearbox power_ps model mileage_km month_reg fuel_type brand damage date_created num_pictures postal_code last_seen . count 371528 | 371528 | 371528 | 371528 | 3.715280e+05 | 371528 | 333659 | 371528.000000 | 351319 | 371528.000000 | 351044 | 371528.000000 | 371528.000000 | 338142 | 371528 | 299468 | 371528 | 371528.0 | 371528.00000 | 371528 | . unique 280500 | 233531 | 2 | 2 | NaN | 2 | 8 | NaN | 2 | NaN | 251 | NaN | NaN | 7 | 40 | 2 | 114 | NaN | NaN | 182806 | . top 2016-03-24 14:49:47 | Ford_Fiesta | privat | Angebot | NaN | test | limousine | NaN | manuell | NaN | golf | NaN | NaN | benzin | volkswagen | nein | 2016-04-03 00:00:00 | NaN | NaN | 2016-04-07 06:45:59 | . freq 7 | 657 | 371525 | 371516 | NaN | 192585 | 95894 | NaN | 274214 | NaN | 30070 | NaN | NaN | 223857 | 79640 | 263182 | 14450 | NaN | NaN | 17 | . mean NaN | NaN | NaN | NaN | 1.729514e+04 | NaN | NaN | 2004.577997 | NaN | 115.549477 | NaN | 125618.688228 | 5.734445 | NaN | NaN | NaN | NaN | 0.0 | 50820.66764 | NaN | . std NaN | NaN | NaN | NaN | 3.587954e+06 | NaN | NaN | 92.866598 | NaN | 192.139578 | NaN | 40112.337051 | 3.712412 | NaN | NaN | NaN | NaN | 0.0 | 25799.08247 | NaN | . min NaN | NaN | NaN | NaN | 0.000000e+00 | NaN | NaN | 1000.000000 | NaN | 0.000000 | NaN | 5000.000000 | 0.000000 | NaN | NaN | NaN | NaN | 0.0 | 1067.00000 | NaN | . 25% NaN | NaN | NaN | NaN | 1.150000e+03 | NaN | NaN | 1999.000000 | NaN | 70.000000 | NaN | 125000.000000 | 3.000000 | NaN | NaN | NaN | NaN | 0.0 | 30459.00000 | NaN | . 50% NaN | NaN | NaN | NaN | 2.950000e+03 | NaN | NaN | 2003.000000 | NaN | 105.000000 | NaN | 150000.000000 | 6.000000 | NaN | NaN | NaN | NaN | 0.0 | 49610.00000 | NaN | . 75% NaN | NaN | NaN | NaN | 7.200000e+03 | NaN | NaN | 2008.000000 | NaN | 150.000000 | NaN | 150000.000000 | 9.000000 | NaN | NaN | NaN | NaN | 0.0 | 71546.00000 | NaN | . max NaN | NaN | NaN | NaN | 2.147484e+09 | NaN | NaN | 9999.000000 | NaN | 20000.000000 | NaN | 150000.000000 | 12.000000 | NaN | NaN | NaN | NaN | 0.0 | 99998.00000 | NaN | . The top row gives the most frequent value in the column. The freq row tells exactly how often the top value occurs. . We can see that in the seller column, &quot;privat&quot; appears 371,525 times; this is close to the total number of entries. Also, in the offer_type column, &quot;Angebot&quot; appears 371,516 times. . Let us first check the unique values of the seller column. . autos[&quot;seller&quot;].value_counts() . privat 371525 gewerblich 3 Name: seller, dtype: int64 . In English, these mean &quot;private&quot; and &quot;commercial.&quot; It is likely that private listings are put up by individuals, whereas commercial listings are put up by used car dealer companies. Most listings scraped were apparently private. . One idea is to compare private listing prices to commercial listing prices. However, there are only 3 commercial listings, so this comparison would be unreliable. Therefore, we can drop this column. . autos.drop(columns = &quot;seller&quot;, inplace = True) . Secondly, let&#39;s check the unique values of the offer_type column. . autos[&quot;offer_type&quot;].value_counts() . Angebot 371516 Gesuch 12 Name: offer_type, dtype: int64 . In English, these mean &quot;offer&quot; and &quot;request.&quot; This isn&#39;t very informative. It could have something to do with the system of sending Offers to buyers in eBay. In any case, it doesn&#39;t seem interesting to analyze, and there are only 12 request listings. Thus, we can also drop this column. . autos.drop(columns = &quot;offer_type&quot;, inplace = True) . Next, the numerical num_pictures column has a minimum of 0 and a maximum of 0. . autos[&quot;num_pictures&quot;].describe() . count 371528.0 mean 0.0 std 0.0 min 0.0 25% 0.0 50% 0.0 75% 0.0 max 0.0 Name: num_pictures, dtype: float64 . This means that none of the listings for used cars were found to have pictures. This data may be true or it may have come from an error, but either way, it is not useful. Thus, it will be dropped. . autos.drop(columns = &quot;num_pictures&quot;, inplace = True) . Outliers in Numerical Columns . Next, we inspect the ranges of numerical columns to see if there are any unusually high or low values. . autos.describe(include = np.number) . price year_reg power_ps mileage_km month_reg postal_code . count 3.715280e+05 | 371528.000000 | 371528.000000 | 371528.000000 | 371528.000000 | 371528.00000 | . mean 1.729514e+04 | 2004.577997 | 115.549477 | 125618.688228 | 5.734445 | 50820.66764 | . std 3.587954e+06 | 92.866598 | 192.139578 | 40112.337051 | 3.712412 | 25799.08247 | . min 0.000000e+00 | 1000.000000 | 0.000000 | 5000.000000 | 0.000000 | 1067.00000 | . 25% 1.150000e+03 | 1999.000000 | 70.000000 | 125000.000000 | 3.000000 | 30459.00000 | . 50% 2.950000e+03 | 2003.000000 | 105.000000 | 150000.000000 | 6.000000 | 49610.00000 | . 75% 7.200000e+03 | 2008.000000 | 150.000000 | 150000.000000 | 9.000000 | 71546.00000 | . max 2.147484e+09 | 9999.000000 | 20000.000000 | 150000.000000 | 12.000000 | 99998.00000 | . The following unusual characteristics are noticeable: . price: The minimum price is 0 (free) and the maximum price is larger than 2 billion. | year_reg: The earliest year of registration is 1000, and the latest year is 9999. | power_ps: The lowest power is 0 PS. The highest is 20,000 PS, which is much higher than the 75th percentile (only 150 PS). | month_reg: The month numbers range from 0 to 12, not 1 to 12. | . All of these columns shall be cleaned of inaccurate data. . Price in Euros . It was seen that the minimum value for price data was 0, and the maximum is over 2,000,000,000. Since this data was scraped from the German eBay, we can assume that these prices are in euros. . Both the minimum and maximum values seem to be very unrealistic for the price of a used car, so we want to clean this data. . Let us create a frequency table of the prices. We will first sort it by price, ascending. . autos[&quot;price&quot;].value_counts().sort_index() . 0 10778 1 1189 2 12 3 8 4 1 ... 32545461 1 74185296 1 99000000 1 99999999 15 2147483647 1 Name: price, Length: 5597, dtype: int64 . Two things are noticeable: . There are over 10,000 instances of 0 euro prices. | There are several extremely high outliers, not just the one above 2 billion. | . Since it is unlikely that the cars are actually free, 0 may represent missing values. As for the high outliers, these may have resulted from inaccurate scraping. . The next question is, what is the most frequent price value? Is it 0? . autos[&quot;price&quot;].value_counts().sort_values(ascending = False) . 0 10778 500 5670 1500 5394 1000 4649 1200 4594 ... 2610 1 20620 1 16689 1 9964 1 10985 1 Name: price, Length: 5597, dtype: int64 . Indeed, the most frequent price is 0, which makes it likely that it represents missing values. The next most frequent prices are 500, 1500, and 1000, most likely because these are easy for buyers to remember. . Now that we know about the low and high outliers, we can clean rows using the interquartile range or IQR. The IQR is bounded by the 25th and 75th percentiles. Reasonable lower and upper bounds can be approximated by multiplying the IQR length by 1.5 and: . subtracting it from the 25th percentile | adding it to the 75th percentile | . Reference: Detection and Removal of Outliers in Python – An Easy to Understand Guide . Below, I use this method to find a lower and upper bound, and I drop rows outside of them. . autos_desc = autos[&quot;price&quot;].describe() q25, q75 = autos_desc[[&quot;25%&quot;, &quot;75%&quot;]] iqr = q75 - q25 lower_bound = q25 - 1.5 * iqr upper_bound = q75 + 1.5 * iqr autos = autos.loc[autos[&quot;price&quot;].between(lower_bound, upper_bound)] print(&quot;Lower bound:&quot;, lower_bound) print(&quot;Upper bound:&quot;, upper_bound) print(&quot;New shape:&quot;, autos.shape) . Lower bound: -7925.0 Upper bound: 16275.0 New shape: (343420, 17) . Now, there are 343,420 rows remaining in the dataset. This is still quite large; not too many datapoints were removed. . Let us view the new distribution of prices in a histogram. . ax = sns.histplot( data = autos, x = &quot;price&quot;, ) ax.set_title(&quot;Used Car Prices, Cleaned Using IQR&quot;) ax.grid(True) plt.show() . Unfortunately, since the lower bound was negative, unrealistically low prices close to 0 were still accepted within the range. . Remember from earlier that the 2nd most frequent price value was found to be 500 euros. This makes it likely that these were correct prices, rather than incorrectly scraped values. Thus, let us keep all price values greater than or equal to 500. . autos = autos.loc[autos[&quot;price&quot;] &gt;= 500] autos.shape . (307358, 17) . Now, 307,358 datapoints remain. Let us look at the final distribution of prices: . ax = sns.histplot( data = autos, x = &quot;price&quot;, ) ax.set_title(&quot;Used Car Prices, Cleaned of Low Values&quot;) ax.grid(True) plt.show() . The distribution is still right-skewed, but at least the price range in the dataset is more reasonable now. . Metric Horsepower . We saw earlier that some power values were at 0 PS, and others were as high as 20,000 PS. . Note: A measure of 1 PS (metric horsepower) is equivalent to 0.98 HP (horsepower). (Deriquito 2020) . autos[&quot;power_ps&quot;].describe() . count 307358.000000 mean 112.551842 std 191.557378 min 0.000000 25% 75.000000 50% 105.000000 75% 143.000000 max 20000.000000 Name: power_ps, dtype: float64 . Above are the new descriptive statistics for metric horsepower, after the transformations that we have done recently. . According to &quot;What is the Average Car Horsepower?&quot; (2021), cars usually have a horsepower of 100 HP to 400 HP, though some have less. Faster cars have 400 HP to 1,000 HP, and some supercars have over 1,000 HP. The equivalent values in PS would be slightly higher. . Considering that these are used cars, it is unlikely for them to be very fast much less to be supercars. Thus, we will keep cars with metric horsepowers between 50 and 450, in order to take as many of the realistic values as possible. . autos = autos.loc[autos[&quot;power_ps&quot;].between(50, 450)] autos[&quot;power_ps&quot;].describe() . count 274514.000000 mean 121.166931 std 50.535320 min 50.000000 25% 82.000000 50% 114.000000 75% 150.000000 max 450.000000 Name: power_ps, dtype: float64 . Let us view the distribution of power values in a histogram. . ax = sns.histplot( data = autos, x = &quot;power_ps&quot;, ) ax.set_title(&quot;Metric Horsepower (PS) of Used Cars&quot;) ax.grid(True) plt.show() . Like the price data, the power data are right-skewed. Lower values are more frequent. Still, we managed to capture most (274,514) of the datapoints using this range. . Year of Registration . Earlier, we noticed that the years of registration ranged from 1000 to 9999. . autos[&quot;year_reg&quot;].describe() . count 274514.000000 mean 2003.658039 std 29.529992 min 1000.000000 25% 1999.000000 50% 2004.000000 75% 2007.000000 max 9999.000000 Name: year_reg, dtype: float64 . Logically, the years of registration should only range from when automobiles were first mass-produced, in the 1800s or 1900s (Encyclopedia Britannica), to the year when the ads were last seen by the web crawler. . We can know when the ads were last seen using the last_seen column of the dataset. The dates are shown as strings with years first, so we can sort them in descending order to find the most recent date. . autos[&quot;last_seen&quot;].sort_values(ascending = False) . 343211 2016-04-07 14:58:51 5069 2016-04-07 14:58:50 72231 2016-04-07 14:58:50 62402 2016-04-07 14:58:50 122068 2016-04-07 14:58:50 ... 244077 2016-03-05 14:35:28 177326 2016-03-05 14:25:59 136842 2016-03-05 14:15:39 275196 2016-03-05 14:15:16 311225 2016-03-05 14:15:08 Name: last_seen, Length: 274514, dtype: object . It looks like all of the ads were last seen in 2016, so the cars could not have been registered beyond that year. . Therefore, a reasonable range of years would be from 1900 to 2016. Let us keep all rows within this range. . autos = autos.loc[autos[&quot;year_reg&quot;].between(1900, 2016)] autos[&quot;year_reg&quot;].describe() . count 264977.000000 mean 2002.977877 std 6.097207 min 1910.000000 25% 1999.000000 50% 2003.000000 75% 2007.000000 max 2016.000000 Name: year_reg, dtype: float64 . Now, the years of registration range from 1910 to 2016. There are also 264,977 datapoints remaining. . Months of Registration . Lastly, we noticed that the months of registration included integers from 0 to 12, not 1 to 12. Let us make a discrete histogram in order to understand the distribution. . ax = sns.histplot( data = autos, x = &quot;month_reg&quot;, discrete = True, ) ax.set_title(&quot;Used Cars&#39; Months of Registration&quot;) ax.grid(True) plt.show() . The graph above shows that 0 is the least frequent value, appearing under 15,000 times. It seems very likely that it is a placeholder for unknown months. Therefore, we can remove rows with month 0. . autos = autos.loc[autos[&quot;month_reg&quot;] &gt;= 1] autos[&quot;month_reg&quot;].describe() . count 251198.000000 mean 6.396237 std 3.346424 min 1.000000 25% 4.000000 50% 6.000000 75% 9.000000 max 12.000000 Name: month_reg, dtype: float64 . Missing Values . Now, we shall deal with the null values in the dataset. First, how many null values are there per column? . autos.isnull().sum() . date_crawled 0 name 0 price 0 abtest 0 vehicle_type 5871 year_reg 0 gearbox 3103 power_ps 0 model 7422 mileage_km 0 month_reg 0 fuel_type 9057 brand 0 damage 28718 date_created 0 postal_code 0 last_seen 0 dtype: int64 . There are thousands of missing values for the vehicle type, gearbox, model, fuel type, and presence of damage. None of these can be easily determined from other columns. . If we were to remove all rows with these missing values, we would be left with: . autos.dropna().shape . (208538, 17) . We would have 208,538 rows left. This is just over half of the original number of datapoints we started with, 371,528. . This project is meant to showcase data cleaning, and we are assuming that all of the columns in the dataset will be used for analysis. Thus, we will have to delete all rows with missing values. . autos.dropna(inplace = True) . Below are the final descriptive statistics for the dataset after all of the cleaning. . autos.describe(include = &quot;all&quot;) . date_crawled name price abtest vehicle_type year_reg gearbox power_ps model mileage_km month_reg fuel_type brand damage date_created postal_code last_seen . count 208538 | 208538 | 208538.000000 | 208538 | 208538 | 208538.000000 | 208538 | 208538.000000 | 208538 | 208538.000000 | 208538.000000 | 208538 | 208538 | 208538 | 208538 | 208538.000000 | 208538 | . unique 177458 | 114617 | NaN | 2 | 8 | NaN | 2 | NaN | 248 | NaN | NaN | 7 | 39 | 2 | 98 | NaN | 116677 | . top 2016-04-02 22:54:55 | BMW_318i | NaN | test | limousine | NaN | manuell | NaN | golf | NaN | NaN | benzin | volkswagen | nein | 2016-04-03 00:00:00 | NaN | 2016-04-07 09:45:41 | . freq 5 | 616 | NaN | 108258 | 62894 | NaN | 166213 | NaN | 17505 | NaN | NaN | 136145 | 44157 | 189720 | 8271 | NaN | 14 | . mean NaN | NaN | 4984.662522 | NaN | NaN | 2003.205953 | NaN | 122.635031 | NaN | 128250.990227 | 6.381225 | NaN | NaN | NaN | NaN | 51907.756020 | NaN | . std NaN | NaN | 3990.028529 | NaN | NaN | 5.696916 | NaN | 50.515054 | NaN | 35781.848172 | 3.350349 | NaN | NaN | NaN | NaN | 25766.228717 | NaN | . min NaN | NaN | 500.000000 | NaN | NaN | 1937.000000 | NaN | 50.000000 | NaN | 5000.000000 | 1.000000 | NaN | NaN | NaN | NaN | 1067.000000 | NaN | . 25% NaN | NaN | 1749.000000 | NaN | NaN | 2000.000000 | NaN | 85.000000 | NaN | 125000.000000 | 3.000000 | NaN | NaN | NaN | NaN | 31319.000000 | NaN | . 50% NaN | NaN | 3750.000000 | NaN | NaN | 2004.000000 | NaN | 116.000000 | NaN | 150000.000000 | 6.000000 | NaN | NaN | NaN | NaN | 51371.000000 | NaN | . 75% NaN | NaN | 7300.000000 | NaN | NaN | 2007.000000 | NaN | 150.000000 | NaN | 150000.000000 | 9.000000 | NaN | NaN | NaN | NaN | 72649.000000 | NaN | . max NaN | NaN | 16270.000000 | NaN | NaN | 2016.000000 | NaN | 450.000000 | NaN | 150000.000000 | 12.000000 | NaN | NaN | NaN | NaN | 99998.000000 | NaN | . Data Transformation . Strings to Datetime Objects . As we saw earlier, the date_crawled, date_created, and last_seen columns contain dates and times in string format. It would be better to store these as datetime objects so that each part (year, month, etc.) can be accessed in analysis. . For this section, I used &quot;Convert the column type from string to datetime format in Pandas dataframe&quot; (2020) as a reference. . First, though, we have to make a format string so that the numbers can be parsed properly. . autos[[&quot;date_crawled&quot;, &quot;date_created&quot;, &quot;last_seen&quot;]].head() . date_crawled date_created last_seen . 3 2016-03-17 16:54:04 | 2016-03-17 00:00:00 | 2016-03-17 17:40:17 | . 4 2016-03-31 17:25:20 | 2016-03-31 00:00:00 | 2016-04-06 10:17:21 | . 5 2016-04-04 17:36:23 | 2016-04-04 00:00:00 | 2016-04-06 19:17:07 | . 6 2016-04-01 20:48:51 | 2016-04-01 00:00:00 | 2016-04-05 18:18:39 | . 10 2016-03-26 19:54:18 | 2016-03-26 00:00:00 | 2016-04-06 10:45:34 | . All 3 columns seem to follow the same format: . {4 digit year}-{2 digit month}-{2 digit day} {24-hour time hour}:{minute}:{second} . The equivalent format string is below. . format_str = &quot;%Y-%m-%d %H:%M:%S&quot; . We can use this string to parse the dates. This can be done quickly by using the pd.to_datetime() vectorized function, instead of using a for-loop with the datetime module. . autos[&quot;date_crawled&quot;] = pd.to_datetime( autos[&quot;date_crawled&quot;], format = format_str, ) autos[&quot;date_crawled&quot;] . 3 2016-03-17 16:54:04 4 2016-03-31 17:25:20 5 2016-04-04 17:36:23 6 2016-04-01 20:48:51 10 2016-03-26 19:54:18 ... 371516 2016-04-04 09:57:12 371517 2016-03-28 13:48:07 371520 2016-03-19 19:53:49 371524 2016-03-05 19:56:21 371525 2016-03-19 18:57:12 Name: date_crawled, Length: 208538, dtype: datetime64[ns] . We can see that the values look similar to how they looked before, but the dtype is now datetime64[ns]. . For example, we can now access the month and day from the datetime objects. . autos[&quot;date_crawled&quot;].iloc[0].month . 3 . autos[&quot;date_crawled&quot;].iloc[0].day . 17 . We can see that the date in the first row has a month of 3 (March) and a day of 17. . Let us do the same for the other 2 date columns. . autos[&quot;date_created&quot;] = pd.to_datetime( autos[&quot;date_created&quot;], format = format_str, ) autos[&quot;last_seen&quot;] = pd.to_datetime( autos[&quot;last_seen&quot;], format = format_str, ) autos[[&quot;date_crawled&quot;, &quot;date_created&quot;, &quot;last_seen&quot;]].describe( datetime_is_numeric = True, ) . date_crawled date_created last_seen . count 208538 | 208538 | 208538 | . mean 2016-03-21 13:39:21.839721984 | 2016-03-20 19:44:12.604321536 | 2016-03-30 09:43:36.598633984 | . min 2016-03-05 14:06:23 | 2015-08-07 00:00:00 | 2016-03-05 14:15:16 | . 25% 2016-03-13 11:06:28 | 2016-03-13 00:00:00 | 2016-03-23 20:52:41.500000 | . 50% 2016-03-21 18:46:44.500000 | 2016-03-21 00:00:00 | 2016-04-04 14:39:49.500000 | . 75% 2016-03-29 15:36:22.500000 | 2016-03-29 00:00:00 | 2016-04-06 11:45:07 | . max 2016-04-07 14:36:58 | 2016-04-07 00:00:00 | 2016-04-07 14:58:51 | . We can see that since the 3 columns are in datetime format, they are now treated as numerical values. They have a mean and a list of percentiles. . Data transformation is done, so we can move on to data analysis. . Data Analysis . Statistical analyses with hypothesis testing are outside of the scope of this project, since it is centered on data cleaning. Thus, I will perform some simple aggregates and visualize them in graphs. . Most Expensive Car Brands . Below, a bar graph shows each car brand and the median price. The mean is not used because we know that the price data are right-skewed, not normal. . plt.figure(figsize = (14, 5)) ax = sns.barplot( data = autos, x = &quot;brand&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Median Prices of Used Cars by Brand&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . The graph shows that Porsche has the highest median price of used cars, at almost 12,000 euros. This is followed by Mini, Land Rover, and Jeep. . Used Car Prices by Age . The line plot below shows the relationship between a car&#39;s year of registration (later -&gt; newer) and the median price. . plt.figure(figsize = (14, 5)) ax = sns.lineplot( data = autos, x = &quot;year_reg&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Median Prices of Used Cars by Year of Registration&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . The line plot shows that used car prices are lowest when the year of registration is in the mid-1990s. . To the right of the minimum, as the year of registration gets later, the median price steadily becomes higher. This makes sense because newer cars would have a higher value, even if they&#39;re used. . On the other hand, to the left of the minimum, the median price increases (albeit erratically) as the year of registration gets earlier. This suggests that very old cars are considered to be valuable because they are rare. It is likely that the avid car collectors are the ones who pay such high prices for these cars. . Used Car Prices by Mileage . The line plot below shows the relationship between used cars&#39; mileage in kilometers and the median price. . plt.figure(figsize = (14, 5)) ax = sns.lineplot( data = autos, x = &quot;mileage_km&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Median Prices of Used Cars by Mileage in Kilometers&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . As expected, price decreases as mileage increases. What is unexpected is that there is a sharp increase in price from around 500 km to 1000 km. This was likely caused by an outlier datapoint which had a low mileage and a low price. . Effect of Presence of Damage on Used Car Price . Lastly, we will analyze the effect of the presence of unrepaired damage on the median price of used cars. . ax = sns.barplot( data = autos, x = &quot;damage&quot;, y = &quot;price&quot;, estimator = np.median, ci = None, ) plt.title(&quot;Effect of Presence of Damage on Median Used Car Price&quot;) plt.grid(True) plt.xticks(rotation = 90) plt.show() . It can be seen that the median price of used cars without damage is 4000 euros. The median price is 1500 euros for those with unrepaired damage. . Therefore, cars with damage generally have lower prices. This makes sense because if the buyer plans to drive the car, they must shoulder the expense of repairing it. . Conclusion . In this project, we did cleaning, transformation, and simple analyses on data about used cars. Below are the insights we gained with regards to each research question. Note that prices were aggregated using the median, not the mean. . Which brands of the used cars are most expensive? | . The top 4 most expensive brands of used cars are Porsche, Mini, Land Rover, and Jeep (descending order). . . Does a used car&#39;s age affect its price? | . Yes. Cars registered in the mid-1990s have the lowest prices, with a median of 1000 euros. . As cars get newer from that point, their median price increases. As cars get older from the same point, their median price also increases. Very old cars are rare and therefore valuable to collectors. . . Does a used car&#39;s mileage affect its price? | . Yes, as mileage increases, price decreases. . . Does the presence of unrepaired damage affect a car&#39;s price? | . Yes, the median price of damaged used cars is much lower than that of cars without damage. . . . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/matplotlib/seaborn/datetime/2021/05/20/Basic-Data-Cleaning-eBay-Car-Sales-Data.html",
            "relUrl": "/python/pandas/numpy/matplotlib/seaborn/datetime/2021/05/20/Basic-Data-Cleaning-eBay-Car-Sales-Data.html",
            "date": " • May 20, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Optimizing Hacker News Posts",
            "content": ". Unsplash | Clint Patterson Overview . Hacker News is a popular website about technology. Specifically, it is a community choice aggregator of tech content. Users can: . Submit tech articles that the user found online. | Submit &quot;Ask&quot; posts to ask the community a question. | Submit &quot;Show&quot; posts to show the community something that the user made. | Vote and comment on other people&#39;s posts. | . In this project, we will analyze and compare Ask posts and Show posts in order to answer the following questions: . Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post? | Can posting at a certain time of day result in getting more comments? | . This analysis can be helpful for Hacker News users who would like for their posts to reach a larger audience on the platform. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Exploring Hacker News Posts. The research questions and general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import altair as alt import datetime as dt . Dataset . The dataset for this project is the Hacker News Posts dataset on Kaggle, uploaded by Hacker News. . The following is quoted from the dataset&#39;s Description. . This data set is Hacker News posts from the last 12 months (up to September 26 2016). . It includes the following columns: . title: title of the post (self explanatory) | url: the url of the item being linked to | num_points: the number of upvotes the post received | num_comments: the number of comments the post received | author: the name of the account that made the post | created_at: the date and time the post was made (the time zone is Eastern Time in the US) | . Let us view the first 5 rows of the dataset below. . hn = pd.read_csv(&quot;./private/2021-05-11-OHNP-Files/HN_posts_year_to_Sep_26_2016.csv&quot;) hn.head() . id title url num_points num_comments author created_at . 0 12579008 | You have two days to comment if you want stem ... | http://www.regulations.gov/document?D=FDA-2015... | 1 | 0 | altstar | 9/26/2016 3:26 | . 1 12579005 | SQLAR the SQLite Archiver | https://www.sqlite.org/sqlar/doc/trunk/README.md | 1 | 0 | blacksqr | 9/26/2016 3:24 | . 2 12578997 | What if we just printed a flatscreen televisio... | https://medium.com/vanmoof/our-secrets-out-f21... | 1 | 0 | pavel_lishin | 9/26/2016 3:19 | . 3 12578989 | algorithmic music | http://cacm.acm.org/magazines/2011/7/109891-al... | 1 | 0 | poindontcare | 9/26/2016 3:16 | . 4 12578979 | How the Data Vault Enables the Next-Gen Data W... | https://www.talend.com/blog/2016/05/12/talend-... | 1 | 0 | markgainor1 | 9/26/2016 3:14 | . Below is the shape of the dataset. . print(hn.shape) . (293119, 7) . There are 293,119 rows and 7 columns in the dataset. . Before the data can be analyzed, it must first be cleaned. . Data Cleaning . Duplicate Rows . Below, I use pandas to delete duplicate rows except for the first instance of each duplicate. . Rows will be considered as duplicates if they are exactly alike in all features. I decided on this because it is possible for two posts to have the same title and/or url but be posted at different times or by different users. Thus, we cannot identify duplicates based on one or two features alone. . hn = hn.drop_duplicates(keep = &quot;first&quot;) print(hn.shape) . (293119, 7) . No duplicates were found. All rows were kept. . Posts without Comments . Our research questions involve the number of comments on each post. However, there are many posts with 0 comments. . To illustrate this, below a frequency table of the number of comments on each post. . def freq_comments(df = hn): &quot;&quot;&quot;Function to make a frequency table of the number of comments per post specifically for the Hacker News dataset.&quot;&quot;&quot; freq_df = df[&quot;num_comments&quot;].value_counts().reset_index() freq_df.columns = [&quot;num_comments&quot;, &quot;frequency&quot;] freq_df = freq_df.sort_values( by = &quot;num_comments&quot;, ).reset_index( drop = True, ) return freq_df freq_df1 = freq_comments() freq_df1 . num_comments frequency . 0 0 | 212718 | . 1 1 | 28055 | . 2 2 | 9731 | . 3 3 | 5016 | . 4 4 | 3272 | . ... ... | ... | . 543 1007 | 1 | . 544 1120 | 1 | . 545 1448 | 1 | . 546 1733 | 1 | . 547 2531 | 1 | . 548 rows × 2 columns . The table above shows that posts with 0 comments are most frequent. . Let us plot the table on a histogram. . def hist_comments(df, title): &quot;&quot;&quot;Function to make a histogram of the number of comments per post specifically for the Hacker News dataset.&quot;&quot;&quot; chart = alt.Chart(df).mark_bar().encode( x = alt.X( &quot;num_comments:Q&quot;, title = &quot;Number of Comments&quot;, bin = alt.Bin(step = 1) ), y = alt.Y( &quot;frequency:Q&quot;, title = &quot;Frequency&quot;, ), ).properties( title = title, width = 700, height = 400, ) return chart hist_comments(freq_df1, &quot;Histogram of Number of Comments per Post&quot;) . There are so many posts with 0 comments that we cannot see the histogram bins for other numbers of comments. . Considering that the dataset is large and most rows have 0 comments, it would be best to drop all rows with 0 comments. This would make analysis less computationally expensive and allow us to answer our research questions. . with_comments = hn[&quot;num_comments&quot;] &gt; 0 hn = hn.loc[with_comments].reset_index(drop = True) print(hn.shape) . (80401, 7) . Now, the dataset is left with only 80,401 rows. This will be easier to work with. . Below is the new histogram. . freq_df2 = freq_comments() hist_comments(freq_df2, &quot;Histogram of Number of Comments per Post&quot;) . The distribution is still heavily right-skewed since many posts have very few comments. What&#39;s important is that unnecessary data has been removed. . Missing Values . Finally, let us remove rows with missing values. In order to answer our research questions, we only need the following columns: . title | num_comments | created_at | . Thus, we will delete rows with missing values in this column. . hn.dropna( subset = [&quot;title&quot;, &quot;num_comments&quot;, &quot;created_at&quot;], inplace = True, ) print(hn.shape) . (80401, 7) . The number of rows did not change from 80401. Therefore, no missing values were found in these columns, and no rows were dropped. . Data cleaning is now done. . Filtering Posts . As mentioned earlier, the first research question involves comparing Ask posts to Show posts. In order to do this, we have to group the posts into three types: . Ask Posts | Show Posts | Other Posts | . Other posts are usually posts that share a tech article found online. . Ask and Show posts can be identified using the start of the post title. Ask posts start with &quot;Ask HN: &quot;. . ask_mask = [index for index, value in hn[&quot;title&quot;].iteritems() if value.startswith(&quot;Ask HN: &quot;) ] hn.loc[ask_mask].head() . id title url num_points num_comments author created_at . 1 12578908 | Ask HN: What TLD do you use for local developm... | NaN | 4 | 7 | Sevrene | 9/26/2016 2:53 | . 6 12578522 | Ask HN: How do you pass on your work when you ... | NaN | 6 | 3 | PascLeRasc | 9/26/2016 1:17 | . 18 12577870 | Ask HN: Why join a fund when you can be an angel? | NaN | 1 | 3 | anthony_james | 9/25/2016 22:48 | . 27 12577647 | Ask HN: Someone uses stock trading as passive ... | NaN | 5 | 2 | 00taffe | 9/25/2016 21:50 | . 41 12576946 | Ask HN: How hard would it be to make a cheap, ... | NaN | 2 | 1 | hkt | 9/25/2016 19:30 | . On the other hand, Show posts start with &quot;Show HN: &quot;. . show_mask = [index for index, value in hn[&quot;title&quot;].iteritems() if value.startswith(&quot;Show HN: &quot;) ] hn.loc[show_mask].head() . id title url num_points num_comments author created_at . 35 12577142 | Show HN: Jumble Essays on the go #PaulInYourP... | https://itunes.apple.com/us/app/jumble-find-st... | 1 | 1 | ryderj | 9/25/2016 20:06 | . 43 12576813 | Show HN: Learn Japanese Vocab via multiple cho... | http://japanese.vul.io/ | 1 | 1 | soulchild37 | 9/25/2016 19:06 | . 52 12576090 | Show HN: Markov chain Twitter bot. Trained on ... | https://twitter.com/botsonasty | 3 | 1 | keepingscore | 9/25/2016 16:50 | . 68 12575471 | Show HN: Project-Okot: Novel, CODE-FREE data-a... | https://studio.nuchwezi.com/ | 3 | 1 | nfixx | 9/25/2016 14:30 | . 88 12574773 | Show HN: Cursor that Screenshot | http://edward.codes/cursor-that-screenshot | 3 | 3 | ed-bit | 9/25/2016 10:50 | . Other posts do not start with any special label. . Below, I create a new column &quot;post_type&quot; and assign the appropriate value to each row. . hn[&quot;post_type&quot;] = &quot;Other&quot; hn.loc[ask_mask, &quot;post_type&quot;] = &quot;Ask&quot; hn.loc[show_mask, &quot;post_type&quot;] = &quot;Show&quot; hn[[&quot;title&quot;, &quot;post_type&quot;]] . title post_type . 0 Saving the Hassle of Shopping | Other | . 1 Ask HN: What TLD do you use for local developm... | Ask | . 2 Amazons Algorithms Dont Find You the Best Deals | Other | . 3 Emergency dose of epinephrine that does not co... | Other | . 4 Phone Makers Could Cut Off Drivers. So Why Don... | Other | . ... ... | ... | . 80396 My Keyboard | Other | . 80397 Google&#39;s new logo was created by Russian desig... | Other | . 80398 Why we aren&#39;t tempted to use ACLs on our Unix ... | Other | . 80399 Ask HN: What is/are your favorite quote(s)? | Ask | . 80400 Dying vets fuck you letter (2013) | Other | . 80401 rows × 2 columns . Each row has now been labeled as a type of post. . Research Question 1: Comparing Ask and Show Posts . The first research question is, &quot;Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post?&quot; . Note that the data is not normally distributed; it is right-skewed. For example, here is the distribution of the number of comments per Ask post. . ask_freq = freq_comments( df = hn.loc[hn[&quot;post_type&quot;] == &quot;Ask&quot;] ) ask_freq . num_comments frequency . 0 1 | 1383 | . 1 2 | 1238 | . 2 3 | 762 | . 3 4 | 592 | . 4 5 | 373 | . ... ... | ... | . 203 898 | 1 | . 204 910 | 1 | . 205 937 | 1 | . 206 947 | 1 | . 207 1007 | 1 | . 208 rows × 2 columns . hist_comments( ask_freq, &quot;Histogram of Number of Comments per Ask Post&quot; ) . The histogram is similar for Show posts. . show_freq = freq_comments( df = hn.loc[hn[&quot;post_type&quot;] == &quot;Show&quot;], ) show_freq . num_comments frequency . 0 1 | 1738 | . 1 2 | 814 | . 2 3 | 504 | . 3 4 | 300 | . 4 5 | 196 | . ... ... | ... | . 137 250 | 1 | . 138 257 | 1 | . 139 280 | 1 | . 140 298 | 1 | . 141 306 | 1 | . 142 rows × 2 columns . hist_comments( show_freq, &quot;Histogram of Number of Comments per Show Post&quot; ) . Therefore, the mean would not be a good measure of central tendency for the &quot;average number of comments per post.&quot; Thus, we will use the median instead. . dct = {&quot;Ask&quot;: None, &quot;Show&quot;: None} for key in dct: median = np.median( hn[&quot;num_comments&quot;].loc[hn[&quot;post_type&quot;] == key] ) dct[key] = median table = pd.DataFrame.from_dict( dct, orient = &quot;index&quot;, ).reset_index( ).rename(columns = { &quot;index&quot;: &quot;post_type&quot;, 0: &quot;median_comments&quot;, }) chart = alt.Chart(table).mark_bar().encode( y = alt.Y(&quot;post_type:N&quot;, title = &quot;Post Type&quot;), x = alt.X(&quot;median_comments:Q&quot;, title = &quot;Median Number of Comments per Post&quot;), ).properties( title = &quot;Median Number of Comments for the Two Post Types&quot;, ) chart . The bar graph shows that Ask posts have a higher median number of comments per post, compared to Show posts. . Important: The results suggest that Ask posts get more comments than Show posts. It may be easier for users to reach a larger audience via Ask posts. . Research Question 2: Active Times . The second research question is, &quot;Can posting at a certain time of day result in getting more comments?&quot; . For this part of the analysis, we will only be using Ask post data for simplicity. . We will divide the day into 24 one-hour periods, and then calculate the number of Ask posts created in each period. . String Template for Time . Before analying, we need to inspect the &quot;created_at&quot; column of the dataset. . hn_ask = hn.loc[ hn[&quot;post_type&quot;] == &quot;Ask&quot; ].reset_index( drop = True, ) hn_ask[[&quot;created_at&quot;]].head() . created_at . 0 9/26/2016 2:53 | . 1 9/26/2016 1:17 | . 2 9/25/2016 22:48 | . 3 9/25/2016 21:50 | . 4 9/25/2016 19:30 | . The strings in this column appear to follow the following format: . month/day/year hour:minute . With the datetime module, the following is the equivalent formatting template. . template = &quot;%m/%d/%Y %H:%M&quot; . Parsing Times . The time data can now be parsed and used for analysis. . hn_ask[&quot;created_at&quot;] = pd.to_datetime( hn_ask[&quot;created_at&quot;], format = template, ) hn_ask[&quot;created_at&quot;].head() . 0 2016-09-26 02:53:00 1 2016-09-26 01:17:00 2 2016-09-25 22:48:00 3 2016-09-25 21:50:00 4 2016-09-25 19:30:00 Name: created_at, dtype: datetime64[ns] . The column is now in datetime format. . With this data, we will make 2 dictionaries. The hours_posts dictionary will count the number of posts at certain hours. The hours_comments dictionary will count the number of comments received by posts made at certain hours. . hours_posts = {} hours_comments = {} for index, row in hn_ask.iterrows(): date_dt = row[&quot;created_at&quot;] num_comments = row[&quot;num_comments&quot;] # extract hour hour = date_dt.hour # update dictionaries hours_posts.setdefault(hour, 0) hours_posts[hour] += 1 hours_comments.setdefault(hour, 0) hours_comments[hour] += num_comments . The hours were parsed and mapped to their respective counts of posts and comments. . The code below transforms the dictionaries into DataFrames for ease of use. . def hour_to_df(dct, data_label): &quot;&quot;&quot;Make a DataFrame from a dictionary that maps an &#39;hour&#39; column to another column, named by `data_label`.&quot;&quot;&quot; result = pd.DataFrame.from_dict( dct, orient = &quot;index&quot;, ).reset_index( ).rename(columns = { &quot;index&quot;: &quot;hour&quot;, 0: data_label, }).sort_values( by = &quot;hour&quot;, ).reset_index( drop = True, ) return result hours_posts_df = hour_to_df( hours_posts, data_label = &quot;num_posts&quot;, ) hours_comments_df = hour_to_df( hours_comments, data_label = &quot;num_comments&quot;, ) hours_posts_df.head() . hour num_posts . 0 0 | 228 | . 1 1 | 222 | . 2 2 | 227 | . 3 3 | 210 | . 4 4 | 184 | . hours_comments_df.head() . hour num_comments . 0 0 | 2261 | . 1 1 | 2068 | . 2 2 | 2996 | . 3 3 | 2152 | . 4 4 | 2353 | . The hours have been parsed, and the tables have been generated. . Additionally, another DataFrame is created below. It calculates the median number of comments per post by the hour posted. . hn_ask[&quot;hour&quot;] = hn_ask[&quot;created_at&quot;].dt.hour hours_median = hn_ask.pivot_table( index = &quot;hour&quot;, values = &quot;num_comments&quot;, aggfunc = np.median, ).reset_index() hours_median . hour num_comments . 0 0 | 3.0 | . 1 1 | 3.0 | . 2 2 | 4.0 | . 3 3 | 3.0 | . 4 4 | 4.0 | . 5 5 | 3.0 | . 6 6 | 3.0 | . 7 7 | 4.0 | . 8 8 | 3.5 | . 9 9 | 3.0 | . 10 10 | 4.0 | . 11 11 | 4.0 | . 12 12 | 4.0 | . 13 13 | 4.0 | . 14 14 | 3.0 | . 15 15 | 4.0 | . 16 16 | 3.0 | . 17 17 | 4.0 | . 18 18 | 3.0 | . 19 19 | 3.0 | . 20 20 | 4.0 | . 21 21 | 3.0 | . 22 22 | 4.0 | . 23 23 | 4.0 | . Number of Posts by Hour of the Day . Below is a table showing the total number of posts that were created, grouped by hour of the day. . hours_posts_df . hour num_posts . 0 0 | 228 | . 1 1 | 222 | . 2 2 | 227 | . 3 3 | 210 | . 4 4 | 184 | . 5 5 | 165 | . 6 6 | 176 | . 7 7 | 156 | . 8 8 | 190 | . 9 9 | 176 | . 10 10 | 218 | . 11 11 | 250 | . 12 12 | 272 | . 13 13 | 323 | . 14 14 | 377 | . 15 15 | 467 | . 16 16 | 412 | . 17 17 | 402 | . 18 18 | 450 | . 19 19 | 418 | . 20 20 | 392 | . 21 21 | 407 | . 22 22 | 286 | . 23 23 | 276 | . This table is in 24-hour time. Hour 13 refers to 1:00 PM. The table shows how many posts are made for every hour in the day. . Below is a line chart that shows this visually. . chart = alt.Chart(hours_posts_df).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;num_posts:Q&quot;, title = &quot;Number of Posts&quot;), ).properties( title = &quot;Number of Posts by Hour of the Day&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . The histogram clearly shows that Hacker News users most actively make posts between 15:00 and 18:00, or from 3:00 PM to 6:00 PM. . The most active hour for posting is 3:00 PM - 4:00 PM. . Number of Comments by Hour Posted . Next, a similar analysis is done for the total number of comments written by Hacker News users, grouped by the hour that the original posts were created. Below is the table for this data. . hours_comments_df . hour num_comments . 0 0 | 2261 | . 1 1 | 2068 | . 2 2 | 2996 | . 3 3 | 2152 | . 4 4 | 2353 | . 5 5 | 1838 | . 6 6 | 1587 | . 7 7 | 1584 | . 8 8 | 2362 | . 9 9 | 1477 | . 10 10 | 3011 | . 11 11 | 2794 | . 12 12 | 4226 | . 13 13 | 7219 | . 14 14 | 4970 | . 15 15 | 18525 | . 16 16 | 4458 | . 17 17 | 5536 | . 18 18 | 4824 | . 19 19 | 3949 | . 20 20 | 4462 | . 21 21 | 4500 | . 22 22 | 3369 | . 23 23 | 2297 | . Below is the line chart that visualizes the table. . chart = alt.Chart(hours_comments_df).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;num_comments:Q&quot;, title = &quot;Number of Comments&quot;), ).properties( title = &quot;Number of Comments by Hour Posted&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . The line chart shows that many comments are made on posts created from 13:00 to 17:00, or 1:00 PM - 5:00 PM. . Notably, there was a total of over 18,000 comments on posts created at 3:00 PM. This may suggest that Hacker News users most actively comment at around this time. . However, there is also the possibility that this total was influenced by a few outliers. Let&#39;s check the distribution of the number of comments made at 3:00 PM. . alt.Chart( hn_ask.loc[hn_ask[&quot;hour&quot;] == 15] ).mark_bar().encode( x = alt.X(&quot;num_comments:Q&quot;), y = alt.Y(&quot;count()&quot;) ).properties( title = &quot;Distribution of Number of Comments per Post (3:00 PM)&quot;, width = 700, height = 200, ).configure_axis( grid = True, ) . Indeed, there are several outlier posts with a very high number of comments, going up to 1000. These influenced the spike in the line chart. . Therefore, we can&#39;t say that a post will definitely get many comments if it is posted at 3:00 PM. However, we can say that generally a lot of comments are made in the afternoon from 1:00 PM to 5:00 PM. . Median Number of Comments per Post, by Hour Posted . The table below shows the median number of comments per post, by the hour of posting. . hours_median . hour num_comments . 0 0 | 3.0 | . 1 1 | 3.0 | . 2 2 | 4.0 | . 3 3 | 3.0 | . 4 4 | 4.0 | . 5 5 | 3.0 | . 6 6 | 3.0 | . 7 7 | 4.0 | . 8 8 | 3.5 | . 9 9 | 3.0 | . 10 10 | 4.0 | . 11 11 | 4.0 | . 12 12 | 4.0 | . 13 13 | 4.0 | . 14 14 | 3.0 | . 15 15 | 4.0 | . 16 16 | 3.0 | . 17 17 | 4.0 | . 18 18 | 3.0 | . 19 19 | 3.0 | . 20 20 | 4.0 | . 21 21 | 3.0 | . 22 22 | 4.0 | . 23 23 | 4.0 | . This is visualized in the line chart below, which looks quite different from the previous two charts. . chart = alt.Chart(hours_median).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;num_comments:Q&quot;, title = &quot;Median Number of Comments per Post&quot;), ).properties( title = &quot;Median Number of Comments per Post, by Hour Posted&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . This graph shows that the median number of comments per post is very consistent throughout the day. It ranges from 3 comments to 4 comments. . This brings us a new question. We&#39;ve seen that Hacker News users most actively post and comment in the afternoon. So, why does the median number of comments per post not increase in the afternoon? . A possible explanation is that since the site is oversaturated with new posts in the afternoon, only the very best posts receive attention. The rest are lost in the flood of new posts. . Conclusion . In this project, we analyzed data about Hacker News posts, specifically regarding the number of comments that they receive. Below are the research questions, and the best answers that we could come up with from our analysis. . . Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post? | . Ask posts receive a higher median number of comments per post, compared to Show posts. In order to reach a wider audience or converse with more users, it is better to make an Ask post. . . Can posting at a certain time of day result in getting more comments? | . Hacker News users are very active in the afternoon, from 1:00 PM to 6:00 PM. However, if you post in the afternoon, your post may get lost in the flood of new posts. Hypothetically, it may be better to post in the morning, like at 7:00 AM, so that some people can notice your post. Then, your post can get more attention in the afternoon. . . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/altair/datetime/2021/05/11/Optimizing-Hacker-News-Posts.html",
            "relUrl": "/python/pandas/numpy/altair/datetime/2021/05/11/Optimizing-Hacker-News-Posts.html",
            "date": " • May 11, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Profitable App Profiles for iOS and Android",
            "content": ". Unsplash | Alexander Shatov Overview . Welcome. In this project, we will be working on data about different types of apps and their corresponding number of users. The goal is to determine which apps can best attract the largest number of users. This will help a hypothetical app company make decisions regarding what apps to develop in the near future. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Profitable App Profiles for the App Store and Google Play Markets. The hypothetical app company and the general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . App Company&#39;s Context . First, we must know the context of the hypothetical app company so that we can align our analysis with it. . This company only makes free apps directed toward an English-speaking audience. They get revenue from in-app advertisements and purchases. Thus, they rely on having a large number of users so that they can generate more revenue. . Additionally, their apps should ideally be successful on both Google Play Store and the Apple App Store. The reason is that the company has the following 3-step validation strategy: . (from the Dataquest guided project) . Build a minimal Android version of the app, and add it to Google Play. | If the app has a good response from users, we develop it further. | If the app is profitable after six months, we build an iOS version of the app and add it to the App Store. | We&#39;ll take this information into consideration throughout our analysis. . Package Installs . import pandas as pd import numpy as np import altair as alt import re . App Data Overview . This project uses two datasets. . The Google Play Store dataset lists over 10,000 Android apps. | The Apple App Store dataset lists over 7,000 iOS apps. | . data_apple = pd.read_csv(&quot;./private/2021-05-08-PAP-Files/AppleStore.csv&quot;, header = 0) data_google = pd.read_csv(&quot;./private/2021-05-08-PAP-Files/googleplaystore.csv&quot;, header = 0) . Apple App Store dataset . print(data_apple.shape) . (7197, 16) . The dataset has 7197 rows (1 row per app), and 16 columns which describe these apps. . According to the Kaggle documentation (Mobile App Store ( 7200 apps)), the following are the columns and their meanings. . &quot;id&quot; : App ID | &quot;track_name&quot;: App Name | &quot;size_bytes&quot;: Size (in Bytes) | &quot;currency&quot;: Currency Type | &quot;price&quot;: Price amount | &quot;ratingcounttot&quot;: User Rating counts (for all version) | &quot;ratingcountver&quot;: User Rating counts (for current version) | &quot;user_rating&quot; : Average User Rating value (for all version) | &quot;userratingver&quot;: Average User Rating value (for current version) | &quot;ver&quot; : Latest version code | &quot;cont_rating&quot;: Content Rating | &quot;prime_genre&quot;: Primary Genre | &quot;sup_devices.num&quot;: Number of supporting devices | &quot;ipadSc_urls.num&quot;: Number of screenshots showed for display | &quot;lang.num&quot;: Number of supported languages | &quot;vpp_lic&quot;: Vpp Device Based Licensing Enabled | . A sample of the first 5 rows of the dataset is shown below. . data_apple.head() . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 0 284882215 | Facebook | 389879808 | USD | 0.0 | 2974676 | 212 | 3.5 | 3.5 | 95.0 | 4+ | Social Networking | 37 | 1 | 29 | 1 | . 1 389801252 | Instagram | 113954816 | USD | 0.0 | 2161558 | 1289 | 4.5 | 4.0 | 10.23 | 12+ | Photo &amp; Video | 37 | 0 | 29 | 1 | . 2 529479190 | Clash of Clans | 116476928 | USD | 0.0 | 2130805 | 579 | 4.5 | 4.5 | 9.24.12 | 9+ | Games | 38 | 5 | 18 | 1 | . 3 420009108 | Temple Run | 65921024 | USD | 0.0 | 1724546 | 3842 | 4.5 | 4.0 | 1.6.2 | 9+ | Games | 40 | 5 | 1 | 1 | . 4 284035177 | Pandora - Music &amp; Radio | 130242560 | USD | 0.0 | 1126879 | 3594 | 4.0 | 4.5 | 8.4.1 | 12+ | Music | 37 | 4 | 1 | 1 | . Google Play Store dataset . print(data_google.shape) . (10841, 13) . The dataset has 10841 rows and 13 columns. . The column names are self-explanatory, so the Kaggle documentation (Google Play Store Apps) does not describe them. . print(list(data_google.columns)) . [&#39;App&#39;, &#39;Category&#39;, &#39;Rating&#39;, &#39;Reviews&#39;, &#39;Size&#39;, &#39;Installs&#39;, &#39;Type&#39;, &#39;Price&#39;, &#39;Content Rating&#39;, &#39;Genres&#39;, &#39;Last Updated&#39;, &#39;Current Ver&#39;, &#39;Android Ver&#39;] . Below is a sample of the dataset. . data_google.head() . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 1 Coloring book moana | ART_AND_DESIGN | 3.9 | 967 | 14M | 500,000+ | Free | 0 | Everyone | Art &amp; Design;Pretend Play | January 15, 2018 | 2.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . Data Cleaning . Before analysis, the data must first be cleaned of unwanted datapoints. . Inaccurate Data . This Kaggle discussion about the Google Play dataset indicates that row 10472 (excluding the header) has an error. . Below, I have printed row 0 and row 10472 so that these can be compared. . data_google.iloc[[0, 10472]] . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 10472 Life Made WI-Fi Touchscreen Photo Frame | 1.9 | 19.0 | 3.0M | 1,000+ | Free | 0 | Everyone | NaN | February 11, 2018 | 1.0.19 | 4.0 and up | NaN | . As we look at row 10472 in the context of the column headers and row 0, the following things become clear. . The &quot;Category&quot; value is not present. Thus, all values to the right of it have been shifted leftward. | The &quot;Android Ver&quot; column was left with a missing value. | . Thus, this row will be removed. . if data_google.iloc[10472, 0] == &#39;Life Made WI-Fi Touchscreen Photo Frame&#39;: # This if-statement prevents more rows from being deleted # if the cell is run again. data_google.drop(10472, inplace = True) print(&quot;The inaccurate row was deleted.&quot;) . The inaccurate row was deleted. . Duplicate Data . There are also duplicate app entries in the Google Play dataset. We can consider a row as a duplicates if another row exists that has the same &quot;App&quot; value. . Here, I count the total number of duplicate rows. This turns out to be 1979 rows. . def count_duplicates(df, col_name): &quot;&quot;&quot;Count the number of duplicate rows in a DataFrame. `col_name` is the name of the column to be used as a basis for duplicate values.&quot;&quot;&quot; all_apps = {} for index, row in df.iterrows(): name = row[col_name] all_apps.setdefault(name, []).append(index) duplicate_inds = [ind for lst in all_apps.values() for ind in lst if len(lst) &gt; 1] n_duplicates = &quot;Duplicates: {}&quot;.format(len(duplicate_inds)) duplicate_rows = df.iloc[duplicate_inds] return n_duplicates, duplicate_rows google_dupes = count_duplicates(data_google, &quot;App&quot;) print(google_dupes[0]) . Duplicates: 1979 . As an example, there are 4 rows for Instagram: . ig_filter = data_google[&quot;App&quot;] == &quot;Instagram&quot; ig_rows = data_google.loc[ig_filter] . ig_rows . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 2545 Instagram | SOCIAL | 4.5 | 66577313 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 2604 Instagram | SOCIAL | 4.5 | 66577446 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 2611 Instagram | SOCIAL | 4.5 | 66577313 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 3909 Instagram | SOCIAL | 4.5 | 66509917 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . Looking closely, we can see that duplicate rows are not exactly identical. The &quot;Reviews&quot; column, which shows the total number of reviews of the app, has different values. . It can be inferred that the row with the largest value is the newest entry for the app. Therefore, all duplicate rows will be dropped except for the ones with the largest &quot;Reviews&quot; values. . def remove_duplicates(df, name_col, reviews_col): # Each key-value pair will follow the format: # {&quot;App Name&quot;: maximum number of reviews among all duplicates} reviews_max = {} for index, row in df.iterrows(): name = row[name_col] n_reviews = int(row[reviews_col]) if n_reviews &gt; reviews_max.get(name, -1): reviews_max[name] = n_reviews # List of duplicate indices to drop, # excluding the row with the highest number of reviews # among that app&#39;s duplicate rows. indices_to_drop = [] # Rows with names that have already been added into this list # will be dropped. already_added = [] for index, row in df.iterrows(): name = row[name_col] n_reviews = int(row[reviews_col]) if (name not in already_added) and (n_reviews == reviews_max[name]): already_added.append(name) else: indices_to_drop.append(index) # Remove duplicates and return the clean dataset. clean = df.drop(indices_to_drop) return clean android_clean = remove_duplicates(data_google, &quot;App&quot;, &quot;Reviews&quot;) print(android_clean.shape) . (9659, 13) . After duplicates were removed, the Google Play dataset was left with 9659 rows. . As for the Apple App Store dataset, there are 4 duplicate rows. . apple_dupes = count_duplicates(data_apple, &quot;track_name&quot;) print(apple_dupes[0]) apple_dupes[1] . Duplicates: 4 . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 2948 1173990889 | Mannequin Challenge | 109705216 | USD | 0.0 | 668 | 87 | 3.0 | 3.0 | 1.4 | 9+ | Games | 37 | 4 | 1 | 1 | . 4463 1178454060 | Mannequin Challenge | 59572224 | USD | 0.0 | 105 | 58 | 4.0 | 4.5 | 1.0.1 | 4+ | Games | 38 | 5 | 1 | 1 | . 4442 952877179 | VR Roller Coaster | 169523200 | USD | 0.0 | 107 | 102 | 3.5 | 3.5 | 2.0.0 | 4+ | Games | 37 | 5 | 1 | 1 | . 4831 1089824278 | VR Roller Coaster | 240964608 | USD | 0.0 | 67 | 44 | 3.5 | 4.0 | 0.81 | 4+ | Games | 38 | 0 | 1 | 1 | . The &quot;rating_count_tot&quot; column in the Apple App Store dataset is like the &quot;Reviews&quot; column in the Google Play dataset. It tells the total number of reviews so far. Therefore, Apple App Store dataset duplicates can be removed by keeping the rows with the highest rating count totals. . ios_clean = remove_duplicates(data_apple, &quot;track_name&quot;, &quot;rating_count_tot&quot;) print(ios_clean.shape) . (7195, 16) . From 7197 rows, there are now 7195 rows in the Apple App Store dataset. . Non-English Apps . The hypothetical app company who will use this analysis is a company that only makes apps in English. Thus, all apps with non-English titles shall be removed from the datasets. . The task now is to identify titles which are not in English. It is known that in the ASCII table, the characters most commonly used in English are within codes 0 to 127. Some English app titles may have special characters or emojis, though, so I will only remove titles which have more than 3 characters outside of the normal range. . def is_english(text): unicode = [ord(char) for char in text] normal = [(code &gt;= 0 and code &lt;= 127) for code in unicode] non_english = len(text) - sum(normal) return non_english &lt;= 3 def keep_english(df, name_col): &quot;&quot;&quot;Return a new DataFrame containing only rows with English names.&quot;&quot;&quot; remove_indices = [] for index, row in df.iterrows(): name = row[name_col] if not is_english(name): remove_indices.append(index) return df.drop(remove_indices) android_clean = keep_english(android_clean, &quot;App&quot;) ios_clean = keep_english(ios_clean, &quot;track_name&quot;) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (9614, 13) Apple App Store Dataset: (6181, 16) . Now, there are only English apps in both datasets. . Paid Apps . As mentioned earlier, the app company only makes free apps. Therefore, data on paid apps is irrelevant to this analysis. Paid apps shall be identified and removed from both datasets. . def remove_paid(df, price_col): &quot;&quot;&quot;Return a new DataFrame without paid apps.&quot;&quot;&quot; remove_indices = [] for index, row in df.iterrows(): price = str(row[price_col]) # Keep characters that are numeric or periods. price = float(re.sub(&quot;[^0-9.]&quot;, &quot;&quot;, price)) if price != 0.0: remove_indices.append(index) return df.drop(remove_indices) android_clean = remove_paid(android_clean, &quot;Price&quot;) ios_clean = remove_paid(ios_clean, &quot;price&quot;) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (8864, 13) Apple App Store Dataset: (3220, 16) . The datasets were left with 8864 apps in Google Play and 3220 apps in the App Store. . Missing Data . Lastly, let us remove rows with missing data. Note that it would be wasteful to remove rows with missing data in columns that we will not inspect. Therefore, we will only remove rows with missing data in relevant columns. (Why these are relevant will be explained later.) These would be the following. . Google Play Store dataset . App | Category | Installs | Genres | . Apple App Store dataset . track_name | prime_genre | rating_count_tot | . I will now remove all rows with missing values in these columns. . android_clean.dropna( subset = [&quot;App&quot;, &quot;Category&quot;, &quot;Installs&quot;, &quot;Genres&quot;], inplace = True, ) ios_clean.dropna( subset = [&quot;track_name&quot;, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;], inplace = True, ) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (8864, 13) Apple App Store Dataset: (3220, 16) . These are the same shapes as before. Therefore, there were no missing values in the relevant columns. No datapoints were removed at this step. . Data cleaning is done, so now we can move on to the analysis. . Common App Genres . Now that the data has been cleaned, let us find out which genres of apps are most common in both app markets. If an app genre is common, then there may be high demand for it among users. . Which columns in the datasets can give information about the app genres? . print(&quot;Google Play Store&quot;) android_clean.head() . Google Play Store . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . 5 Paper flowers instructions | ART_AND_DESIGN | 4.4 | 167 | 5.6M | 50,000+ | Free | 0 | Everyone | Art &amp; Design | March 26, 2017 | 1.0 | 2.3 and up | . print(&quot; nApple App Store&quot;) ios_clean.head() . Apple App Store . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 0 284882215 | Facebook | 389879808 | USD | 0.0 | 2974676 | 212 | 3.5 | 3.5 | 95.0 | 4+ | Social Networking | 37 | 1 | 29 | 1 | . 1 389801252 | Instagram | 113954816 | USD | 0.0 | 2161558 | 1289 | 4.5 | 4.0 | 10.23 | 12+ | Photo &amp; Video | 37 | 0 | 29 | 1 | . 2 529479190 | Clash of Clans | 116476928 | USD | 0.0 | 2130805 | 579 | 4.5 | 4.5 | 9.24.12 | 9+ | Games | 38 | 5 | 18 | 1 | . 3 420009108 | Temple Run | 65921024 | USD | 0.0 | 1724546 | 3842 | 4.5 | 4.0 | 1.6.2 | 9+ | Games | 40 | 5 | 1 | 1 | . 4 284035177 | Pandora - Music &amp; Radio | 130242560 | USD | 0.0 | 1126879 | 3594 | 4.0 | 4.5 | 8.4.1 | 12+ | Music | 37 | 4 | 1 | 1 | . For Google Play, some columns that seem relevant are &quot;Category&quot; and &quot;Genres&quot;. For the Apple App Store, the relevant column is &quot;prime_genre&quot;. . We can determine the most common genres by using frequency tables of the mentioned columns. . def freq_table(df, label): &quot;&quot;&quot;Return a frequency table of the values in a column of a DataFrame.&quot;&quot;&quot; col = df[label] freq = {} for value in col: freq.setdefault(value, 0) freq[value] += 1 for key in freq: freq[key] /= len(df) / 100 freq_series = pd.Series(freq).sort_values(ascending = False) return freq_series def sr_to_df(sr, col_name = &quot;number&quot;, n_head = None): &quot;&quot;&quot;Return a DataFrame by resetting the index of a Series.&quot;&quot;&quot; df = sr.rename(col_name).reset_index().rename(columns = {&quot;index&quot;:&quot;name&quot;}) if n_head is not None: df = df.head(n_head) return df google_categories = freq_table(android_clean, &quot;Category&quot;) google_genres = freq_table(android_clean, &quot;Genres&quot;) apple_genres = freq_table(ios_clean, &quot;prime_genre&quot;) . The frequency tables will be inspected in the sections below. Only the top positions in each table will be shown, for brevity. . Apple App Store: Prime Genres . First, the frequency table of Apple App Store prime genres shall be analyzed. Below, I have ordered the table by frequency, descending. I have also made bar graphs showing the top 10 positions in each frequency table. . sr_to_df(apple_genres, &quot;percentage&quot;, n_head = 5) . name percentage . 0 Games | 58.136646 | . 1 Entertainment | 7.888199 | . 2 Photo &amp; Video | 4.968944 | . 3 Education | 3.664596 | . 4 Social Networking | 3.291925 | . def bar_n(series, chart_title, ylabel, n = 10, perc = False): &quot;&quot;&quot;Takes a series and outputs a bar graph of the first n items.&quot;&quot;&quot; series.index.name = &quot;name&quot; df = series.rename(&quot;number&quot;).reset_index() df[&quot;number&quot;] = [round(i, 2) for i in df[&quot;number&quot;]] df = df[:n] bar = alt.Chart(df).mark_bar().encode( x = alt.X(&quot;name&quot;, title = &quot;Name&quot;, sort = &quot;-y&quot;), y = alt.Y(&quot;number&quot;, title = ylabel), ) text = bar.mark_text( align = &#39;center&#39;, baseline = &#39;middle&#39;, dy = -5, # Nudge text upward ).encode( text = &#39;number:Q&#39; ) chart = (bar + text).properties( title = chart_title, width = 700, height = 400, ) return chart bar_n( apple_genres, &quot;Top 10 Most Common Prime Genres of iOS Apps&quot;, &quot;Percentage of Apps&quot;, perc = True, ) . The top 5 most common prime genres in the Apple App Store are Games, Entertainment, Photo &amp; Video, Education, and Social Networking. Games are at the top, occupying over 58% of all apps. This is a much higher percentage than any other single genre occupies. . Important: The general impression is that there are many more iOS apps that are entertainment-related apps compared to practical apps. . Google Play Store: Categories . Next, below is the frequency table for Google Play Store app categories. . sr_to_df(google_categories, &quot;percentage&quot;, 5) . name percentage . 0 FAMILY | 18.907942 | . 1 GAME | 9.724729 | . 2 TOOLS | 8.461191 | . 3 BUSINESS | 4.591606 | . 4 LIFESTYLE | 3.903430 | . bar_n( google_categories, &quot;Top 10 Most Common Categories of Android Apps&quot;, &quot;Percentage of Apps&quot;, perc = True, ) . The picture here seems to be different. The most common category is Family occupying almost 19% of all apps, followed by Game, Tools, Business, and Lifestyle. . Important: The table suggests that practical app categories are more common in Google Play than in the Apple App Store. . Google Play Store: Genres . Lastly, below is the frequency table for Google Play Store app genres. . sr_to_df(google_genres, &quot;percentage&quot;, 5) . name percentage . 0 Tools | 8.449910 | . 1 Entertainment | 6.069495 | . 2 Education | 5.347473 | . 3 Business | 4.591606 | . 4 Productivity | 3.892148 | . bar_n( google_genres, &quot;Top 10 Most Common Genres of Android Apps&quot;, &quot;Percentage of Apps&quot;, ) . There are 114 genres in this table, so it is not fully displayed. However, it would appear that the top 5 genres are Tools (8%), Entertainment, Education, Business, and Lifestyle. Like with the categories, practical apps are very common. . However, I noticed something special about this frequency table. Some genres are actually combinations of multiple genres, separated by semi-colons. If I can extract and count individual genres from these combined genres, then I can get a more accurate idea of app genres in the Google Play Store. . Note: This frequency table will show numbers instead of percentages. Since the genres overlap, the percentages would add up to greater than 100%. . freq = {} for value in android_clean[&quot;Genres&quot;]: genres = value.split(&quot;;&quot;) for genre in genres: freq.setdefault(genre, 0) freq[genre] += 1 google_genres_split = pd.Series(freq).sort_values(ascending = False) sr_to_df(google_genres_split, n_head = 5) . name number . 0 Tools | 750 | . 1 Education | 606 | . 2 Entertainment | 569 | . 3 Business | 407 | . 4 Lifestyle | 347 | . bar_n( google_genres_split, &quot;Top 10 Most Common Genres of Android Apps (Split Up)&quot;, &quot;Number of Apps&quot;, ) . It can be seen that the frequency table has slightly different placements now. However, the top genres are still Tools, Education, Entertainment, Business, and Lifestyle. Practical app genres are very common in the Google Play Store. They are more common here than in the Apple App Store. . Important: Based on the results, the Google Play Store has a selection of apps that is more balanced between entertainment and practicality. . . Going back to the the frequency table of Categories, since it seems that each Category represents a group of Genres. For example, one would expect apps in the Simulation, Arcade, Puzzle, Strategy, etc. genres to be under the Game category. It was shown earlier that this category is the 2nd most common in the Google Play Store. . The Categories column is more general and gives a more accurate picture of the common types of apps. Thus, from here on, I will be analyzing only the &quot;Category&quot; column and not the &quot;Genres&quot; column. . Note: I will now use &quot;app type&quot; to generally refer to the Apple App Store&#8217;s &quot;prime_genre&quot; values or the Google Play Store&#8217;s &quot;Category&quot; values. . App Types by Number of Users . We first looked at app types in terms of how common they are in the two app markets. Now, we shall see how many users there are for each app type. . Apple App Store: Rating Counts . In the Apple App Store dataset, there is no column that indicates the number of users. . print(list(ios_clean.columns)) . [&#39;id&#39;, &#39;track_name&#39;, &#39;size_bytes&#39;, &#39;currency&#39;, &#39;price&#39;, &#39;rating_count_tot&#39;, &#39;rating_count_ver&#39;, &#39;user_rating&#39;, &#39;user_rating_ver&#39;, &#39;ver&#39;, &#39;cont_rating&#39;, &#39;prime_genre&#39;, &#39;sup_devices.num&#39;, &#39;ipadSc_urls.num&#39;, &#39;lang.num&#39;, &#39;vpp_lic&#39;] . However, the &quot;rating_count_tot&quot; column exists. It indicates the total number of ratings given to each app. We can use it as a proxy for the number of users of each app. . The function below will return a Series showing the average number of users per app within each type. (Not the total number of users per type.) . def users_by_type(df, type_col, users_col, moct = &quot;mean&quot;): &quot;&quot;&quot;Return a Series that maps each app type to the average number of users per app for that type. Specify &#39;mean&#39; or &#39;median&#39; for the measure of central tendency.&quot;&quot;&quot; dct = {} for index, row in df.iterrows(): app_type = row[type_col] users = row[users_col] dct.setdefault(app_type, []).append(users) dct2 = {} for app_type in dct: counts = dct[app_type] if moct == &quot;mean&quot;: dct2[app_type] = np.mean(counts) elif moct == &quot;median&quot;: dct2[app_type] = np.median(counts) result = pd.Series(dct2).sort_values(ascending = False) return result ios_users = users_by_type(ios_clean, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;) sr_to_df(ios_users, n_head = 5) . name number . 0 Navigation | 86090.333333 | . 1 Reference | 74942.111111 | . 2 Social Networking | 71548.349057 | . 3 Music | 57326.530303 | . 4 Weather | 52279.892857 | . bar_n( ios_users, &quot;Top 10 Most Popular iOS App Types&quot;, &quot;Mean Number of Users per App&quot;, ) . The top 5 iOS app types with the highest mean average number of users per app are Navigation, Reference, Social Networking, Music, and Weather. . However, these mean averages may be skewed by a few particularly popular apps. For example, let us look at the number of users of the top 5 Navigation apps. . ios_nav = ios_clean[[ &quot;track_name&quot;, &quot;rating_count_tot&quot;, ]].loc[ ios_clean[&quot;prime_genre&quot;] == &quot;Navigation&quot; ].sort_values( by = &quot;rating_count_tot&quot;, ascending = False, ).set_index( &quot;track_name&quot;, ) # `ios_nav` is still a DataFrame at this point. # It becomes a Series below. ios_nav = ios_nav[&quot;rating_count_tot&quot;] sr_to_df(ios_nav, n_head = 5) . track_name number . 0 Waze - GPS Navigation, Maps &amp; Real-time Traffic | 345046 | . 1 Google Maps - Navigation &amp; Transit | 154911 | . 2 Geocaching® | 12811 | . 3 CoPilot GPS – Car Navigation &amp; Offline Maps | 3582 | . 4 ImmobilienScout24: Real Estate Search in Germany | 187 | . bar_n( ios_nav, &quot;iOS Navigation Apps by Popularity&quot;, &quot;Number of Users&quot;, ) . Clearly, the distribution is skewed because Waze has such a high number of users. Therefore, a better measure of central tendency to use would be the median, not the mean. . Let us repeat the analysis using the median this time: . ios_users = users_by_type( ios_clean, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;, moct = &quot;median&quot;, ) sr_to_df(ios_users, n_head = 5) . name number . 0 Productivity | 8737.5 | . 1 Navigation | 8196.5 | . 2 Reference | 6614.0 | . 3 Shopping | 5936.0 | . 4 Social Networking | 4199.0 | . bar_n( ios_users, &quot;Top 10 Most Popular iOS App Types&quot;, &quot;Median Number of Users per App&quot;, ) . The top 5 most popular iOS apps by median number of users per app are: . Productivity | Navigation | Reference | Shopping | Social Networking | . These placements are quite different from the top 5 most common iOS apps (Games, Entertainment, Photo &amp; Video, Education, and Social Networking). . . . Important: We can say the following about the Apple App Store. . Apps for entertainment and fun, notably Games, are the most common apps. | Apps for practical purposes, notably Productivity, are the most popular apps. | . Google Play Store: Installs . Let us see which columns in the Google Play Store dataset can tell us about the number of users per app. . android_clean.head() . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . 5 Paper flowers instructions | ART_AND_DESIGN | 4.4 | 167 | 5.6M | 50,000+ | Free | 0 | Everyone | Art &amp; Design | March 26, 2017 | 1.0 | 2.3 and up | . The &quot;Installs&quot; column seems like the best indicator of the number of users. . android_clean[[&quot;App&quot;, &quot;Installs&quot;]] . App Installs . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | 10,000+ | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | 5,000,000+ | . 3 Sketch - Draw &amp; Paint | 50,000,000+ | . 4 Pixel Draw - Number Art Coloring Book | 100,000+ | . 5 Paper flowers instructions | 50,000+ | . ... ... | ... | . 10836 Sya9a Maroc - FR | 5,000+ | . 10837 Fr. Mike Schmitz Audio Teachings | 100+ | . 10838 Parkinson Exercices FR | 1,000+ | . 10839 The SCP Foundation DB fr nn5n | 1,000+ | . 10840 iHoroscope - 2018 Daily Horoscope &amp; Astrology | 10,000,000+ | . 8864 rows × 2 columns . The column contains strings which indicate the general range of how many users installed the apps. Since we cannot find the exact number of installs, we will simply remove the &quot;+&quot; signs and convert the numbers into integers. . android_clean[&quot;Installs&quot;] = [int(re.sub(&quot;[,+]&quot;, &quot;&quot;, text)) for text in android_clean[&quot;Installs&quot;]] android_clean[[&quot;Installs&quot;]] . Installs . 0 10000 | . 2 5000000 | . 3 50000000 | . 4 100000 | . 5 50000 | . ... ... | . 10836 5000 | . 10837 100 | . 10838 1000 | . 10839 1000 | . 10840 10000000 | . 8864 rows × 1 columns . Let us now see which app categories are most popular. We will use the median average here, as we did for iOS apps. . android_users = users_by_type( android_clean, &quot;Category&quot;, &quot;Installs&quot;, moct = &quot;median&quot;, ) sr_to_df(android_users, n_head = 10) . name number . 0 ENTERTAINMENT | 1000000.0 | . 1 EDUCATION | 1000000.0 | . 2 GAME | 1000000.0 | . 3 PHOTOGRAPHY | 1000000.0 | . 4 SHOPPING | 1000000.0 | . 5 WEATHER | 1000000.0 | . 6 VIDEO_PLAYERS | 1000000.0 | . 7 COMMUNICATION | 500000.0 | . 8 FOOD_AND_DRINK | 500000.0 | . 9 HEALTH_AND_FITNESS | 500000.0 | . bar_n( android_users, &quot;Top 10 Most Popular Android App Types&quot;, &quot;Median Number of Users per App&quot;, n = 10, ) . Since the top 5 spots all had the same median number of users per app (1000000), the graph was expanded to include the top 10 spots. . It appears that the types of Android apps with the highest median number of users per app are: . GAME | VIDEO_PLAYERS | WEATHER | EDUCATION | ENTERTAINMENT | PHOTOGRAPHY | SHOPPING | . . . Important: We can say the following about the Google Play Store. . Both fun apps and practical apps are very common. | The most popular apps are also a mix of fun apps and practical apps. | . App Profile Ideas . Based on the results, we can now determine a profitable app profile for the hypothetical app company. . Here is a summary of the findings on the 2 app stores. . The Google Play Store has a balanced mix of fun and practical apps, so we can pick either kind. | On the other hand, the Apple App Store appears to be oversaturated with game apps, and practical apps are more popular. | . Therefore, in order to get the most users, the app company can set themselves apart in the Apple App Store by developing a useful practical app. . The most popular types of practical apps for the Apple App Store would be: . Productivity | Navigation | Reference | Shopping | . For the Google Play Store, these would be: . Weather | Education | Photography | Shopping | . Shopping appears in both lists, so it may be the most profitable type of app. However, the app company would have to make a unique app that has an edge over existing popular shopping apps. The same would apply for making a navigation app. . Considering that Reference and Education apps are popular, perhaps these two types could be combined into one app. First, let us find out the titles of the most popular apps in these genres. . reference_popularity = ios_clean[[ &quot;track_name&quot;, &quot;rating_count_tot&quot; ]].loc[ ios_clean[&quot;prime_genre&quot;] == &quot;Reference&quot; ].dropna( ).sort_values( &quot;rating_count_tot&quot;, ascending = False, ).set_index( &quot;track_name&quot;, )[&quot;rating_count_tot&quot;] sr_to_df(reference_popularity, n_head = 10) . track_name number . 0 Bible | 985920 | . 1 Dictionary.com Dictionary &amp; Thesaurus | 200047 | . 2 Dictionary.com Dictionary &amp; Thesaurus for iPad | 54175 | . 3 Google Translate | 26786 | . 4 Muslim Pro: Ramadan 2017 Prayer Times, Azan, Q... | 18418 | . 5 New Furniture Mods - Pocket Wiki &amp; Game Tools ... | 17588 | . 6 Merriam-Webster Dictionary | 16849 | . 7 Night Sky | 12122 | . 8 City Maps for Minecraft PE - The Best Maps for... | 8535 | . 9 LUCKY BLOCK MOD ™ for Minecraft PC Edition - T... | 4693 | . bar_n( reference_popularity, &quot;Top 10 Most Popular iOS Reference Apps&quot;, &quot;Number of Users&quot;, ) . education_popularity = android_clean[[ &quot;App&quot;, &quot;Installs&quot; ]].loc[ android_clean[&quot;Category&quot;] == &quot;EDUCATION&quot; ].dropna( ).sort_values( &quot;Installs&quot;, ascending = False, ).set_index( &quot;App&quot;, )[&quot;Installs&quot;] sr_to_df(education_popularity, n_head = 5) . App number . 0 Quizlet: Learn Languages &amp; Vocab with Flashcards | 10000000 | . 1 Learn languages, grammar &amp; vocabulary with Mem... | 10000000 | . 2 Learn English with Wlingua | 10000000 | . 3 Remind: School Communication | 10000000 | . 4 Math Tricks | 10000000 | . bar_n( education_popularity, &quot;Top 10 Most Popular Android Education Apps&quot;, &quot;Number of Users&quot;, ) . The most popular Reference apps are the Bible and some dictionary and translation apps. The most popular Education apps teach languages (especially English), or Math. . Therefore, the following are some ideas of a profitable app: . An app containing the Bible, another religious text, or another well-known text. The app can additionally include reflections, analyses, or quizzes about the text. | An app that contains an English dictionary, a translator, and some quick guides on English vocabulary and grammar. An app like the above, but for a different language that is spoken by many people. | . | An app that teaches English and Math lessons. Perhaps it could be marketed as a practice app for an entrance exam. | . Conclusion . In this project, we analyzed app data from a Google Play Store dataset and an Apple App Store dataset. Apps were limited to free apps targeted towards English speakers, because the hypothetical app company makes these kinds of apps. The most common and popular app genres were determined. . In the end, several ideas of profitable apps were listed. The app company may now review the analysis and consider the suggestions. This may help them make an informed, data-driven decision regarding the next app that they will develop. .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/altair/2021/05/08/Profitable-App-Profiles.html",
            "relUrl": "/python/pandas/numpy/altair/2021/05/08/Profitable-App-Profiles.html",
            "date": " • May 8, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Miguel Antonio H. Germar, but you can call me Migs. I am currently a high school student from Quezon City, Philippines. I am interested in pursuing a college degree and career in Data Science. . . Contact . Email: migs.germar@gmail.com . Facebook: https://www.facebook.com/miguelantonio.germar/ . Github: https://github.com/MiguelAHG . My Learning Journey . I set up this website using fastpages as a blog and portfolio for Data Science projects. Below, I outline my learning journey in Data Science. . How I started in Data Science . I first found out about data science in 2020, around the time that the COVID-19 pandemic started. I decided to take online courses about data science in the summer, in order to see if it was interesting. First, I took the Python Core, Data Science, and Machine learning courses on Sololearn. . Here, I learned basic skills in the following: . Python | Spyder IDE | Jupyter Noteboook | Numpy | Pandas | Matplotlib | Scikit-learn | SQL basics | . Data Science in Practical Research . When classes started again, I prioritized schoolwork. However, I was able to apply my data science skills in my group’s Practical Research project. Our research paper was entitled “The Effect of COVID-19’s Consequences on Philippine Frontliners on their Mental Health: A Descriptive, Correlational Study.” We collected survey responses from 196 frontliners. I wrote the entire analysis in Python, from data cleaning to transformation to modeling. . I had to learn new things in order to do this, including: . Statsmodels | Dummy-coding categorical variables | Multiple linear regression | Testing the assumptions of OLS regression | Interpreting model results | . Dataquest Online Platform . In April of 2022, I completed the Data Scientist in Python track on Dataquest. The courses included many guided projects in Jupyter Notebook, some of which have been posted on my blog. . I plan to continue studying Dataquest’s other courses, such as Data Engineer in Python, and the Power BI skill path. . .",
          "url": "https://miguelahg.github.io/mahg-data-science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://miguelahg.github.io/mahg-data-science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}