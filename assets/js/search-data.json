{
  
    
        "post0": {
            "title": "Optimizing Hacker News Posts",
            "content": ". Overview . Hacker News is a popular website about technology. Specifically, it is a community choice aggregator of tech content. Users can: . Submit tech articles that the user found online. | Submit &quot;Ask&quot; posts to ask the community a question. | Submit &quot;Show&quot; posts to show the community something that the user made. | Vote and comment on other people&#39;s posts. | . In this project, we will analyze and compare Ask posts and Show posts in order to answer the following questions: . Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post? | Can posting at a certain time of day result in getting more comments? | . This analysis can be helpful for Hacker News users who would like for their posts to reach a larger audience on the platform. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Exploring Hacker News Posts. The research questions and general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . Package Installs . import pandas as pd import numpy as np import altair as alt import datetime as dt . Dataset . The dataset for this project is the Hacker News Posts dataset on Kaggle, uploaded by Hacker News. . The following is quoted from the dataset&#39;s Description. . This data set is Hacker News posts from the last 12 months (up to September 26 2016). . It includes the following columns: . title: title of the post (self explanatory) | url: the url of the item being linked to | num_points: the number of upvotes the post received | num_comments: the number of comments the post received | author: the name of the account that made the post | created_at: the date and time the post was made (the time zone is Eastern Time in the US) | . Let us view the first 5 rows of the dataset below. . hn = pd.read_csv(&quot;./2021-05-11-OHNP-Files/HN_posts_year_to_Sep_26_2016.csv&quot;) hn.head() . id title url num_points num_comments author created_at . 0 12579008 | You have two days to comment if you want stem ... | http://www.regulations.gov/document?D=FDA-2015... | 1 | 0 | altstar | 9/26/2016 3:26 | . 1 12579005 | SQLAR the SQLite Archiver | https://www.sqlite.org/sqlar/doc/trunk/README.md | 1 | 0 | blacksqr | 9/26/2016 3:24 | . 2 12578997 | What if we just printed a flatscreen televisio... | https://medium.com/vanmoof/our-secrets-out-f21... | 1 | 0 | pavel_lishin | 9/26/2016 3:19 | . 3 12578989 | algorithmic music | http://cacm.acm.org/magazines/2011/7/109891-al... | 1 | 0 | poindontcare | 9/26/2016 3:16 | . 4 12578979 | How the Data Vault Enables the Next-Gen Data W... | https://www.talend.com/blog/2016/05/12/talend-... | 1 | 0 | markgainor1 | 9/26/2016 3:14 | . Below is the shape of the dataset. . print(hn.shape) . (293119, 7) . There are 293,119 rows and 7 columns in the dataset. . Before the data can be analyzed, it must first be cleaned. . Data Cleaning . Duplicate Rows . Below, I use pandas to delete duplicate rows except for the first instance of each duplicate. . Rows will be considered as duplicates if they are exactly alike in all features. I decided on this because it is possible for two posts to have the same title and/or url but be posted at different times or by different users. Thus, we cannot identify duplicates based on one or two features alone. . hn = hn.drop_duplicates(keep = &quot;first&quot;) print(hn.shape) . (293119, 7) . No duplicates were found. All rows were kept. . Posts without Comments . Our research questions involve the number of comments on each post. However, there are many posts with 0 comments. . To illustrate this, below a frequency table of the number of comments on each post. . def freq_comments(df = hn): &quot;&quot;&quot;Function to make a frequency table of the number of comments per post specifically for the Hacker News dataset.&quot;&quot;&quot; freq_df = df[&quot;num_comments&quot;].value_counts().reset_index() freq_df.columns = [&quot;num_comments&quot;, &quot;frequency&quot;] freq_df = freq_df.sort_values( by = &quot;num_comments&quot;, ).reset_index( drop = True, ) return freq_df freq_df1 = freq_comments() freq_df1 . num_comments frequency . 0 0 | 212718 | . 1 1 | 28055 | . 2 2 | 9731 | . 3 3 | 5016 | . 4 4 | 3272 | . ... ... | ... | . 543 1007 | 1 | . 544 1120 | 1 | . 545 1448 | 1 | . 546 1733 | 1 | . 547 2531 | 1 | . 548 rows × 2 columns . The table above shows that posts with 0 comments are most frequent. . Let us plot the table on a histogram. . def hist_comments(df, title): &quot;&quot;&quot;Function to make a histogram of the number of comments per post specifically for the Hacker News dataset.&quot;&quot;&quot; chart = alt.Chart(df).mark_bar().encode( x = alt.X( &quot;num_comments:Q&quot;, title = &quot;Number of Comments&quot;, bin = alt.Bin(step = 1) ), y = alt.Y( &quot;frequency:Q&quot;, title = &quot;Frequency&quot;, ), ).properties( title = title, width = 700, height = 400, ) return chart hist_comments(freq_df1, &quot;Histogram of Number of Comments per Post&quot;) . There are so many posts with 0 comments that we cannot see the histogram bins for other numbers of comments. . Considering that the dataset is large and most rows have 0 comments, it would be best to drop all rows with 0 comments. This would make analysis less computationally expensive and allow us to answer our research questions. . with_comments = hn[&quot;num_comments&quot;] &gt; 0 hn = hn.loc[with_comments].reset_index(drop = True) print(hn.shape) . (80401, 7) . Now, the dataset is left with only 80,401 rows. This will be easier to work with. . Below is the new histogram. . freq_df2 = freq_comments() hist_comments(freq_df2, &quot;Histogram of Number of Comments per Post&quot;) . The distribution is still heavily right-skewed since many posts have very few comments. What&#39;s important is that unnecessary data has been removed. . Missing Values . Finally, let us remove rows with missing values. In order to answer our research questions, we only need the following columns: . title | num_comments | created_at | . Thus, we will delete rows with missing values in this column. . hn.dropna( subset = [&quot;title&quot;, &quot;num_comments&quot;, &quot;created_at&quot;], inplace = True, ) print(hn.shape) . (80401, 7) . The number of rows did not change from 80401. Therefore, no missing values were found in these columns, and no rows were dropped. . Data cleaning is now done. . Filtering Posts . As mentioned earlier, the first research question involves comparing Ask posts to Show posts. In order to do this, we have to group the posts into three types: . Ask Posts | Show Posts | Other Posts | . Other posts are usually posts that share a tech article found online. . Ask and Show posts can be identified using the start of the post title. Ask posts start with &quot;Ask HN: &quot;. . ask_mask = [index for index, value in hn[&quot;title&quot;].iteritems() if value.startswith(&quot;Ask HN: &quot;) ] hn.loc[ask_mask].head() . id title url num_points num_comments author created_at . 1 12578908 | Ask HN: What TLD do you use for local developm... | NaN | 4 | 7 | Sevrene | 9/26/2016 2:53 | . 6 12578522 | Ask HN: How do you pass on your work when you ... | NaN | 6 | 3 | PascLeRasc | 9/26/2016 1:17 | . 18 12577870 | Ask HN: Why join a fund when you can be an angel? | NaN | 1 | 3 | anthony_james | 9/25/2016 22:48 | . 27 12577647 | Ask HN: Someone uses stock trading as passive ... | NaN | 5 | 2 | 00taffe | 9/25/2016 21:50 | . 41 12576946 | Ask HN: How hard would it be to make a cheap, ... | NaN | 2 | 1 | hkt | 9/25/2016 19:30 | . On the other hand, Show posts start with &quot;Show HN: &quot;. . show_mask = [index for index, value in hn[&quot;title&quot;].iteritems() if value.startswith(&quot;Show HN: &quot;) ] hn.loc[show_mask].head() . id title url num_points num_comments author created_at . 35 12577142 | Show HN: Jumble Essays on the go #PaulInYourP... | https://itunes.apple.com/us/app/jumble-find-st... | 1 | 1 | ryderj | 9/25/2016 20:06 | . 43 12576813 | Show HN: Learn Japanese Vocab via multiple cho... | http://japanese.vul.io/ | 1 | 1 | soulchild37 | 9/25/2016 19:06 | . 52 12576090 | Show HN: Markov chain Twitter bot. Trained on ... | https://twitter.com/botsonasty | 3 | 1 | keepingscore | 9/25/2016 16:50 | . 68 12575471 | Show HN: Project-Okot: Novel, CODE-FREE data-a... | https://studio.nuchwezi.com/ | 3 | 1 | nfixx | 9/25/2016 14:30 | . 88 12574773 | Show HN: Cursor that Screenshot | http://edward.codes/cursor-that-screenshot | 3 | 3 | ed-bit | 9/25/2016 10:50 | . Other posts do not start with any special label. . Below, I create a new column &quot;post_type&quot; and assign the appropriate value to each row. . hn[&quot;post_type&quot;] = &quot;Other&quot; hn.loc[ask_mask, &quot;post_type&quot;] = &quot;Ask&quot; hn.loc[show_mask, &quot;post_type&quot;] = &quot;Show&quot; hn[[&quot;title&quot;, &quot;post_type&quot;]] . title post_type . 0 Saving the Hassle of Shopping | Other | . 1 Ask HN: What TLD do you use for local developm... | Ask | . 2 Amazons Algorithms Dont Find You the Best Deals | Other | . 3 Emergency dose of epinephrine that does not co... | Other | . 4 Phone Makers Could Cut Off Drivers. So Why Don... | Other | . ... ... | ... | . 80396 My Keyboard | Other | . 80397 Google&#39;s new logo was created by Russian desig... | Other | . 80398 Why we aren&#39;t tempted to use ACLs on our Unix ... | Other | . 80399 Ask HN: What is/are your favorite quote(s)? | Ask | . 80400 Dying vets fuck you letter (2013) | Other | . 80401 rows × 2 columns . Each row has now been labeled as a type of post. . Research Question 1: Comparing Ask and Show Posts . The first research question is, &quot;Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post?&quot; . Note that the data is not normally distributed; it is right-skewed. For example, here is the distribution of the number of comments per Ask post. . ask_freq = freq_comments( df = hn.loc[hn[&quot;post_type&quot;] == &quot;Ask&quot;] ) ask_freq . num_comments frequency . 0 1 | 1383 | . 1 2 | 1238 | . 2 3 | 762 | . 3 4 | 592 | . 4 5 | 373 | . ... ... | ... | . 203 898 | 1 | . 204 910 | 1 | . 205 937 | 1 | . 206 947 | 1 | . 207 1007 | 1 | . 208 rows × 2 columns . hist_comments( ask_freq, &quot;Histogram of Number of Comments per Ask Post&quot; ) . The histogram is similar for Show posts. . show_freq = freq_comments( df = hn.loc[hn[&quot;post_type&quot;] == &quot;Show&quot;], ) show_freq . num_comments frequency . 0 1 | 1738 | . 1 2 | 814 | . 2 3 | 504 | . 3 4 | 300 | . 4 5 | 196 | . ... ... | ... | . 137 250 | 1 | . 138 257 | 1 | . 139 280 | 1 | . 140 298 | 1 | . 141 306 | 1 | . 142 rows × 2 columns . hist_comments( show_freq, &quot;Histogram of Number of Comments per Show Post&quot; ) . Therefore, the mean would not be a good measure of central tendency for the &quot;average number of comments per post.&quot; Thus, we will use the median instead. . dct = {&quot;Ask&quot;: None, &quot;Show&quot;: None} for key in dct: median = np.median( hn[&quot;num_comments&quot;].loc[hn[&quot;post_type&quot;] == key] ) dct[key] = median table = pd.DataFrame.from_dict( dct, orient = &quot;index&quot;, ).reset_index( ).rename(columns = { &quot;index&quot;: &quot;post_type&quot;, 0: &quot;median_comments&quot;, }) chart = alt.Chart(table).mark_bar().encode( y = alt.Y(&quot;post_type:N&quot;, title = &quot;Post Type&quot;), x = alt.X(&quot;median_comments:Q&quot;, title = &quot;Median Number of Comments per Post&quot;), ).properties( title = &quot;Median Number of Comments for the Two Post Types&quot;, ) chart . The bar graph shows that Show posts have a higher median number of comments per post, compared to Ask posts. . Important: The results suggest that Show posts get more comments than Ask posts. It may be easier for users to reach a larger audience via Show posts. . Research Question 2: Active Times . The second research question is, &quot;Can posting at a certain time of day result in getting more comments?&quot; . For this part of the analysis, we will only be using Show post data for simplicity. . We will divide the day into 24 one-hour periods, and then calculate the number of Show posts created in each period. . String Template for Time . Before analying, we need to inspect the &quot;created_at&quot; column of the dataset. . hn_show = hn.loc[ hn[&quot;post_type&quot;] == &quot;Show&quot; ].reset_index( drop = True, ) hn_show[[&quot;created_at&quot;]].head() . created_at . 0 9/25/2016 20:06 | . 1 9/25/2016 19:06 | . 2 9/25/2016 16:50 | . 3 9/25/2016 14:30 | . 4 9/25/2016 10:50 | . The strings in this column appear to follow the following format: . month/day/year hour:minute . With the datetime module, the following is the equivalent formatting template. . template = &quot;%m/%d/%Y %H:%M&quot; . Parsing Times . The time data can now be parsed and used for analysis. . The hours_posts dictionary will count the number of posts at certain hours. The hours_comments dictionary will count the number of comments received by posts made at certain hours. . hours_posts = {} hours_comments = {} for index, row in hn_show.iterrows(): date_str = row[&quot;created_at&quot;] num_comments = row[&quot;num_comments&quot;] # datetime object date_dt = dt.datetime.strptime( date_str, template, ) # extract hour hour = date_dt.hour # update dictionaries hours_posts.setdefault(hour, 0) hours_posts[hour] += 1 hours_comments.setdefault(hour, 0) hours_comments[hour] += num_comments . The hours were parsed and mapped to their respective counts of posts and comments. . The code below transforms the dictionaries into DataFrames for ease of use. . def hour_to_df(dct, data_label): &quot;&quot;&quot;Make a DataFrame from a dictionary that maps an &#39;hour&#39; column to another column, named by `data_label`.&quot;&quot;&quot; result = pd.DataFrame.from_dict( dct, orient = &quot;index&quot;, ).reset_index( ).rename(columns = { &quot;index&quot;: &quot;hour&quot;, 0: data_label, }).sort_values( by = &quot;hour&quot;, ).reset_index( drop = True, ) return result hours_posts_df = hour_to_df( hours_posts, data_label = &quot;num_posts&quot;, ) hours_comments_df = hour_to_df( hours_comments, data_label = &quot;num_comments&quot;, ) hours_posts_df.head() . hour num_posts . 0 0 | 141 | . 1 1 | 133 | . 2 2 | 103 | . 3 3 | 97 | . 4 4 | 90 | . hours_comments_df.head() . hour num_comments . 0 0 | 1283 | . 1 1 | 1001 | . 2 2 | 1074 | . 3 3 | 934 | . 4 4 | 978 | . The hours have been parsed, and the tables have been generated. . Additionally, another DataFrame is created below. It calculates the average number of comments per post by the hour posted. . frames = [hours_posts_df, hours_comments_df] hours_mean = pd.concat(frames, axis = 1).drop_duplicates() # Drop duplicate columns hours_mean = hours_mean.loc[:, ~hours_mean.columns.duplicated()] means = [] for index, row in hours_mean.iterrows(): hour, posts, comments = row means.append(comments / posts) hours_mean[&quot;comments_per_post&quot;] = means hours_mean . hour num_posts num_comments comments_per_post . 0 0 | 141 | 1283 | 9.099291 | . 1 1 | 133 | 1001 | 7.526316 | . 2 2 | 103 | 1074 | 10.427184 | . 3 3 | 97 | 934 | 9.628866 | . 4 4 | 90 | 978 | 10.866667 | . 5 5 | 75 | 591 | 7.880000 | . 6 6 | 95 | 904 | 9.515789 | . 7 7 | 125 | 1572 | 12.576000 | . 8 8 | 159 | 1770 | 11.132075 | . 9 9 | 158 | 1411 | 8.930380 | . 10 10 | 155 | 1225 | 7.903226 | . 11 11 | 227 | 2412 | 10.625551 | . 12 12 | 300 | 3609 | 12.030000 | . 13 13 | 334 | 3314 | 9.922156 | . 14 14 | 331 | 3839 | 11.598187 | . 15 15 | 390 | 3822 | 9.800000 | . 16 16 | 387 | 3767 | 9.733850 | . 17 17 | 368 | 3228 | 8.771739 | . 18 18 | 311 | 3242 | 10.424437 | . 19 19 | 270 | 2791 | 10.337037 | . 20 20 | 246 | 2183 | 8.873984 | . 21 21 | 209 | 1759 | 8.416268 | . 22 22 | 192 | 1450 | 7.552083 | . 23 23 | 147 | 1443 | 9.816327 | . Number of Posts by Hour of the Day . Below are the table and graph for the number of posts per hour of the day. . hours_posts_df . hour num_posts . 0 0 | 141 | . 1 1 | 133 | . 2 2 | 103 | . 3 3 | 97 | . 4 4 | 90 | . 5 5 | 75 | . 6 6 | 95 | . 7 7 | 125 | . 8 8 | 159 | . 9 9 | 158 | . 10 10 | 155 | . 11 11 | 227 | . 12 12 | 300 | . 13 13 | 334 | . 14 14 | 331 | . 15 15 | 390 | . 16 16 | 387 | . 17 17 | 368 | . 18 18 | 311 | . 19 19 | 270 | . 20 20 | 246 | . 21 21 | 209 | . 22 22 | 192 | . 23 23 | 147 | . This table is in 24-hour time. Hour 13 refers to 1:00 PM. The table shows how many posts are made for every hour in the day. . Below is a line chart that shows this visually. . chart = alt.Chart(hours_posts_df).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;num_posts:Q&quot;, title = &quot;Number of Posts&quot;), ).properties( title = &quot;Number of Posts by Hour of the Day&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . The histogram clearly shows that Hacker News users most actively make posts between 15:00 and 18:00, or from 3:00 PM to 6:00 PM. . The most active hour for posting is 3:00 PM - 4:00 PM. . Number of Comments by Hour Posted . Next, a similar analysis is done for the number of comments received by posts, grouped by the hour that they were created. Below is the table for this data. . hours_comments_df . hour num_comments . 0 0 | 1283 | . 1 1 | 1001 | . 2 2 | 1074 | . 3 3 | 934 | . 4 4 | 978 | . 5 5 | 591 | . 6 6 | 904 | . 7 7 | 1572 | . 8 8 | 1770 | . 9 9 | 1411 | . 10 10 | 1225 | . 11 11 | 2412 | . 12 12 | 3609 | . 13 13 | 3314 | . 14 14 | 3839 | . 15 15 | 3822 | . 16 16 | 3767 | . 17 17 | 3228 | . 18 18 | 3242 | . 19 19 | 2791 | . 20 20 | 2183 | . 21 21 | 1759 | . 22 22 | 1450 | . 23 23 | 1443 | . Below is the line chart that visualizes the data about the number of comments received by the hour posted. . chart = alt.Chart(hours_comments_df).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;num_comments:Q&quot;, title = &quot;Number of Comments&quot;), ).properties( title = &quot;Number of Comments by Hour Posted&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . The results in this histogram are similar to the previous one. Posts that are made from 14:00 to 17:00, or 2:00 PM - 5:00 PM, receive the most comments. . Posts made at 2:00 PM - 3:00 PM receive an especially high number of comments. . Mean Number of Comments per Post, by Hour Posted . The table below shows the mean number of comments per post, by the hour of posting. . hours_mean . hour num_posts num_comments comments_per_post . 0 0 | 141 | 1283 | 9.099291 | . 1 1 | 133 | 1001 | 7.526316 | . 2 2 | 103 | 1074 | 10.427184 | . 3 3 | 97 | 934 | 9.628866 | . 4 4 | 90 | 978 | 10.866667 | . 5 5 | 75 | 591 | 7.880000 | . 6 6 | 95 | 904 | 9.515789 | . 7 7 | 125 | 1572 | 12.576000 | . 8 8 | 159 | 1770 | 11.132075 | . 9 9 | 158 | 1411 | 8.930380 | . 10 10 | 155 | 1225 | 7.903226 | . 11 11 | 227 | 2412 | 10.625551 | . 12 12 | 300 | 3609 | 12.030000 | . 13 13 | 334 | 3314 | 9.922156 | . 14 14 | 331 | 3839 | 11.598187 | . 15 15 | 390 | 3822 | 9.800000 | . 16 16 | 387 | 3767 | 9.733850 | . 17 17 | 368 | 3228 | 8.771739 | . 18 18 | 311 | 3242 | 10.424437 | . 19 19 | 270 | 2791 | 10.337037 | . 20 20 | 246 | 2183 | 8.873984 | . 21 21 | 209 | 1759 | 8.416268 | . 22 22 | 192 | 1450 | 7.552083 | . 23 23 | 147 | 1443 | 9.816327 | . This is visualized in the line chart below, which looks quite different from the previous two charts. . chart = alt.Chart(hours_mean).mark_line().encode( x = alt.X( &quot;hour:O&quot;, title = &quot;Hour of the Day&quot;, axis = alt.Axis(labelAngle = 0) ), y = alt.Y(&quot;comments_per_post:Q&quot;, title = &quot;Mean Number of Comments per Post&quot;), ).properties( title = &quot;Mean Number of Comments per Post, by Hour Posted&quot;, width = 700, height = 400, ).configure_axis( grid = True, ) chart . This graph shows that the mean number of comments per post is more consistent throughout the day. The statistic is highest at 7:00 AM - 8:00 AM, but the afternoon values are only slightly lower. . This brings us a new question: Why does the mean number of comments per post not peak in the afternoon? . A possible explanation is that since the site is oversaturated with new posts in the afternoon, only the very best posts receive attention. The rest are lost in the flood of new posts. . Important: Based on the results, even if users are most active in the afternoon, it may be best to post in the morning. . If you post in the morning, you will receive at least some attention since you won&#39;t have to compete with many other new posts. You may get a few upvotes and comments. Then, the many users logging in during the afternoon would see that your posted has already received comments. Thus, they would be interested in it and bring more attention to it. . Conclusion . In this project, we analyzed data about Hacker News posts, specifically regarding the number of comments that they receive. Below are the research questions, and the best answers that we could come up with from our analysis. . . Between Ask posts and Show posts, which type receives more comments in terms of average number of comments per post? | . Show posts receive a higher median number of comments per post, compared to Ask posts. In order to reach a wider audience or converse with more users, it is better to make a Show post. . . Can posting at a certain time of day result in getting more comments? | . Hacker News users are very active in the afternoon, from 2:00 PM to 6:00 PM. However, if you post in the afternoon, your post may get lost in the flood of new posts. It is better to post in the morning, like at 7:00 AM, so that some people can notice your post. Then, your post can get more attention in the afternoon. . . Thanks for reading! .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/altair/datetime/2021/05/11/Optimizing-Hacker-News-Posts.html",
            "relUrl": "/python/pandas/numpy/altair/datetime/2021/05/11/Optimizing-Hacker-News-Posts.html",
            "date": " • May 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Profitable App Profiles for iOS and Android",
            "content": ". Overview . Welcome. In this project, we will be working on data about different types of apps and their corresponding number of users. The goal is to determine which apps can best attract the largest number of users. This will help a hypothetical app company make decisions regarding what apps to develop in the near future. . Note: I wrote this notebook for the Dataquest course&#8217;s Guided Project: Profitable App Profiles for the App Store and Google Play Markets. The hypothetical app company and the general project flow came from Dataquest. However, all of the text and code here are written by me unless stated otherwise. . App Company&#39;s Context . First, we must know the context of the hypothetical app company so that we can align our analysis with it. . This company only makes free apps directed toward an English-speaking audience. They get revenue from in-app advertisements and purchases. Thus, they rely on having a large number of users so that they can generate more revenue. . Additionally, their apps should ideally be successful on both Google Play Store and the Apple App Store. The reason is that the company has the following 3-step validation strategy: . (from the Dataquest guided project) . Build a minimal Android version of the app, and add it to Google Play. | If the app has a good response from users, we develop it further. | If the app is profitable after six months, we build an iOS version of the app and add it to the App Store. | We&#39;ll take this information into consideration throughout our analysis. . Package Installs . import pandas as pd import numpy as np import altair as alt import re . App Data Overview . This project uses two datasets. . The Google Play Store dataset lists over 10,000 Android apps. | The Apple App Store dataset lists over 7,000 iOS apps. | . data_apple = pd.read_csv(&quot;./2021-05-08-PAP-Files/AppleStore.csv&quot;, header = 0) data_google = pd.read_csv(&quot;./2021-05-08-PAP-Files/googleplaystore.csv&quot;, header = 0) . Apple App Store dataset . print(data_apple.shape) . (7197, 16) . The dataset has 7197 rows (1 row per app), and 16 columns which describe these apps. . According to the Kaggle documentation (Mobile App Store ( 7200 apps)), the following are the columns and their meanings. . &quot;id&quot; : App ID | &quot;track_name&quot;: App Name | &quot;size_bytes&quot;: Size (in Bytes) | &quot;currency&quot;: Currency Type | &quot;price&quot;: Price amount | &quot;ratingcounttot&quot;: User Rating counts (for all version) | &quot;ratingcountver&quot;: User Rating counts (for current version) | &quot;user_rating&quot; : Average User Rating value (for all version) | &quot;userratingver&quot;: Average User Rating value (for current version) | &quot;ver&quot; : Latest version code | &quot;cont_rating&quot;: Content Rating | &quot;prime_genre&quot;: Primary Genre | &quot;sup_devices.num&quot;: Number of supporting devices | &quot;ipadSc_urls.num&quot;: Number of screenshots showed for display | &quot;lang.num&quot;: Number of supported languages | &quot;vpp_lic&quot;: Vpp Device Based Licensing Enabled | . A sample of the first 5 rows of the dataset is shown below. . data_apple.head() . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 0 284882215 | Facebook | 389879808 | USD | 0.0 | 2974676 | 212 | 3.5 | 3.5 | 95.0 | 4+ | Social Networking | 37 | 1 | 29 | 1 | . 1 389801252 | Instagram | 113954816 | USD | 0.0 | 2161558 | 1289 | 4.5 | 4.0 | 10.23 | 12+ | Photo &amp; Video | 37 | 0 | 29 | 1 | . 2 529479190 | Clash of Clans | 116476928 | USD | 0.0 | 2130805 | 579 | 4.5 | 4.5 | 9.24.12 | 9+ | Games | 38 | 5 | 18 | 1 | . 3 420009108 | Temple Run | 65921024 | USD | 0.0 | 1724546 | 3842 | 4.5 | 4.0 | 1.6.2 | 9+ | Games | 40 | 5 | 1 | 1 | . 4 284035177 | Pandora - Music &amp; Radio | 130242560 | USD | 0.0 | 1126879 | 3594 | 4.0 | 4.5 | 8.4.1 | 12+ | Music | 37 | 4 | 1 | 1 | . Google Play Store dataset . print(data_google.shape) . (10841, 13) . The dataset has 10841 rows and 13 columns. . The column names are self-explanatory, so the Kaggle documentation (Google Play Store Apps) does not describe them. . print(list(data_google.columns)) . [&#39;App&#39;, &#39;Category&#39;, &#39;Rating&#39;, &#39;Reviews&#39;, &#39;Size&#39;, &#39;Installs&#39;, &#39;Type&#39;, &#39;Price&#39;, &#39;Content Rating&#39;, &#39;Genres&#39;, &#39;Last Updated&#39;, &#39;Current Ver&#39;, &#39;Android Ver&#39;] . Below is a sample of the dataset. . data_google.head() . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 1 Coloring book moana | ART_AND_DESIGN | 3.9 | 967 | 14M | 500,000+ | Free | 0 | Everyone | Art &amp; Design;Pretend Play | January 15, 2018 | 2.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . Data Cleaning . Before analysis, the data must first be cleaned of unwanted datapoints. . Inaccurate Data . This Kaggle discussion about the Google Play dataset indicates that row 10472 (excluding the header) has an error. . Below, I have printed row 0 and row 10472 so that these can be compared. . data_google.iloc[[0, 10472]] . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 10472 Life Made WI-Fi Touchscreen Photo Frame | 1.9 | 19.0 | 3.0M | 1,000+ | Free | 0 | Everyone | NaN | February 11, 2018 | 1.0.19 | 4.0 and up | NaN | . As we look at row 10472 in the context of the column headers and row 0, the following things become clear. . The &quot;Category&quot; value is not present. Thus, all values to the right of it have been shifted leftward. | The &quot;Android Ver&quot; column was left with a missing value. | . Thus, this row will be removed. . if data_google.iloc[10472, 0] == &#39;Life Made WI-Fi Touchscreen Photo Frame&#39;: # This if-statement prevents more rows from being deleted # if the cell is run again. data_google.drop(10472, inplace = True) print(&quot;The inaccurate row was deleted.&quot;) . The inaccurate row was deleted. . Duplicate Data . There are also duplicate app entries in the Google Play dataset. We can consider a row as a duplicates if another row exists that has the same &quot;App&quot; value. . Here, I count the total number of duplicate rows. This turns out to be 1979 rows. . def count_duplicates(df, col_name): &quot;&quot;&quot;Count the number of duplicate rows in a DataFrame. `col_name` is the name of the column to be used as a basis for duplicate values.&quot;&quot;&quot; all_apps = {} for index, row in df.iterrows(): name = row[col_name] all_apps.setdefault(name, []).append(index) duplicate_inds = [ind for lst in all_apps.values() for ind in lst if len(lst) &gt; 1] n_duplicates = &quot;Duplicates: {}&quot;.format(len(duplicate_inds)) duplicate_rows = df.iloc[duplicate_inds] return n_duplicates, duplicate_rows google_dupes = count_duplicates(data_google, &quot;App&quot;) print(google_dupes[0]) . Duplicates: 1979 . As an example, there are 4 rows for Instagram: . ig_filter = data_google[&quot;App&quot;] == &quot;Instagram&quot; ig_rows = data_google.loc[ig_filter] . ig_rows . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 2545 Instagram | SOCIAL | 4.5 | 66577313 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 2604 Instagram | SOCIAL | 4.5 | 66577446 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 2611 Instagram | SOCIAL | 4.5 | 66577313 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . 3909 Instagram | SOCIAL | 4.5 | 66509917 | Varies with device | 1,000,000,000+ | Free | 0 | Teen | Social | July 31, 2018 | Varies with device | Varies with device | . Looking closely, we can see that duplicate rows are not exactly identical. The &quot;Reviews&quot; column, which shows the total number of reviews of the app, has different values. . It can be inferred that the row with the largest value is the newest entry for the app. Therefore, all duplicate rows will be dropped except for the ones with the largest &quot;Reviews&quot; values. . def remove_duplicates(df, name_col, reviews_col): # Each key-value pair will follow the format: # {&quot;App Name&quot;: maximum number of reviews among all duplicates} reviews_max = {} for index, row in df.iterrows(): name = row[name_col] n_reviews = int(row[reviews_col]) if n_reviews &gt; reviews_max.get(name, -1): reviews_max[name] = n_reviews # List of duplicate indices to drop, # excluding the row with the highest number of reviews # among that app&#39;s duplicate rows. indices_to_drop = [] # Rows with names that have already been added into this list # will be dropped. already_added = [] for index, row in df.iterrows(): name = row[name_col] n_reviews = int(row[reviews_col]) if (name not in already_added) and (n_reviews == reviews_max[name]): already_added.append(name) else: indices_to_drop.append(index) # Remove duplicates and return the clean dataset. clean = df.drop(indices_to_drop) return clean android_clean = remove_duplicates(data_google, &quot;App&quot;, &quot;Reviews&quot;) print(android_clean.shape) . (9659, 13) . After duplicates were removed, the Google Play dataset was left with 9659 rows. . As for the Apple App Store dataset, there are 4 duplicate rows. . apple_dupes = count_duplicates(data_apple, &quot;track_name&quot;) print(apple_dupes[0]) apple_dupes[1] . Duplicates: 4 . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 2948 1173990889 | Mannequin Challenge | 109705216 | USD | 0.0 | 668 | 87 | 3.0 | 3.0 | 1.4 | 9+ | Games | 37 | 4 | 1 | 1 | . 4463 1178454060 | Mannequin Challenge | 59572224 | USD | 0.0 | 105 | 58 | 4.0 | 4.5 | 1.0.1 | 4+ | Games | 38 | 5 | 1 | 1 | . 4442 952877179 | VR Roller Coaster | 169523200 | USD | 0.0 | 107 | 102 | 3.5 | 3.5 | 2.0.0 | 4+ | Games | 37 | 5 | 1 | 1 | . 4831 1089824278 | VR Roller Coaster | 240964608 | USD | 0.0 | 67 | 44 | 3.5 | 4.0 | 0.81 | 4+ | Games | 38 | 0 | 1 | 1 | . The &quot;rating_count_tot&quot; column in the Apple App Store dataset is like the &quot;Reviews&quot; column in the Google Play dataset. It tells the total number of reviews so far. Therefore, Apple App Store dataset duplicates can be removed by keeping the rows with the highest rating count totals. . ios_clean = remove_duplicates(data_apple, &quot;track_name&quot;, &quot;rating_count_tot&quot;) print(ios_clean.shape) . (7195, 16) . From 7197 rows, there are now 7195 rows in the Apple App Store dataset. . Non-English Apps . The hypothetical app company who will use this analysis is a company that only makes apps in English. Thus, all apps with non-English titles shall be removed from the datasets. . The task now is to identify titles which are not in English. It is known that in the ASCII table, the characters most commonly used in English are within codes 0 to 127. Some English app titles may have special characters or emojis, though, so I will only remove titles which have more than 3 characters outside of the normal range. . def is_english(text): unicode = [ord(char) for char in text] normal = [(code &gt;= 0 and code &lt;= 127) for code in unicode] non_english = len(text) - sum(normal) return non_english &lt;= 3 def keep_english(df, name_col): &quot;&quot;&quot;Return a new DataFrame containing only rows with English names.&quot;&quot;&quot; remove_indices = [] for index, row in df.iterrows(): name = row[name_col] if not is_english(name): remove_indices.append(index) return df.drop(remove_indices) android_clean = keep_english(android_clean, &quot;App&quot;) ios_clean = keep_english(ios_clean, &quot;track_name&quot;) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (9614, 13) Apple App Store Dataset: (6181, 16) . Now, there are only English apps in both datasets. . Paid Apps . As mentioned earlier, the app company only makes free apps. Therefore, data on paid apps is irrelevant to this analysis. Paid apps shall be identified and removed from both datasets. . def remove_paid(df, price_col): &quot;&quot;&quot;Return a new DataFrame without paid apps.&quot;&quot;&quot; remove_indices = [] for index, row in df.iterrows(): price = str(row[price_col]) # Keep characters that are numeric or periods. price = float(re.sub(&quot;[^0-9.]&quot;, &quot;&quot;, price)) if price != 0.0: remove_indices.append(index) return df.drop(remove_indices) android_clean = remove_paid(android_clean, &quot;Price&quot;) ios_clean = remove_paid(ios_clean, &quot;price&quot;) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (8864, 13) Apple App Store Dataset: (3220, 16) . The datasets were left with 8864 apps in Google Play and 3220 apps in the App Store. . Missing Data . Lastly, let us remove rows with missing data. Note that it would be wasteful to remove rows with missing data in columns that we will not inspect. Therefore, we will only remove rows with missing data in relevant columns. (Why these are relevant will be explained later.) These would be the following. . Google Play Store dataset . App | Category | Installs | Genres | . Apple App Store dataset . track_name | prime_genre | rating_count_tot | . I will now remove all rows with missing values in these columns. . android_clean.dropna( subset = [&quot;App&quot;, &quot;Category&quot;, &quot;Installs&quot;, &quot;Genres&quot;], inplace = True, ) ios_clean.dropna( subset = [&quot;track_name&quot;, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;], inplace = True, ) print(&quot;Google Play Store Dataset:&quot;, android_clean.shape) print(&quot;Apple App Store Dataset:&quot;, ios_clean.shape) . Google Play Store Dataset: (8864, 13) Apple App Store Dataset: (3220, 16) . These are the same shapes as before. Therefore, there were no missing values in the relevant columns. No datapoints were removed at this step. . Data cleaning is done, so now we can move on to the analysis. . Common App Genres . Now that the data has been cleaned, let us find out which genres of apps are most common in both app markets. If an app genre is common, then there may be high demand for it among users. . Which columns in the datasets can give information about the app genres? . print(&quot;Google Play Store&quot;) android_clean.head() . Google Play Store . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . 5 Paper flowers instructions | ART_AND_DESIGN | 4.4 | 167 | 5.6M | 50,000+ | Free | 0 | Everyone | Art &amp; Design | March 26, 2017 | 1.0 | 2.3 and up | . print(&quot; nApple App Store&quot;) ios_clean.head() . Apple App Store . id track_name size_bytes currency price rating_count_tot rating_count_ver user_rating user_rating_ver ver cont_rating prime_genre sup_devices.num ipadSc_urls.num lang.num vpp_lic . 0 284882215 | Facebook | 389879808 | USD | 0.0 | 2974676 | 212 | 3.5 | 3.5 | 95.0 | 4+ | Social Networking | 37 | 1 | 29 | 1 | . 1 389801252 | Instagram | 113954816 | USD | 0.0 | 2161558 | 1289 | 4.5 | 4.0 | 10.23 | 12+ | Photo &amp; Video | 37 | 0 | 29 | 1 | . 2 529479190 | Clash of Clans | 116476928 | USD | 0.0 | 2130805 | 579 | 4.5 | 4.5 | 9.24.12 | 9+ | Games | 38 | 5 | 18 | 1 | . 3 420009108 | Temple Run | 65921024 | USD | 0.0 | 1724546 | 3842 | 4.5 | 4.0 | 1.6.2 | 9+ | Games | 40 | 5 | 1 | 1 | . 4 284035177 | Pandora - Music &amp; Radio | 130242560 | USD | 0.0 | 1126879 | 3594 | 4.0 | 4.5 | 8.4.1 | 12+ | Music | 37 | 4 | 1 | 1 | . For Google Play, some columns that seem relevant are &quot;Category&quot; and &quot;Genres&quot;. For the Apple App Store, the relevant column is &quot;prime_genre&quot;. . We can determine the most common genres by using frequency tables of the mentioned columns. . def freq_table(df, label): &quot;&quot;&quot;Return a frequency table of the values in a column of a DataFrame.&quot;&quot;&quot; col = df[label] freq = {} for value in col: freq.setdefault(value, 0) freq[value] += 1 for key in freq: freq[key] /= len(df) / 100 freq_series = pd.Series(freq).sort_values(ascending = False) return freq_series def sr_to_df(sr, col_name = &quot;number&quot;, n_head = None): &quot;&quot;&quot;Return a DataFrame by resetting the index of a Series.&quot;&quot;&quot; df = sr.rename(col_name).reset_index().rename(columns = {&quot;index&quot;:&quot;name&quot;}) if n_head is not None: df = df.head(n_head) return df google_categories = freq_table(android_clean, &quot;Category&quot;) google_genres = freq_table(android_clean, &quot;Genres&quot;) apple_genres = freq_table(ios_clean, &quot;prime_genre&quot;) . The frequency tables will be inspected in the sections below. Only the top positions in each table will be shown, for brevity. . Apple App Store: Prime Genres . First, the frequency table of Apple App Store prime genres shall be analyzed. Below, I have ordered the table by frequency, descending. I have also made bar graphs showing the top 10 positions in each frequency table. . sr_to_df(apple_genres, &quot;percentage&quot;, n_head = 5) . name percentage . 0 Games | 58.136646 | . 1 Entertainment | 7.888199 | . 2 Photo &amp; Video | 4.968944 | . 3 Education | 3.664596 | . 4 Social Networking | 3.291925 | . def bar_n(series, chart_title, ylabel, n = 10, perc = False): &quot;&quot;&quot;Takes a series and outputs a bar graph of the first n items.&quot;&quot;&quot; series.index.name = &quot;name&quot; df = series.rename(&quot;number&quot;).reset_index() df[&quot;number&quot;] = [round(i, 2) for i in df[&quot;number&quot;]] df = df[:n] bar = alt.Chart(df).mark_bar().encode( x = alt.X(&quot;name&quot;, title = &quot;Name&quot;, sort = &quot;-y&quot;), y = alt.Y(&quot;number&quot;, title = ylabel), ) text = bar.mark_text( align = &#39;center&#39;, baseline = &#39;middle&#39;, dy = -5, # Nudge text upward ).encode( text = &#39;number:Q&#39; ) chart = (bar + text).properties( title = chart_title, width = 700, height = 400, ) return chart bar_n( apple_genres, &quot;Top 10 Most Common Prime Genres of iOS Apps&quot;, &quot;Percentage of Apps&quot;, perc = True, ) . The top 5 most common prime genres in the Apple App Store are Games, Entertainment, Photo &amp; Video, Education, and Social Networking. Games are at the top, occupying over 58% of all apps. This is a much higher percentage than any other single genre occupies. . Important: The general impression is that there are many more iOS apps that are entertainment-related apps compared to practical apps. . Google Play Store: Categories . Next, below is the frequency table for Google Play Store app categories. . sr_to_df(google_categories, &quot;percentage&quot;, 5) . name percentage . 0 FAMILY | 18.907942 | . 1 GAME | 9.724729 | . 2 TOOLS | 8.461191 | . 3 BUSINESS | 4.591606 | . 4 LIFESTYLE | 3.903430 | . bar_n( google_categories, &quot;Top 10 Most Common Categories of Android Apps&quot;, &quot;Percentage of Apps&quot;, perc = True, ) . The picture here seems to be different. The most common category is Family occupying almost 19% of all apps, followed by Game, Tools, Business, and Lifestyle. . Important: The table suggests that practical app categories are more common in Google Play than in the Apple App Store. . Google Play Store: Genres . Lastly, below is the frequency table for Google Play Store app genres. . sr_to_df(google_genres, &quot;percentage&quot;, 5) . name percentage . 0 Tools | 8.449910 | . 1 Entertainment | 6.069495 | . 2 Education | 5.347473 | . 3 Business | 4.591606 | . 4 Productivity | 3.892148 | . bar_n( google_genres, &quot;Top 10 Most Common Genres of Android Apps&quot;, &quot;Percentage of Apps&quot;, ) . There are 114 genres in this table, so it is not fully displayed. However, it would appear that the top 5 genres are Tools (8%), Entertainment, Education, Business, and Lifestyle. Like with the categories, practical apps are very common. . However, I noticed something special about this frequency table. Some genres are actually combinations of multiple genres, separated by semi-colons. If I can extract and count individual genres from these combined genres, then I can get a more accurate idea of app genres in the Google Play Store. . Note: This frequency table will show numbers instead of percentages. Since the genres overlap, the percentages would add up to greater than 100%. . freq = {} for value in android_clean[&quot;Genres&quot;]: genres = value.split(&quot;;&quot;) for genre in genres: freq.setdefault(genre, 0) freq[genre] += 1 google_genres_split = pd.Series(freq).sort_values(ascending = False) sr_to_df(google_genres_split, n_head = 5) . name number . 0 Tools | 750 | . 1 Education | 606 | . 2 Entertainment | 569 | . 3 Business | 407 | . 4 Lifestyle | 347 | . bar_n( google_genres_split, &quot;Top 10 Most Common Genres of Android Apps (Split Up)&quot;, &quot;Number of Apps&quot;, ) . It can be seen that the frequency table has slightly different placements now. However, the top genres are still Tools, Education, Entertainment, Business, and Lifestyle. Practical app genres are very common in the Google Play Store. They are more common here than in the Apple App Store. . Important: Based on the results, the Google Play Store has a selection of apps that is more balanced between entertainment and practicality. . . Going back to the the frequency table of Categories, since it seems that each Category represents a group of Genres. For example, one would expect apps in the Simulation, Arcade, Puzzle, Strategy, etc. genres to be under the Game category. It was shown earlier that this category is the 2nd most common in the Google Play Store. . The Categories column is more general and gives a more accurate picture of the common types of apps. Thus, from here on, I will be analyzing only the &quot;Category&quot; column and not the &quot;Genres&quot; column. . Note: I will now use &quot;app type&quot; to generally refer to the Apple App Store&#8217;s &quot;prime_genre&quot; values or the Google Play Store&#8217;s &quot;Category&quot; values. . App Types by Number of Users . We first looked at app types in terms of how common they are in the two app markets. Now, we shall see how many users there are for each app type. . Apple App Store: Rating Counts . In the Apple App Store dataset, there is no column that indicates the number of users. . print(list(ios_clean.columns)) . [&#39;id&#39;, &#39;track_name&#39;, &#39;size_bytes&#39;, &#39;currency&#39;, &#39;price&#39;, &#39;rating_count_tot&#39;, &#39;rating_count_ver&#39;, &#39;user_rating&#39;, &#39;user_rating_ver&#39;, &#39;ver&#39;, &#39;cont_rating&#39;, &#39;prime_genre&#39;, &#39;sup_devices.num&#39;, &#39;ipadSc_urls.num&#39;, &#39;lang.num&#39;, &#39;vpp_lic&#39;] . However, the &quot;rating_count_tot&quot; column exists. It indicates the total number of ratings given to each app. We can use it as a proxy for the number of users of each app. . The function below will return a Series showing the average number of users per app within each type. (Not the total number of users per type.) . def users_by_type(df, type_col, users_col, moct = &quot;mean&quot;): &quot;&quot;&quot;Return a Series that maps each app type to the average number of users per app for that type. Specify &#39;mean&#39; or &#39;median&#39; for the measure of central tendency.&quot;&quot;&quot; dct = {} for index, row in df.iterrows(): app_type = row[type_col] users = row[users_col] dct.setdefault(app_type, []).append(users) dct2 = {} for app_type in dct: counts = dct[app_type] if moct == &quot;mean&quot;: dct2[app_type] = np.mean(counts) elif moct == &quot;median&quot;: dct2[app_type] = np.median(counts) result = pd.Series(dct2).sort_values(ascending = False) return result ios_users = users_by_type(ios_clean, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;) sr_to_df(ios_users, n_head = 5) . name number . 0 Navigation | 86090.333333 | . 1 Reference | 74942.111111 | . 2 Social Networking | 71548.349057 | . 3 Music | 57326.530303 | . 4 Weather | 52279.892857 | . bar_n( ios_users, &quot;Top 10 Most Popular iOS App Types&quot;, &quot;Mean Number of Users per App&quot;, ) . The top 5 iOS app types with the highest mean average number of users per app are Navigation, Reference, Social Networking, Music, and Weather. . However, these mean averages may be skewed by a few particularly popular apps. For example, let us look at the number of users of the top 5 Navigation apps. . ios_nav = ios_clean[[ &quot;track_name&quot;, &quot;rating_count_tot&quot;, ]].loc[ ios_clean[&quot;prime_genre&quot;] == &quot;Navigation&quot; ].sort_values( by = &quot;rating_count_tot&quot;, ascending = False, ).set_index( &quot;track_name&quot;, ) # `ios_nav` is still a DataFrame at this point. # It becomes a Series below. ios_nav = ios_nav[&quot;rating_count_tot&quot;] sr_to_df(ios_nav, n_head = 5) . track_name number . 0 Waze - GPS Navigation, Maps &amp; Real-time Traffic | 345046 | . 1 Google Maps - Navigation &amp; Transit | 154911 | . 2 Geocaching® | 12811 | . 3 CoPilot GPS – Car Navigation &amp; Offline Maps | 3582 | . 4 ImmobilienScout24: Real Estate Search in Germany | 187 | . bar_n( ios_nav, &quot;iOS Navigation Apps by Popularity&quot;, &quot;Number of Users&quot;, ) . Clearly, the distribution is skewed because Waze has such a high number of users. Therefore, a better measure of central tendency to use would be the median, not the mean. . Let us repeat the analysis using the median this time: . ios_users = users_by_type( ios_clean, &quot;prime_genre&quot;, &quot;rating_count_tot&quot;, moct = &quot;median&quot;, ) sr_to_df(ios_users, n_head = 5) . name number . 0 Productivity | 8737.5 | . 1 Navigation | 8196.5 | . 2 Reference | 6614.0 | . 3 Shopping | 5936.0 | . 4 Social Networking | 4199.0 | . bar_n( ios_users, &quot;Top 10 Most Popular iOS App Types&quot;, &quot;Median Number of Users per App&quot;, ) . The top 5 most popular iOS apps by median number of users per app are: . Productivity | Navigation | Reference | Shopping | Social Networking | . These placements are quite different from the top 5 most common iOS apps (Games, Entertainment, Photo &amp; Video, Education, and Social Networking). . . . Important: We can say the following about the Apple App Store. . Apps for entertainment and fun, notably Games, are the most common apps. | Apps for practical purposes, notably Productivity, are the most popular apps. | . Google Play Store: Installs . Let us see which columns in the Google Play Store dataset can tell us about the number of users per app. . android_clean.head() . App Category Rating Reviews Size Installs Type Price Content Rating Genres Last Updated Current Ver Android Ver . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | ART_AND_DESIGN | 4.1 | 159 | 19M | 10,000+ | Free | 0 | Everyone | Art &amp; Design | January 7, 2018 | 1.0.0 | 4.0.3 and up | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | ART_AND_DESIGN | 4.7 | 87510 | 8.7M | 5,000,000+ | Free | 0 | Everyone | Art &amp; Design | August 1, 2018 | 1.2.4 | 4.0.3 and up | . 3 Sketch - Draw &amp; Paint | ART_AND_DESIGN | 4.5 | 215644 | 25M | 50,000,000+ | Free | 0 | Teen | Art &amp; Design | June 8, 2018 | Varies with device | 4.2 and up | . 4 Pixel Draw - Number Art Coloring Book | ART_AND_DESIGN | 4.3 | 967 | 2.8M | 100,000+ | Free | 0 | Everyone | Art &amp; Design;Creativity | June 20, 2018 | 1.1 | 4.4 and up | . 5 Paper flowers instructions | ART_AND_DESIGN | 4.4 | 167 | 5.6M | 50,000+ | Free | 0 | Everyone | Art &amp; Design | March 26, 2017 | 1.0 | 2.3 and up | . The &quot;Installs&quot; column seems like the best indicator of the number of users. . android_clean[[&quot;App&quot;, &quot;Installs&quot;]] . App Installs . 0 Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook | 10,000+ | . 2 U Launcher Lite – FREE Live Cool Themes, Hide ... | 5,000,000+ | . 3 Sketch - Draw &amp; Paint | 50,000,000+ | . 4 Pixel Draw - Number Art Coloring Book | 100,000+ | . 5 Paper flowers instructions | 50,000+ | . ... ... | ... | . 10836 Sya9a Maroc - FR | 5,000+ | . 10837 Fr. Mike Schmitz Audio Teachings | 100+ | . 10838 Parkinson Exercices FR | 1,000+ | . 10839 The SCP Foundation DB fr nn5n | 1,000+ | . 10840 iHoroscope - 2018 Daily Horoscope &amp; Astrology | 10,000,000+ | . 8864 rows × 2 columns . The column contains strings which indicate the general range of how many users installed the apps. Since we cannot find the exact number of installs, we will simply remove the &quot;+&quot; signs and convert the numbers into integers. . android_clean[&quot;Installs&quot;] = [int(re.sub(&quot;[,+]&quot;, &quot;&quot;, text)) for text in android_clean[&quot;Installs&quot;]] android_clean[[&quot;Installs&quot;]] . Installs . 0 10000 | . 2 5000000 | . 3 50000000 | . 4 100000 | . 5 50000 | . ... ... | . 10836 5000 | . 10837 100 | . 10838 1000 | . 10839 1000 | . 10840 10000000 | . 8864 rows × 1 columns . Let us now see which app categories are most popular. We will use the median average here, as we did for iOS apps. . android_users = users_by_type( android_clean, &quot;Category&quot;, &quot;Installs&quot;, moct = &quot;median&quot;, ) sr_to_df(android_users, n_head = 10) . name number . 0 ENTERTAINMENT | 1000000.0 | . 1 EDUCATION | 1000000.0 | . 2 GAME | 1000000.0 | . 3 PHOTOGRAPHY | 1000000.0 | . 4 SHOPPING | 1000000.0 | . 5 WEATHER | 1000000.0 | . 6 VIDEO_PLAYERS | 1000000.0 | . 7 COMMUNICATION | 500000.0 | . 8 FOOD_AND_DRINK | 500000.0 | . 9 HEALTH_AND_FITNESS | 500000.0 | . bar_n( android_users, &quot;Top 10 Most Popular Android App Types&quot;, &quot;Median Number of Users per App&quot;, n = 10, ) . Since the top 5 spots all had the same median number of users per app (1000000), the graph was expanded to include the top 10 spots. . It appears that the types of Android apps with the highest median number of users per app are: . GAME | VIDEO_PLAYERS | WEATHER | EDUCATION | ENTERTAINMENT | PHOTOGRAPHY | SHOPPING | . . . Important: We can say the following about the Google Play Store. . Both fun apps and practical apps are very common. | The most popular apps are also a mix of fun apps and practical apps. | . App Profile Ideas . Based on the results, we can now determine a profitable app profile for the hypothetical app company. . Here is a summary of the findings on the 2 app stores. . The Google Play Store has a balanced mix of fun and practical apps, so we can pick either kind. | On the other hand, the Apple App Store appears to be oversaturated with game apps, and practical apps are more popular. | . Therefore, in order to get the most users, the app company can set themselves apart in the Apple App Store by developing a useful practical app. . The most popular types of practical apps for the Apple App Store would be: . Productivity | Navigation | Reference | Shopping | . For the Google Play Store, these would be: . Weather | Education | Photography | Shopping | . Shopping appears in both lists, so it may be the most profitable type of app. However, the app company would have to make a unique app that has an edge over existing popular shopping apps. The same would apply for making a navigation app. . Considering that Reference and Education apps are popular, perhaps these two types could be combined into one app. First, let us find out the titles of the most popular apps in these genres. . reference_popularity = ios_clean[[ &quot;track_name&quot;, &quot;rating_count_tot&quot; ]].loc[ ios_clean[&quot;prime_genre&quot;] == &quot;Reference&quot; ].dropna( ).sort_values( &quot;rating_count_tot&quot;, ascending = False, ).set_index( &quot;track_name&quot;, )[&quot;rating_count_tot&quot;] sr_to_df(reference_popularity, n_head = 10) . track_name number . 0 Bible | 985920 | . 1 Dictionary.com Dictionary &amp; Thesaurus | 200047 | . 2 Dictionary.com Dictionary &amp; Thesaurus for iPad | 54175 | . 3 Google Translate | 26786 | . 4 Muslim Pro: Ramadan 2017 Prayer Times, Azan, Q... | 18418 | . 5 New Furniture Mods - Pocket Wiki &amp; Game Tools ... | 17588 | . 6 Merriam-Webster Dictionary | 16849 | . 7 Night Sky | 12122 | . 8 City Maps for Minecraft PE - The Best Maps for... | 8535 | . 9 LUCKY BLOCK MOD ™ for Minecraft PC Edition - T... | 4693 | . bar_n( reference_popularity, &quot;Top 10 Most Popular iOS Reference Apps&quot;, &quot;Number of Users&quot;, ) . education_popularity = android_clean[[ &quot;App&quot;, &quot;Installs&quot; ]].loc[ android_clean[&quot;Category&quot;] == &quot;EDUCATION&quot; ].dropna( ).sort_values( &quot;Installs&quot;, ascending = False, ).set_index( &quot;App&quot;, )[&quot;Installs&quot;] sr_to_df(education_popularity, n_head = 5) . App number . 0 Quizlet: Learn Languages &amp; Vocab with Flashcards | 10000000 | . 1 Learn languages, grammar &amp; vocabulary with Mem... | 10000000 | . 2 Learn English with Wlingua | 10000000 | . 3 Remind: School Communication | 10000000 | . 4 Math Tricks | 10000000 | . bar_n( education_popularity, &quot;Top 10 Most Popular Android Education Apps&quot;, &quot;Number of Users&quot;, ) . The most popular Reference apps are the Bible and some dictionary and translation apps. The most popular Education apps teach languages (especially English), or Math. . Therefore, the following are some ideas of a profitable app: . An app containing the Bible, another religious text, or another well-known text. The app can additionally include reflections, analyses, or quizzes about the text. | An app that contains an English dictionary, a translator, and some quick guides on English vocabulary and grammar. An app like the above, but for a different language that is spoken by many people. | . | An app that teaches English and Math lessons. Perhaps it could be marketed as a practice app for an entrance exam. | . Conclusion . In this project, we analyzed app data from a Google Play Store dataset and an Apple App Store dataset. Apps were limited to free apps targeted towards English speakers, because the hypothetical app company makes these kinds of apps. The most common and popular app genres were determined. . In the end, several ideas of profitable apps were listed. The app company may now review the analysis and consider the suggestions. This may help them make an informed, data-driven decision regarding the next app that they will develop. .",
            "url": "https://miguelahg.github.io/mahg-data-science/python/pandas/numpy/altair/2021/05/08/Profitable-App-Profiles.html",
            "relUrl": "/python/pandas/numpy/altair/2021/05/08/Profitable-App-Profiles.html",
            "date": " • May 8, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Miguel Antonio H. Germar, but you can call me Migs. I am currently a high school student in the Philippines. I am interested in pursuing a college degree and career in Data Science. Other than this, my interests also include anime and archery. . . Contact . Email: migs.germar@gmail.com . Facebook: https://www.facebook.com/miguelantonio.germar/ . My Learning Journey . I set up this website using fastpages as a blog and portfolio for Data Science projects. Below, I outline my learning journey in Data Science. Eventually, I will make blog posts about most of the projects that I have done along the way. . How I started in Data Science . I first found out about data science in 2020, around the time that the COVID-19 pandemic started. I decided to take online courses about data science in the summer, in order to see if it was interesting. First, I took the Python Core, Data Science, and Machine learning courses on Sololearn. . Here, I learned basic skills in the following: . Python | Spyder IDE | Jupyter Noteboook | Numpy | Pandas | Matplotlib | Scikit-learn | SQL basics | . Data Science in Practical Research . When classes started again, I prioritized schoolwork. However, I was able to apply my data science skills in my group’s Practical Research project. Our research paper was entitled “The Effect of COVID-19’s Consequences on Philippine Frontliners on their Mental Health: A Descriptive, Correlational Study.” We collected survey responses from 196 frontliners. I wrote the entire analysis in Python, from data cleaning to transformation to modeling. . I had to learn new things in order to do this, including: . Statsmodels | Dummy-coding categorical variables | Multiple linear regression | Testing the assumptions of OLS regression | Interpreting and reporting p-values | . The Present: Summer of 2021 . Currently, I am working on the Data Scientist in Python course on Dataquest. The course includes many guided projects in Jupyter Notebook, and I will post each project on my blog as I go along. . I also aim to complete a few courses in the Data Scientist pathway of DOST’s Sparta project, for the sake of learning theory and concepts. . .",
          "url": "https://miguelahg.github.io/mahg-data-science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://miguelahg.github.io/mahg-data-science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}